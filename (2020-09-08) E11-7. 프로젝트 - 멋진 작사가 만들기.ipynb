{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11-7. 프로젝트: 멋진 작사가 만들기\n",
    "\n",
    "* 자연어 처리에 많이 쓰이는 모듈인 LSTM 을 사용해 문장 생성 모델을 만들어봤다. 학습 데이터는 캐글의 ['Song Lyrics'](https://www.kaggle.com/paultimothymooney/poetry/data) 이다.   \n",
    "   \n",
    "   \n",
    "* .txt 파일 49개를 불러와 전처리를 진행했다. 정규표현식으로 특수문자에 공백을 추가하고, 여러 개의 공백은 하나로 변환하고, 알파벳이나 몇몇 특수문자를 제외한 나머지 문자는 전부 공백으로 변환했다. 이후 ```tf.keras.preprocessing.text.Tokenizer()``` 를 이용해 정제한 가사를 토큰화했다. 토큰화 과정에서 단어 수는 프로젝트 과제에 주어진 **num_words=12000** 로 설정했다.   \n",
    "   \n",
    "   \n",
    "* 각 텐서의 길이를 맞추기 위해 ```tf.keras.preprocessing.sequence.pad_sequences()``` 로 패딩을 설정했다. 프로젝트 과제에 주어진 대로 **maxlen=15**로 설정했다. 여러 번 해본 결과 padding 방향이 pre보다 post일 때 loss가 더 낮아 ```post``` 를 유지했다.   \n",
    "   \n",
    "   \n",
    "* 모델 설계 과정에서 Embedding Layer 의 **embedding_size=512**로, LSTM의 **hidden_size=1024** 로 설정했을 때 **epoch=10**에서 **loss가 2.1089** 로 주어진 조건을 만족했다.   \n",
    "   \n",
    "   \n",
    "* 느낀 점: LSTM은 속도가 느리다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['The Cat in the Hat', 'By Dr. Seuss', 'The sun did not shine.', 'It was too wet to play.', 'So we sat in the house', 'All that cold cold wet day.', 'I sat there with Sally.', 'We sat there we two.', 'And I said How I wish', 'We had something to do!', 'Too wet to go out', 'And too cold to play ball.', 'So we sat in the house.', 'We did nothing at all.', 'So all we could do was to', 'Sit!', 'Sit!', 'Sit!', 'Sit!', 'And we did not like it.', 'Not one little bit.', 'BUMP!', 'And then', 'something went BUMP!', 'How that bump made us jump!', 'We looked!', 'Then we saw him step in on the mat!', 'We looked!', 'And we saw him!', 'The Cat in the Hat!', 'And he said to us', 'Why do you sit there like that?', 'I know it is wet', 'And the sun is not sunny.', 'But we can have', 'Lots of good fun that is funny!', 'I know some good games we could play', 'Said the cat.', 'I know some new tricks', 'Said the Cat in the Hat.', 'A lot of good tricks.', 'I will show them to you.', 'Your mother', 'Will not mind at all if I do.', 'Then Sally and I', 'Did not know what to say.', 'Our mother was out of the house', 'For the day.', 'But our fish said No! No!', 'Make that cat go away!']\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = 'data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 정제\n",
    "* preprocess_sentence() 활용\n",
    "* 문장을 토큰화했을 때 토큰 수가 15개 넘어가면 잘라낼 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> the cat in the hat <end>',\n",
       " '<start> by dr . seuss <end>',\n",
       " '<start> the sun did not shine . <end>',\n",
       " '<start> it was too wet to play . <end>',\n",
       " '<start> so we sat in the house <end>',\n",
       " '<start> all that cold cold wet day . <end>',\n",
       " '<start> i sat there with sally . <end>',\n",
       " '<start> we sat there we two . <end>',\n",
       " '<start> and i said how i wish <end>',\n",
       " '<start> we had something to do ! <end>',\n",
       " '<start> too wet to go out <end>',\n",
       " '<start> and too cold to play ball . <end>',\n",
       " '<start> so we sat in the house . <end>',\n",
       " '<start> we did nothing at all . <end>',\n",
       " '<start> so all we could do was to <end>',\n",
       " '<start> sit ! <end>',\n",
       " '<start> sit ! <end>',\n",
       " '<start> sit ! <end>',\n",
       " '<start> sit ! <end>',\n",
       " '<start> and we did not like it . <end>',\n",
       " '<start> not one little bit . <end>',\n",
       " '<start> bump ! <end>',\n",
       " '<start> and then <end>',\n",
       " '<start> something went bump ! <end>',\n",
       " '<start> how that bump made us jump ! <end>',\n",
       " '<start> we looked ! <end>',\n",
       " '<start> then we saw him step in on the mat ! <end>',\n",
       " '<start> we looked ! <end>',\n",
       " '<start> and we saw him ! <end>',\n",
       " '<start> the cat in the hat ! <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전처리\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "    \n",
    "    \n",
    "# print(preprocess_sentence(\"This @_is ;;;sample        sentence.\")) # 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "# print(preprocess_sentence(raw_corpus[9]))\n",
    "\n",
    "# 정제 데이터 구축\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "    \n",
    "corpus[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    6  903 ...    0    0    0]\n",
      " [   2  122 2584 ...    0    0    0]\n",
      " [   2    6  304 ...    0    0    0]\n",
      " ...\n",
      " [ 673   27    6 ...    6  189    3]\n",
      " [   2  673   27 ...    0    0    0]\n",
      " [   2  673   27 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f079092b410>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, # 전체 단어의 개수 \n",
    "        filters=' ', # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\" # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus) # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus) # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding 메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    6  903   14    6 1350    3    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2  122 2584   20    1    3    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2    6  304  166   70  559   20    3    0    0    0    0    0    0\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :]) # length of each tensor: 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27592"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break\n",
    "      \n",
    "len(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2    6  903   14    6 1350    3    0    0    0    0    0    0    0]\n",
      "[   6  903   14    6 1350    3    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1] # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:] # tensor에서 <START>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 데이터셋 분리\n",
    "* sklearn 모듈의 train_test_split() 함수로 훈련/평가 데이터 분리\n",
    "* 단어장 크기는 12000 이상으로\n",
    "* 총 데이터의 20%를 평가 데이터셋으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140599, 14)\n",
      "Target Train: (140599, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "print(VOCAB_SIZE) # 12001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인공지능 만들기\n",
    "* embedding_size 와 hidden_size 를 조절하며 10 epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 3.01960572e-05, -3.29620561e-05, -1.31654233e-04, ...,\n",
       "          2.90282216e-04,  1.32823858e-04,  3.67608009e-05],\n",
       "        [-1.63750301e-05,  8.19140259e-05, -1.32783316e-04, ...,\n",
       "          3.16095509e-04, -5.89688498e-05,  4.47397688e-05],\n",
       "        [-5.14268875e-04,  4.08227468e-04, -1.57484446e-05, ...,\n",
       "          3.81971331e-04, -6.22258231e-05,  2.24955729e-04],\n",
       "        ...,\n",
       "        [-8.65585636e-04,  4.83993528e-04, -1.54074834e-04, ...,\n",
       "          7.30732572e-04,  6.87194522e-04,  9.69527697e-04],\n",
       "        [-1.10901624e-03,  8.83469998e-04,  6.39091086e-05, ...,\n",
       "          6.43138133e-04,  4.21367586e-04,  1.08836498e-03],\n",
       "        [-1.23321009e-03,  1.49754586e-03,  3.26021487e-04, ...,\n",
       "          2.09461054e-04,  1.13486014e-04,  1.13339582e-03]],\n",
       "\n",
       "       [[ 3.01960572e-05, -3.29620561e-05, -1.31654233e-04, ...,\n",
       "          2.90282216e-04,  1.32823858e-04,  3.67608009e-05],\n",
       "        [-3.54013056e-04,  1.36320406e-04, -1.12357186e-04, ...,\n",
       "          6.16989622e-04,  3.64447696e-05,  1.25569917e-04],\n",
       "        [-3.70869529e-04,  5.90217533e-05, -2.73776299e-04, ...,\n",
       "          4.63497767e-04, -2.74452148e-04,  1.32025263e-04],\n",
       "        ...,\n",
       "        [ 1.13212189e-03, -4.59712261e-04,  7.11557223e-04, ...,\n",
       "          2.55364488e-04, -1.11455226e-03,  1.42176368e-03],\n",
       "        [ 5.60077548e-04, -2.76223116e-04,  7.44318124e-04, ...,\n",
       "          6.82396712e-05, -1.11211522e-03,  1.18851790e-03],\n",
       "        [-1.91086205e-04, -3.16669873e-04,  6.95367926e-04, ...,\n",
       "          3.57341167e-04, -1.17494725e-03,  1.09004497e-03]],\n",
       "\n",
       "       [[ 3.01960572e-05, -3.29620561e-05, -1.31654233e-04, ...,\n",
       "          2.90282216e-04,  1.32823858e-04,  3.67608009e-05],\n",
       "        [-1.36194256e-04,  2.05276137e-06, -5.04743133e-04, ...,\n",
       "          6.44258747e-04, -2.82183784e-04,  1.30890548e-04],\n",
       "        [-6.73580216e-05, -1.98168636e-04, -5.41270245e-04, ...,\n",
       "          7.72721309e-04, -5.21160197e-04,  2.58502347e-04],\n",
       "        ...,\n",
       "        [-1.87243673e-03,  3.52840219e-03,  5.21035690e-04, ...,\n",
       "         -1.28713797e-03, -1.87743118e-03,  9.96781397e-04],\n",
       "        [-1.84272975e-03,  4.26859269e-03,  8.30071454e-04, ...,\n",
       "         -1.78178796e-03, -1.90458808e-03,  1.10734499e-03],\n",
       "        [-1.75747485e-03,  4.94627561e-03,  1.15788588e-03, ...,\n",
       "         -2.22687051e-03, -1.91382959e-03,  1.25230930e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 3.01960572e-05, -3.29620561e-05, -1.31654233e-04, ...,\n",
       "          2.90282216e-04,  1.32823858e-04,  3.67608009e-05],\n",
       "        [-3.54013056e-04,  1.36320406e-04, -1.12357186e-04, ...,\n",
       "          6.16989622e-04,  3.64447696e-05,  1.25569917e-04],\n",
       "        [-4.89627360e-04,  2.64557806e-04, -1.92813008e-04, ...,\n",
       "          4.56124224e-04, -7.95683416e-04,  1.13942704e-04],\n",
       "        ...,\n",
       "        [ 7.06835766e-04,  5.84922789e-04,  9.07138165e-05, ...,\n",
       "         -1.02682214e-03,  1.99418049e-04,  9.59669938e-04],\n",
       "        [ 5.71524608e-04,  1.46856532e-03,  4.68321028e-04, ...,\n",
       "         -1.56256836e-03, -5.37962223e-05,  1.04683125e-03],\n",
       "        [ 4.40173608e-04,  2.34808261e-03,  8.61207081e-04, ...,\n",
       "         -2.04837532e-03, -2.93782912e-04,  1.17739150e-03]],\n",
       "\n",
       "       [[ 3.01960572e-05, -3.29620561e-05, -1.31654233e-04, ...,\n",
       "          2.90282216e-04,  1.32823858e-04,  3.67608009e-05],\n",
       "        [ 1.47470055e-04, -3.13078141e-04, -2.99677660e-04, ...,\n",
       "          4.01852478e-04,  8.85697082e-04,  4.34120739e-04],\n",
       "        [ 7.77466412e-05, -2.05947421e-04, -7.85815573e-05, ...,\n",
       "          5.14674059e-04,  1.08530244e-03,  8.58760090e-04],\n",
       "        ...,\n",
       "        [-9.64771782e-04,  3.22530488e-03,  1.06822769e-03, ...,\n",
       "         -2.03125505e-03, -9.91806271e-04,  1.20014977e-03],\n",
       "        [-8.69138807e-04,  3.97727545e-03,  1.43035536e-03, ...,\n",
       "         -2.51569506e-03, -1.13645080e-03,  1.36703148e-03],\n",
       "        [-7.93020125e-04,  4.67160763e-03,  1.78669742e-03, ...,\n",
       "         -2.92640948e-03, -1.25431432e-03,  1.55141612e-03]],\n",
       "\n",
       "       [[ 3.01960572e-05, -3.29620561e-05, -1.31654233e-04, ...,\n",
       "          2.90282216e-04,  1.32823858e-04,  3.67608009e-05],\n",
       "        [-9.81059056e-05,  7.41357871e-05, -4.31950466e-04, ...,\n",
       "          2.60892964e-04,  5.83048793e-04,  9.28437075e-05],\n",
       "        [ 8.78401479e-05, -1.89810657e-04, -1.80838106e-04, ...,\n",
       "          3.13997472e-04,  1.00199564e-03,  4.02369078e-05],\n",
       "        ...,\n",
       "        [ 1.76734186e-03, -5.51628531e-04,  1.09737995e-03, ...,\n",
       "          5.30476798e-04,  4.95316461e-04,  3.96134492e-05],\n",
       "        [ 1.87312963e-03, -1.93680462e-04,  1.38917693e-03, ...,\n",
       "          2.87351228e-04,  8.56537081e-04,  1.74876390e-04],\n",
       "        [ 1.47227303e-03, -2.05400807e-04,  1.38726470e-03, ...,\n",
       "          5.52890531e-04,  9.30951675e-04,  3.35927296e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델에 데이터 태워보기\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  6144512   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  6295552   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 33,133,793\n",
      "Trainable params: 33,133,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "549/549 [==============================] - 86s 157ms/step - loss: 3.6423\n",
      "Epoch 2/10\n",
      "549/549 [==============================] - 90s 164ms/step - loss: 3.1394\n",
      "Epoch 3/10\n",
      "549/549 [==============================] - 91s 166ms/step - loss: 2.9492\n",
      "Epoch 4/10\n",
      "549/549 [==============================] - 91s 166ms/step - loss: 2.8037\n",
      "Epoch 5/10\n",
      "549/549 [==============================] - 91s 165ms/step - loss: 2.6723\n",
      "Epoch 6/10\n",
      "549/549 [==============================] - 91s 165ms/step - loss: 2.5501\n",
      "Epoch 7/10\n",
      "549/549 [==============================] - 90s 164ms/step - loss: 2.4326\n",
      "Epoch 8/10\n",
      "549/549 [==============================] - 90s 164ms/step - loss: 2.3204\n",
      "Epoch 9/10\n",
      "549/549 [==============================] - 91s 166ms/step - loss: 2.2132\n",
      "Epoch 10/10\n",
      "549/549 [==============================] - 90s 164ms/step - loss: 2.1089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f078f2fca10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10) # loss < 2.2를 충족시킬 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you mom , you re my favorite girl <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4. 모델이 생성한 가사 한 줄을 제출하세요.\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor) # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <END>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated # 이것이 최종적으로 모델이 생성한 자연어 문장입니다.\n",
    "\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
