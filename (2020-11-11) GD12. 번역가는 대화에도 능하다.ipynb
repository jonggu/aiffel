{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GD12. 번역기는 대화에도 능하다\n",
    "* https://www.youtube.com/watch?v=8zfYINYNS38\n",
    "   \n",
    "```\n",
    "$ mkdir transformer_chatbot\n",
    "$ sudo apt-get install g++ openjdk-8-jdk\n",
    "$ sudo apt-get install curl\n",
    "$ bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
    "$ pip install konlpy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 번역 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# !pip install nltk # nltk가 설치되어 있지 않은 경우 주석 해제\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import math\n",
    "\n",
    "import gensim\n",
    "\n",
    "import gensim.downloader as api\n",
    "import random\n",
    "\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip',\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    corpus = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(corpus))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in corpus[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화\n",
    "* SentencePiece: https://github.com/google/sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"spa-eng\",\n",
    "                       pad_id=0,\n",
    "                       bos_id=1,\n",
    "                       eos_id=2,\n",
    "                       unk_id=3):\n",
    "    file = \"./%s_corpus.txt\" % lang\n",
    "    model = \"%s_spm\" % lang\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        for row in corpus: f.write(str(row) + '\\n')\n",
    "\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
    "        % (file, model, vocab_size) + \\\n",
    "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
    "        % (pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model' % model)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = list(set(corpus))\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "tokenizer = generate_tokenizer(cleaned_corpus, VOCAB_SIZE)\n",
    "tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence): # 문장부호, 대소문자 등 정제\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,¿¡])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿¡]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook    # Process 과정을 보기 위해\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "for pair in tqdm_notebook(cleaned_corpus):\n",
    "    src, tgt = pair.split('\\t')\n",
    "\n",
    "    src_tokens = tokenizer.encode_as_ids(preprocess_sentence(src))\n",
    "    tgt_tokens = tokenizer.encode_as_ids(preprocess_sentence(tgt))\n",
    "    \n",
    "    # 토큰이 50개 이상인 경우 제거\n",
    "    if (len(src_tokens) > 50): continue\n",
    "    if (len(tgt_tokens) > 50): continue\n",
    "    \n",
    "    src_corpus.append(src_tokens)\n",
    "    tgt_corpus.append(tgt_tokens)\n",
    "\n",
    "len(src_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(enc_tensor, dec_tensor, test_size=0.01) # test set은 1%만\n",
    "\n",
    "print(len(enc_train), len(enc_val), len(dec_train), len(dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트랜스포머 구현\n",
    "* 기본 구조: https://wikidocs.net/31379\n",
    "* PyTorch로 구현된 트랜스포머\n",
    "    - https://paul-hyun.github.io/transformer-01/\n",
    "    - https://paul-hyun.github.io/transformer-02/\n",
    "    - https://paul-hyun.github.io/transformer-03/\n",
    "* Attention Layer 구현: https://rubikscode.net/2019/08/05/transformer-with-python-and-tensorflow-2-0-attention-layers/\n",
    "* Encoder와 Decoder 각각의 Embedding과 출력층의 Linear, 총 3개의 레이어가 Weight를 공유할 수 있게 할 것\n",
    "* 하이퍼파라미터\n",
    "```\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 구현\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask  생성하기\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Head Attention 구현\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position-wise Feed Forward Network 구현\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder의 레이어 구현\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.dec_self_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 구현\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "\n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 구현\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 전체 모델 조립\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 하이퍼파라미터로 Transformer 인스턴스 생성\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler 구현\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate 인스턴스 선언 & Optimizer 구현\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 정의\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]\n",
    "    gold = tgt[:, 1:]\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련시키기\n",
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 번역 성능 측정하기 (1) BLEU Score\n",
    "### NLTK를 활용한 BLEU Score\n",
    "* BLEU Score가 50점을 넘는다는 것은 번역 품질이 높다는 뜻 (논문 제시: 20~40점)\n",
    "* $(\\prod_{i=1}^4 precision_i)^{\\frac{1}{4}} = (\\text{1-gram} \\times\\text{2-gram} \\times\\text{3-gram} \\times\\text{4-gram})^{\\frac{1}{4}}$\n",
    "    - 1-gram부터 4-gram까지의 점수 (precision) 를 모두 곱한 후, 루트를 두 번 씌우면 (^{1/4}) BLEU Score\n",
    "    - 어떤 N-gram이 0의 값을 갖는다면 그 하위 N-gram 점수들이 모두 소멸되는 문제\n",
    "    - 일치하는 N-gram이 없더라도 점수를 1.0 으로 유지하여 하위 점수를 보존하게끔 구현이 되어있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"많 은 자연어 처리 연구자 들 이 트랜스포머 를 선호 한다\".split()\n",
    "candidate = \"적 은 자연어 학 개발자 들 가 트랜스포머 을 선호 한다 요\".split()\n",
    "\n",
    "print(\"원문:\", reference)\n",
    "print(\"번역문:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_bleu([reference], candidate, weights=[1, 0, 0, 0]))   # 1-gram\n",
    "print(sentence_bleu([reference], candidate, weights=[0, 1, 0, 0]))   # 2-gram\n",
    "print(sentence_bleu([reference], candidate, weights=[0, 0, 1, 0]))   # 3-gram\n",
    "print(sentence_bleu([reference], candidate, weights=[0, 0, 0, 1]))   # 4-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```SmoothingFunction()```으로 정확한 BLEU Score 얻기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Smoothing 함수: 모든 Precision에 아주 작은 epsilon 값을 더해주는 역할\n",
    "    - 0점이 부여된 Precision도 완전한 0이 되지 않으니 점수를 1.0 으로 대체할 필요가 없어진다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트랜스포머 모델의 번역 성능 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate()\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(src_corpus, tgt_corpus, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(tgt_corpus)\n",
    "\n",
    "    for idx in tqdm_notebook(range(sample_size)):\n",
    "        src_tokens = src_corpus[idx]\n",
    "        tgt_tokens = tgt_corpus[idx]\n",
    "\n",
    "        src_sentence = tokenizer.decode_ids((src_tokens.tolist()))\n",
    "        tgt_sentence = tokenizer.decode_ids((tgt_tokens.tolist()))\n",
    "\n",
    "        reference = preprocess_sentence(tgt_sentence).split()\n",
    "        candidate = translate(src_sentence, transformer, tokenizer, tokenizer).split()\n",
    "\n",
    "        score = sentence_bleu([reference], candidate,\n",
    "                              smoothing_function=SmoothingFunction().method1)\n",
    "        total_score += score\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Source Sentence: \", src_sentence)\n",
    "            print(\"Model Prediction: \", candidate)\n",
    "            print(\"Real: \", reference)\n",
    "            print(\"Score: %lf\\n\" % score)\n",
    "\n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_bleu(enc_val[:3], dec_val[:3], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 테스트셋으로 측정하는 것은 시간이 제법 걸리니 1/10만 사용해서 실습하는 걸 권장\n",
    "# enc_val[::10] 의 [::10] 은 리스트를 10개씩 건너뛰어 추출하라는 의미\n",
    "eval_bleu(enc_val[::10], dec_val[::10], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 번역 성능 측정하기 (2) Beam Search Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decoder(prob, beam_size):\n",
    "    sequences = [[[], 1.0]]  # 생성된 문장과 점수를 저장\n",
    "\n",
    "    for tok in prob:\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score in sequences:\n",
    "            for idx, p in enumerate(tok): # 각 단어의 확률을 총점에 누적 곱\n",
    "                candidate = [seq + [idx], score * -math.log(-(p-1))]\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        ordered = sorted(all_candidates,\n",
    "                         key=lambda tup:tup[1],\n",
    "                         reverse=True) # 총점 순 정렬\n",
    "        sequences = ordered[:beam_size] # Beam Size에 해당하는 문장만 저장 \n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    0: \"<pad>\",\n",
    "    1: \"까요?\",\n",
    "    2: \"커피\",\n",
    "    3: \"마셔\",\n",
    "    4: \"가져\",\n",
    "    5: \"될\",\n",
    "    6: \"를\",\n",
    "    7: \"한\",\n",
    "    8: \"잔\",\n",
    "    9: \"도\",\n",
    "}\n",
    "\n",
    "prob_seq = [[0.01, 0.01, 0.60, 0.32, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.75, 0.01, 0.01, 0.17],\n",
    "            [0.01, 0.01, 0.01, 0.35, 0.48, 0.10, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.24, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.68],\n",
    "            [0.01, 0.01, 0.12, 0.01, 0.01, 0.80, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.81, 0.01, 0.01, 0.01, 0.01, 0.11, 0.01, 0.01, 0.01],\n",
    "            [0.70, 0.22, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]]\n",
    "\n",
    "prob_seq = np.array(prob_seq)\n",
    "beam_size = 3\n",
    "\n",
    "result = beam_search_decoder(prob_seq, beam_size)\n",
    "\n",
    "for seq, score in result:\n",
    "    sentence = \"\"\n",
    "\n",
    "    for word in seq:\n",
    "        sentence += vocab[word] + \" \"\n",
    "\n",
    "    print(sentence, \"// Score: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search Decoder 작성 및 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_search_decoder() 구현\n",
    "def calc_prob(src_ids, tgt_ids, model):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "    generate_masks(src_ids, tgt_ids)\n",
    "\n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "    model(src_ids, \n",
    "            tgt_ids,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask)\n",
    "\n",
    "    return tf.math.softmax(predictions, axis=-1)\n",
    "  \n",
    "def beam_search_decoder(sentence, \n",
    "                        src_len,\n",
    "                        tgt_len,\n",
    "                        model,\n",
    "                        src_tokenizer,\n",
    "                        tgt_tokenizer,\n",
    "                        beam_size):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    src_in = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                            maxlen=src_len,\n",
    "                                                            padding='post')\n",
    "\n",
    "    pred_cache = np.zeros((beam_size * beam_size, tgt_len), dtype=np.long)\n",
    "    pred = np.zeros((beam_size, tgt_len), dtype=np.long)\n",
    "\n",
    "    eos_flag = np.zeros((beam_size, ), dtype=np.long)\n",
    "    scores = np.ones((beam_size, ))\n",
    "\n",
    "    pred[:, 0] = tgt_tokenizer.bos_id()\n",
    "\n",
    "    dec_in = tf.expand_dims(pred[0, :1], 0)\n",
    "    prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "    for seq_pos in range(1, tgt_len):\n",
    "        score_cache = np.ones((beam_size * beam_size, ))\n",
    "\n",
    "        # init\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            score_cache[cache_pos:cache_pos+beam_size] = scores[branch_idx]\n",
    "            pred_cache[cache_pos:cache_pos+beam_size, :seq_pos] = \\\n",
    "            pred[branch_idx, :seq_pos]\n",
    "\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            if seq_pos != 1:   # 모든 Branch를 <BOS>로 시작하는 경우를 방지\n",
    "                dec_in = pred_cache[branch_idx, :seq_pos]\n",
    "                dec_in = tf.expand_dims(dec_in, 0)\n",
    "\n",
    "                prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "            for beam_idx in range(beam_size):\n",
    "                max_idx = np.argmax(prob)\n",
    "\n",
    "                score_cache[cache_pos+beam_idx] *= prob[max_idx]\n",
    "                pred_cache[cache_pos+beam_idx, seq_pos] = max_idx\n",
    "\n",
    "                prob[max_idx] = -1\n",
    "\n",
    "        for beam_idx in range(beam_size):\n",
    "            if eos_flag[beam_idx] == -1: continue\n",
    "\n",
    "            max_idx = np.argmax(score_cache)\n",
    "            prediction = pred_cache[max_idx, :seq_pos+1]\n",
    "\n",
    "            pred[beam_idx, :seq_pos+1] = prediction\n",
    "            scores[beam_idx] = score_cache[max_idx]\n",
    "            score_cache[max_idx] = -1\n",
    "\n",
    "            if prediction[-1] == tgt_tokenizer.eos_id():\n",
    "                eos_flag[beam_idx] = -1\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_bleu() 구현\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                            candidate,\n",
    "                            weights=weights,\n",
    "                            smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "def beam_bleu(reference, ids, tokenizer):\n",
    "    reference = reference.split()\n",
    "\n",
    "    total_score = 0.0\n",
    "    for _id in ids:\n",
    "        candidate = tokenizer.decode_ids(_id.tolist()).split()\n",
    "        score = calculate_bleu(reference, candidate)\n",
    "\n",
    "        print(\"Reference:\", reference)\n",
    "        print(\"Candidate:\", candidate)\n",
    "        print(\"BLEU:\", calculate_bleu(reference, candidate))\n",
    "\n",
    "        total_score += score\n",
    "\n",
    "    return total_score / len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 324\n",
    "\n",
    "ids = \\\n",
    "beam_search_decoder(tokenizer.decode_ids(enc_val[idx].tolist()),\n",
    "                    enc_train.shape[-1],\n",
    "                    dec_train.shape[-1],\n",
    "                    transformer,\n",
    "                    tokenizer,\n",
    "                    tokenizer,\n",
    "                    beam_size=5)\n",
    "\n",
    "bleu = beam_bleu(tokenizer.decode_ids(dec_val[idx].tolist()), ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 부풀리기\n",
    "```\n",
    "$ pip install gensim\n",
    "```\n",
    "   \n",
    "* gensim에 사전 훈련된 embedding 모델을 불러오는 두 가지 방법\n",
    "    - 직접 모델을 다운로드해 load 하는 방법\n",
    "    - gensim 이 자체적으로 지원하는 downloader 를 활용해 모델을 load 하는 방법 (한국어는 gensim이 지원하지 않기 때문에 불가)\n",
    "* https://github.com/RaRe-Technologies/gensim-data\n",
    "* 대표적으로 사용되는 embedding 모댈: word2vec-google-news-300 (시간 너무 많이 걸림)\n",
    "* 아래에서 사용하는 모델: (적당한 사이즈의) glove-wiki-gigaword-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(\"banana\")\n",
    "\n",
    "'''\n",
    "[('bananas', 0.6691170930862427),\n",
    " ('mango', 0.580410361289978),\n",
    " ('pineapple', 0.5492372512817383),\n",
    " ('coconut', 0.5462779402732849),\n",
    " ('papaya', 0.541056752204895),\n",
    " ('fruit', 0.52181077003479),\n",
    " ('growers', 0.4877638518810272),\n",
    " ('nut', 0.48399588465690613),\n",
    " ('peanut', 0.4806201756000519),\n",
    " ('potato', 0.4806118309497833)]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = \"you know ? all you need is attention .\"\n",
    "sample_tokens = sample_sentence.split()\n",
    "\n",
    "selected_tok = random.choice(sample_tokens)\n",
    "\n",
    "result = \"\"\n",
    "for tok in sample_tokens:\n",
    "    if tok is selected_tok:\n",
    "        result += wv.most_similar(tok)[0][0] + \" \"\n",
    "\n",
    "    else:\n",
    "        result += tok + \" \"\n",
    "\n",
    "print(\"From:\", sample_sentence)\n",
    "print(\"To:\", result)\n",
    "\n",
    "'''\n",
    "From: you know ? all you need is attention .\n",
    "To: you know ? all you need is attention , \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Substitution 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Substitution 구현하기\n",
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "\n",
    "    res = \"\"\n",
    "    toks = sentence.split()\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "\n",
    "    except:   # 단어장에 없는 단어\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "new_corpus = []\n",
    "\n",
    "for idx in tqdm_notebook(range(3000)):\n",
    "    old_src = tokenizer.decode_ids(src_corpus[idx])\n",
    "\n",
    "    new_src = lexical_sub(old_src, wv)\n",
    "\n",
    "    if new_src is not None: new_corpus.append(new_src)\n",
    "\n",
    "    new_corpus.append(old_src)\n",
    "\n",
    "print(new_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프로젝트: 멋진 챗봇 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 다운로드\n",
    "* https://github.com/songys/Chatbot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SD카드 망가졌어</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SD카드 안돼</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SNS 맞팔 왜 안하지ㅠㅠ</td>\n",
       "      <td>잘 모르고 있을 수도 있어요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SNS 시간낭비인 거 아는데 매일 하는 중</td>\n",
       "      <td>시간을 정하고 해보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SNS 시간낭비인데 자꾸 보게됨</td>\n",
       "      <td>시간을 정하고 해보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Q                   A  label\n",
       "0                   12시 땡!          하루가 또 가네요.      0\n",
       "1              1지망 학교 떨어졌어           위로해 드립니다.      0\n",
       "2             3박4일 놀러가고 싶다         여행은 언제나 좋죠.      0\n",
       "3          3박4일 정도 놀러가고 싶다         여행은 언제나 좋죠.      0\n",
       "4                  PPL 심하네          눈살이 찌푸려지죠.      0\n",
       "5                SD카드 망가졌어  다시 새로 사는 게 마음 편해요.      0\n",
       "6                  SD카드 안돼  다시 새로 사는 게 마음 편해요.      0\n",
       "7           SNS 맞팔 왜 안하지ㅠㅠ    잘 모르고 있을 수도 있어요.      0\n",
       "8  SNS 시간낭비인 거 아는데 매일 하는 중       시간을 정하고 해보세요.      0\n",
       "9        SNS 시간낭비인데 자꾸 보게됨       시간을 정하고 해보세요.      0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('ChatbotData.csv')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 정졔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() # 영문자를 모두 소문자로 변환\n",
    "    sentence = re.sub('[^a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣?.!,1-9\\\\s]', \"\", sentence) # 영문자와 한글, 숫자, 그리고 주요 특수문자를 제외하곤 정규식을 활용하여 모두 제거\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3박4일 정도 놀러가고 싶다hello\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(\"3박4일 정도 놀러가고 싶다~~~~~~~~$*$&@($_)hello\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11672\n",
      "11672\n"
     ]
    }
   ],
   "source": [
    "questions = data['Q'] # source\n",
    "answers = data['A'] # target\n",
    "\n",
    "def build_corpus(src, tgt):\n",
    "    que_corpus = []\n",
    "    ans_corpus = []\n",
    "    \n",
    "    mecab = Mecab()\n",
    "    \n",
    "    for idx, sentence in enumerate(src):\n",
    "        src_preprocessed = preprocess_sentence(sentence)\n",
    "        tgt_preprocessed = preprocess_sentence(tgt[idx])\n",
    "        \n",
    "        if src_preprocessed in que_corpus or tgt_preprocessed in ans_corpus:\n",
    "            continue\n",
    "        else:\n",
    "            src_tokenized = mecab.morphs(src_preprocessed)\n",
    "            tgt_tokenized = mecab.morphs(tgt_preprocessed)\n",
    "            \n",
    "            # 토큰 길이는 각 최대 20으로 설정\n",
    "            if len(src_tokenized) <= 20 and len(tgt_tokenized) <= 20:\n",
    "                que_corpus.append(src_tokenized)\n",
    "                ans_corpus.append(tgt_tokenized)\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    return que_corpus, ans_corpus\n",
    "\n",
    "que_corpus, ans_corpus = build_corpus(questions, answers)\n",
    "\n",
    "# 둘이 같아야 함\n",
    "print(len(que_corpus))\n",
    "print(len(ans_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['12', '시', '땡', '!'],\n",
       " ['1', '지망', '학교', '떨어졌', '어'],\n",
       " ['3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다'],\n",
       " ['3', '박', '4', '일', '정도', '놀', '러', '가', '고', '싶', '다'],\n",
       " ['ppl', '심하', '네'],\n",
       " ['sd', '카드', '망가졌', '어'],\n",
       " ['sd', '카드', '안', '돼'],\n",
       " ['sns', '맞', '팔', '왜', '안', '하', '지', 'ㅠㅠ'],\n",
       " ['sns', '시간', '낭비', '인', '거', '아', '는데', '매일', '하', '는', '중'],\n",
       " ['sns', '시간', '낭비', '인데', '자꾸', '보', '게', '됨']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['하루', '가', '또', '가', '네요', '.'],\n",
       " ['위로', '해', '드립니다', '.'],\n",
       " ['여행', '은', '언제나', '좋', '죠', '.'],\n",
       " ['여행', '은', '언제나', '좋', '죠', '.'],\n",
       " ['눈살', '이', '찌푸려', '지', '죠', '.'],\n",
       " ['다시', '새로', '사', '는', '게', '마음', '편해요', '.'],\n",
       " ['다시', '새로', '사', '는', '게', '마음', '편해요', '.'],\n",
       " ['잘', '모르', '고', '있', '을', '수', '도', '있', '어요', '.'],\n",
       " ['시간', '을', '정하', '고', '해', '보', '세요', '.'],\n",
       " ['시간', '을', '정하', '고', '해', '보', '세요', '.']]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation\n",
    "* https://github.com/Kyubyong/wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본: ['12', '시', '땡', '!']\n",
      "결과: 12 시 끗 ! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "word2vec = gensim.models.Word2Vec.load('ko.bin')\n",
    "\n",
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "\n",
    "    res = \"\"\n",
    "    # toks = sentence.split()\n",
    "    toks = sentence\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "\n",
    "    except:   # 단어장에 없는 단어\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res += _to + \" \"\n",
    "        else: res += tok + \" \"\n",
    "\n",
    "    return res\n",
    "\n",
    "# 테스트\n",
    "mecab = Mecab()\n",
    "print(\"원본:\", que_corpus[0])\n",
    "print(\"결과:\", lexical_sub(que_corpus[0], word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45dc15d8ce2042e7bc3e68614476bba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11672.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37b26517ab34d199c90344d4eab5fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11672.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20176\n",
      "20176 \n",
      "\n",
      "['12', '시', '끗', '!']\n",
      "['하루', '가', '또', '가', '네요', '.'] \n",
      "\n",
      "['강아지', '키울', '가능성', '있', '을까']\n",
      "['먼저', '생활', '패턴', '을', '살펴', '보', '세요', '.'] \n",
      "\n",
      "31848\n",
      "31848 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "new_que_corpus = []\n",
    "new_ans_corpus = []\n",
    "\n",
    "# Augmentation된 que_corpus 와 원본 ans_corpus 가 병렬을 이루도록\n",
    "for idx in tqdm_notebook(range(len(que_corpus))):\n",
    "    que_augmented = lexical_sub(que_corpus[idx], word2vec)\n",
    "    ans = ans_corpus[idx]\n",
    "    \n",
    "    if que_augmented is not None:\n",
    "        new_que_corpus.append(que_augmented.split())\n",
    "        new_ans_corpus.append(ans)\n",
    "        # print(\"[que]\", que_corpus[idx], \"->\", que_augmented, \"/\", ans)\n",
    "    else:\n",
    "        # print(\"[que] Augmentation is None:\", que_corpus[idx], \"/\", ans_corpus[idx])\n",
    "        continue\n",
    "    \n",
    "for idx in tqdm_notebook(range(len(ans_corpus))):\n",
    "    que = que_corpus[idx]\n",
    "    ans_augmented = lexical_sub(ans_corpus[idx], word2vec)\n",
    "    \n",
    "    if ans_augmented is not None:\n",
    "        new_que_corpus.append(que)\n",
    "        new_ans_corpus.append(ans_augmented.split())\n",
    "        # print(\"[ans]\", que, \"/\", ans_corpus[idx], \"->\", ans_augmented)\n",
    "    else:\n",
    "        # print(\"[ans] Augmentation is None:\", que_corpus[idx], \"/\", ans_corpus[idx])\n",
    "        continue\n",
    "    \n",
    "\n",
    "print(len(new_que_corpus))\n",
    "print(len(new_ans_corpus), '\\n')\n",
    "\n",
    "print(new_que_corpus[0])\n",
    "print(new_ans_corpus[0], '\\n')\n",
    "\n",
    "print(new_que_corpus[56])\n",
    "print(new_ans_corpus[56], '\\n')\n",
    "\n",
    "que_corpus = que_corpus + new_que_corpus\n",
    "ans_corpus = ans_corpus + new_ans_corpus\n",
    "\n",
    "print(len(que_corpus))\n",
    "print(len(ans_corpus), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 벡터화\n",
    "* 타겟 데이터인 ```ans_corpus``` 에 ```<start>```, ```<end>``` 토큰 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', '하루', '가', '또', '가', '네요', '.', '<end>']\n",
      "['<start>', '이야기', '를', '해', '보', '세요', '.', '<end>']\n"
     ]
    }
   ],
   "source": [
    "tgt_corpus = []\n",
    "\n",
    "for corpus in ans_corpus:\n",
    "    tgt_corpus.append([\"<start>\"] + corpus + [\"<end>\"])\n",
    "    \n",
    "print(tgt_corpus[0])\n",
    "print(tgt_corpus[325])\n",
    "\n",
    "ans_corpus = tgt_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " '.': 2,\n",
       " '<start>': 3,\n",
       " '<end>': 4,\n",
       " '이': 5,\n",
       " '하': 6,\n",
       " '는': 7,\n",
       " '을': 8,\n",
       " '세요': 9,\n",
       " '가': 10,\n",
       " '어': 11,\n",
       " '고': 12,\n",
       " '좋': 13,\n",
       " '해': 14,\n",
       " '거': 15,\n",
       " '있': 16,\n",
       " '보': 17,\n",
       " '은': 18,\n",
       " '지': 19,\n",
       " '?': 20,\n",
       " '도': 21,\n",
       " '아': 22,\n",
       " '나': 23,\n",
       " '게': 24,\n",
       " '겠': 25,\n",
       " '는데': 26,\n",
       " '에': 27,\n",
       " '예요': 28,\n",
       " '사람': 29,\n",
       " '를': 30,\n",
       " '어요': 31,\n",
       " '다': 32,\n",
       " '같': 33,\n",
       " '죠': 34,\n",
       " '한': 35,\n",
       " '싶': 36,\n",
       " '없': 37,\n",
       " '사랑': 38,\n",
       " '네요': 39,\n",
       " '면': 40,\n",
       " '것': 41,\n",
       " '수': 42,\n",
       " '안': 43,\n",
       " '네': 44,\n",
       " '친구': 45,\n",
       " '봐요': 46,\n",
       " '의': 47,\n",
       " '잘': 48,\n",
       " '아요': 49,\n",
       " '생각': 50,\n",
       " '말': 51,\n",
       " '않': 52,\n",
       " '할': 53,\n",
       " '마음': 54,\n",
       " '되': 55,\n",
       " '너무': 56,\n",
       " '주': 57,\n",
       " '했': 58,\n",
       " '만': 59,\n",
       " '더': 60,\n",
       " '일': 61,\n",
       " '기': 62,\n",
       " '내': 63,\n",
       " '었': 64,\n",
       " '들': 65,\n",
       " '연락': 66,\n",
       " '이별': 67,\n",
       " '남자': 68,\n",
       " '여자': 69,\n",
       " '힘들': 70,\n",
       " '많이': 71,\n",
       " '해요': 72,\n",
       " '시간': 73,\n",
       " '으면': 74,\n",
       " '남': 75,\n",
       " '한테': 76,\n",
       " '길': 77,\n",
       " '에요': 78,\n",
       " '먹': 79,\n",
       " '썸': 80,\n",
       " '괜찮': 81,\n",
       " '때': 82,\n",
       " '좀': 83,\n",
       " '으로': 84,\n",
       " '에서': 85,\n",
       " '았': 86,\n",
       " '많': 87,\n",
       " '짝': 88,\n",
       " '야': 89,\n",
       " '저': 90,\n",
       " '!': 91,\n",
       " '요': 92,\n",
       " '받': 93,\n",
       " '건': 94,\n",
       " '뭐': 95,\n",
       " '만나': 96,\n",
       " '그러': 97,\n",
       " '을까': 98,\n",
       " '알': 99,\n",
       " '마세요': 100,\n",
       " '로': 101,\n",
       " '오늘': 102,\n",
       " '적': 103,\n",
       " '에게': 104,\n",
       " '애': 105,\n",
       " '해도': 106,\n",
       " '그': 107,\n",
       " '못': 108,\n",
       " '인': 109,\n",
       " '자신': 110,\n",
       " '습니다': 111,\n",
       " '이제': 112,\n",
       " '은데': 113,\n",
       " '던': 114,\n",
       " '아니': 115,\n",
       " '살': 116,\n",
       " '연애': 117,\n",
       " '필요': 118,\n",
       " '랑': 119,\n",
       " '할까': 120,\n",
       " '라고': 121,\n",
       " '모르': 122,\n",
       " '끝': 123,\n",
       " '해야': 124,\n",
       " '어서': 125,\n",
       " '왜': 126,\n",
       " '걸': 127,\n",
       " '다른': 128,\n",
       " '타': 129,\n",
       " '잊': 130,\n",
       " '년': 131,\n",
       " '혼자': 132,\n",
       " '지만': 133,\n",
       " '시키': 134,\n",
       " '제': 135,\n",
       " '데': 136,\n",
       " '달': 137,\n",
       " '당신': 138,\n",
       " '어떻게': 139,\n",
       " '지금': 140,\n",
       " '다시': 141,\n",
       " '서': 142,\n",
       " '오': 143,\n",
       " '돼': 144,\n",
       " '전': 145,\n",
       " '과': 146,\n",
       " '조금': 147,\n",
       " '정리': 148,\n",
       " '방법': 149,\n",
       " '날': 150,\n",
       " '될': 151,\n",
       " '정말': 152,\n",
       " '같이': 153,\n",
       " 'ㄴ다는': 154,\n",
       " '라': 155,\n",
       " '중': 156,\n",
       " '또': 157,\n",
       " '싫': 158,\n",
       " '봐': 159,\n",
       " '결혼': 160,\n",
       " '사': 161,\n",
       " '중요': 162,\n",
       " '인데': 163,\n",
       " '니': 164,\n",
       " '와': 165,\n",
       " '준비': 166,\n",
       " '인가': 167,\n",
       " '헤어지': 168,\n",
       " 'ㅂ시오': 169,\n",
       " '놀드': 170,\n",
       " '이랑': 171,\n",
       " '행복': 172,\n",
       " '면서': 173,\n",
       " '물': 174,\n",
       " '참': 175,\n",
       " '하나': 176,\n",
       " '아직': 177,\n",
       " '고민': 178,\n",
       " '짝사랑': 179,\n",
       " '지요': 180,\n",
       " '선물': 181,\n",
       " '시': 182,\n",
       " '돼요': 183,\n",
       " '집': 184,\n",
       " '줄': 185,\n",
       " '녀': 186,\n",
       " '자꾸': 187,\n",
       " '보다': 188,\n",
       " '합니다': 189,\n",
       " '때문': 190,\n",
       " '왔': 191,\n",
       " '후회': 192,\n",
       " '아서': 193,\n",
       " '볼까': 194,\n",
       " '까지': 195,\n",
       " '맞': 196,\n",
       " '먼저': 197,\n",
       " '다면': 198,\n",
       " '이야기': 199,\n",
       " '살펴보': 200,\n",
       " '그런': 201,\n",
       " '고백': 202,\n",
       " '다고': 203,\n",
       " '는지': 204,\n",
       " '그녀': 205,\n",
       " '꿈': 206,\n",
       " '술': 207,\n",
       " '봅니다': 208,\n",
       " '될까': 209,\n",
       " '바랄게요': 210,\n",
       " '진짜': 211,\n",
       " '시작': 212,\n",
       " '쓰': 213,\n",
       " '자': 214,\n",
       " '...': 215,\n",
       " '사귀': 216,\n",
       " '놀': 217,\n",
       " '좋아하': 218,\n",
       " '헤어진지': 219,\n",
       " '감정': 220,\n",
       " '힘든': 221,\n",
       " '공부': 222,\n",
       " '군요': 223,\n",
       " '됐': 224,\n",
       " '번': 225,\n",
       " '그럴': 226,\n",
       " '쉬': 227,\n",
       " '계속': 228,\n",
       " '해의': 229,\n",
       " '두': 230,\n",
       " '서로': 231,\n",
       " '해서': 232,\n",
       " '가능': 233,\n",
       " '드세요': 234,\n",
       " '듯': 235,\n",
       " '걱정': 236,\n",
       " '든': 237,\n",
       " '만큼': 238,\n",
       " '으며': 239,\n",
       " '라도': 240,\n",
       " '기에': 241,\n",
       " '눈': 242,\n",
       " '힘드': 243,\n",
       " '기분': 244,\n",
       " '여친': 245,\n",
       " '만들': 246,\n",
       " '1': 247,\n",
       " '돈': 248,\n",
       " '신경': 249,\n",
       " '곳': 250,\n",
       " '씩': 251,\n",
       " '쉽': 252,\n",
       " '바랍니다': 253,\n",
       " '이상': 254,\n",
       " '여행': 255,\n",
       " '러': 256,\n",
       " '자고': 257,\n",
       " '입니다': 258,\n",
       " '데이트': 259,\n",
       " '후': 260,\n",
       " '줘': 261,\n",
       " '어도': 262,\n",
       " '못하': 263,\n",
       " '려고': 264,\n",
       " '너': 265,\n",
       " '나요': 266,\n",
       " '무슨': 267,\n",
       " '2': 268,\n",
       " '꼼짝': 269,\n",
       " '째': 270,\n",
       " '하루': 271,\n",
       " '기억': 272,\n",
       " '걸까': 273,\n",
       " '니까요': 274,\n",
       " '남친': 275,\n",
       " '카톡': 276,\n",
       " '믿': 277,\n",
       " '인지': 278,\n",
       " '였': 279,\n",
       " '표현': 280,\n",
       " '그냥': 281,\n",
       " '이렇게': 282,\n",
       " '대화': 283,\n",
       " '니까': 284,\n",
       " '함께': 285,\n",
       " '상처': 286,\n",
       " '헤어졌': 287,\n",
       " '어떨까': 288,\n",
       " '이해': 289,\n",
       " '내일': 290,\n",
       " '몰라요': 291,\n",
       " '전화': 292,\n",
       " '어디': 293,\n",
       " '어떤': 294,\n",
       " '셨': 295,\n",
       " '관심': 296,\n",
       " '머리': 297,\n",
       " '3': 298,\n",
       " '언제': 299,\n",
       " '맛있': 300,\n",
       " '나의': 301,\n",
       " '기다리': 302,\n",
       " '다가': 303,\n",
       " '된': 304,\n",
       " '누구': 305,\n",
       " '바라': 306,\n",
       " '결정': 307,\n",
       " '긴': 308,\n",
       " ',': 309,\n",
       " '회사': 310,\n",
       " '님': 311,\n",
       " '아프': 312,\n",
       " '새로운': 313,\n",
       " '나가': 314,\n",
       " '선택': 315,\n",
       " '오래': 316,\n",
       " '비': 317,\n",
       " '운동': 318,\n",
       " '궁금': 319,\n",
       " '맘': 320,\n",
       " '음': 321,\n",
       " '잠': 322,\n",
       " '이유': 323,\n",
       " '보내': 324,\n",
       " '도록': 325,\n",
       " '별': 326,\n",
       " '썸남': 327,\n",
       " '함': 328,\n",
       " '속': 329,\n",
       " '노력': 330,\n",
       " '헤어진': 331,\n",
       " '미련': 332,\n",
       " '생각나': 333,\n",
       " '부담': 334,\n",
       " '입': 335,\n",
       " '부터': 336,\n",
       " '직접': 337,\n",
       " '는데요': 338,\n",
       " '처럼': 339,\n",
       " '따라': 340,\n",
       " '어떡': 341,\n",
       " '앞': 342,\n",
       " '인연': 343,\n",
       " '죽': 344,\n",
       " '만날': 345,\n",
       " '인생': 346,\n",
       " '아침': 347,\n",
       " '잘못': 348,\n",
       " '차': 349,\n",
       " '늦': 350,\n",
       " '대로': 351,\n",
       " '우리': 352,\n",
       " '스트레스': 353,\n",
       " '드': 354,\n",
       " '뿐': 355,\n",
       " '첫': 356,\n",
       " '개월': 357,\n",
       " '잡': 358,\n",
       " '이젠': 359,\n",
       " '그게': 360,\n",
       " '상황': 361,\n",
       " '텐데': 362,\n",
       " '똑같': 363,\n",
       " '한가': 364,\n",
       " '찾아보': 365,\n",
       " '가지': 366,\n",
       " '항상': 367,\n",
       " '변화': 368,\n",
       " '라는': 369,\n",
       " '연인': 370,\n",
       " '생겼': 371,\n",
       " '라면': 372,\n",
       " '자주': 373,\n",
       " '거나': 374,\n",
       " '느낌': 375,\n",
       " '도움': 376,\n",
       " '그렇게': 377,\n",
       " '문제': 378,\n",
       " '으세요': 379,\n",
       " '올': 380,\n",
       " '대': 381,\n",
       " '용기': 382,\n",
       " '을까요': 383,\n",
       " '추억': 384,\n",
       " '나이': 385,\n",
       " '사이': 386,\n",
       " '만났': 387,\n",
       " '열심히': 388,\n",
       " '노래': 389,\n",
       " '처음': 390,\n",
       " '볼': 391,\n",
       " '조심': 392,\n",
       " '이나': 393,\n",
       " '답답': 394,\n",
       " '슬픔': 395,\n",
       " '반': 396,\n",
       " '현실': 397,\n",
       " '상대': 398,\n",
       " '다는': 399,\n",
       " '충분히': 400,\n",
       " '축하': 401,\n",
       " '매일': 402,\n",
       " '다음': 403,\n",
       " '카나': 404,\n",
       " '결국': 405,\n",
       " '이에': 406,\n",
       " '보이': 407,\n",
       " '찾': 408,\n",
       " '이런': 409,\n",
       " '순간': 410,\n",
       " '건강': 411,\n",
       " '습관': 412,\n",
       " '만남': 413,\n",
       " '덜': 414,\n",
       " '마지막': 415,\n",
       " '줬': 416,\n",
       " '어렵': 417,\n",
       " '건가': 418,\n",
       " '다르': 419,\n",
       " '요즘': 420,\n",
       " '힘': 421,\n",
       " '의미': 422,\n",
       " '말씀': 423,\n",
       " '힘내': 424,\n",
       " '가슴': 425,\n",
       " '어야': 426,\n",
       " '몸': 427,\n",
       " '자기': 428,\n",
       " '확인': 429,\n",
       " '추천': 430,\n",
       " '까': 431,\n",
       " '꼭': 432,\n",
       " '기대': 433,\n",
       " '아무': 434,\n",
       " '분': 435,\n",
       " '스럽': 436,\n",
       " '세상': 437,\n",
       " '질': 438,\n",
       " '봤': 439,\n",
       " '마다': 440,\n",
       " '화': 441,\n",
       " '건지': 442,\n",
       " '감': 443,\n",
       " '귀찮': 444,\n",
       " '넘': 445,\n",
       " '일까': 446,\n",
       " '제일': 447,\n",
       " '부모': 448,\n",
       " '큰': 449,\n",
       " '그만': 450,\n",
       " '몇': 451,\n",
       " '다니': 452,\n",
       " '지내': 453,\n",
       " '재회': 454,\n",
       " '젊은이': 455,\n",
       " '나쁜': 456,\n",
       " '났': 457,\n",
       " '호감': 458,\n",
       " '갈': 459,\n",
       " '관계': 460,\n",
       " '정도': 461,\n",
       " '놓': 462,\n",
       " '챙겨': 463,\n",
       " '밥': 464,\n",
       " '웃': 465,\n",
       " '얼른': 466,\n",
       " '으니까요': 467,\n",
       " '난': 468,\n",
       " '마시': 469,\n",
       " '영화': 470,\n",
       " '판단': 471,\n",
       " '아닌': 472,\n",
       " '어때': 473,\n",
       " '울': 474,\n",
       " '그렇': 475,\n",
       " '진심': 476,\n",
       " '아도': 477,\n",
       " '어제': 478,\n",
       " '상대방': 479,\n",
       " '못가': 480,\n",
       " '빨리': 481,\n",
       " '연습': 482,\n",
       " '가장': 483,\n",
       " '한두': 484,\n",
       " '재밌': 485,\n",
       " '갖': 486,\n",
       " '졌': 487,\n",
       " '사진': 488,\n",
       " '모든': 489,\n",
       " '엄마': 490,\n",
       " '은데요': 491,\n",
       " '가끔': 492,\n",
       " '얼굴': 493,\n",
       " '주말': 494,\n",
       " '커피': 495,\n",
       " '다가가': 496,\n",
       " '남편': 497,\n",
       " '성공': 498,\n",
       " '핸드폰': 499,\n",
       " '스스로': 500,\n",
       " '자연': 501,\n",
       " '둘': 502,\n",
       " '듣': 503,\n",
       " '워낙': 504,\n",
       " '드릴게요': 505,\n",
       " '없이': 506,\n",
       " '선': 507,\n",
       " '그래도': 508,\n",
       " '새': 509,\n",
       " '모두': 510,\n",
       " '확신': 511,\n",
       " '짜증': 512,\n",
       " '소개팅': 513,\n",
       " '뭘': 514,\n",
       " '예의': 515,\n",
       " '받아들이': 516,\n",
       " '날씨': 517,\n",
       " '복잡': 518,\n",
       " '가져': 519,\n",
       " '톡': 520,\n",
       " '대한': 521,\n",
       " '실수': 522,\n",
       " '엄청': 523,\n",
       " '아야': 524,\n",
       " '갑자기': 525,\n",
       " '관리': 526,\n",
       " '해야지': 527,\n",
       " '기다려': 528,\n",
       " '딱': 529,\n",
       " '붙잡': 530,\n",
       " '정신': 531,\n",
       " '헤어짐': 532,\n",
       " '아픔': 533,\n",
       " '눈물': 534,\n",
       " '편하': 535,\n",
       " '티': 536,\n",
       " '차단': 537,\n",
       " '예쁘': 538,\n",
       " '스러운': 539,\n",
       " '답': 540,\n",
       " '확실': 541,\n",
       " '없었': 542,\n",
       " '게임': 543,\n",
       " '씹': 544,\n",
       " '피곤': 545,\n",
       " '버리': 546,\n",
       " '을지': 547,\n",
       " '5': 548,\n",
       " '학교': 549,\n",
       " '4': 550,\n",
       " '뭘까': 551,\n",
       " '짧': 552,\n",
       " '후폭풍': 553,\n",
       " '아파': 554,\n",
       " '위로': 555,\n",
       " '천천히': 556,\n",
       " '충분': 557,\n",
       " '글': 558,\n",
       " '바람': 559,\n",
       " '6': 560,\n",
       " '갔': 561,\n",
       " '운명': 562,\n",
       " '더니': 563,\n",
       " '란': 564,\n",
       " '재미': 565,\n",
       " '바': 566,\n",
       " '약': 567,\n",
       " '주변': 568,\n",
       " 'ㄴ다면': 569,\n",
       " '옷': 570,\n",
       " '시켜': 571,\n",
       " '냐': 572,\n",
       " '쉬운': 573,\n",
       " '별로': 574,\n",
       " '읽': 575,\n",
       " '캐치': 576,\n",
       " '사세요': 577,\n",
       " '무시': 578,\n",
       " '법': 579,\n",
       " '점점': 580,\n",
       " '나와': 581,\n",
       " '나오': 582,\n",
       " '진': 583,\n",
       " '모습': 584,\n",
       " '보여': 585,\n",
       " '봄': 586,\n",
       " '바쁘': 587,\n",
       " '잔': 588,\n",
       " '따뜻': 589,\n",
       " '아픈': 590,\n",
       " '감기': 591,\n",
       " '일어나': 592,\n",
       " '극복': 593,\n",
       " '인정': 594,\n",
       " '괴로움': 595,\n",
       " '밤': 596,\n",
       " '성격': 597,\n",
       " '거기': 598,\n",
       " '도와': 599,\n",
       " '익숙': 600,\n",
       " '가족': 601,\n",
       " '화장': 602,\n",
       " '마련': 603,\n",
       " '집착': 604,\n",
       " '직장': 605,\n",
       " '때때': 606,\n",
       " '찍': 607,\n",
       " '아무것': 608,\n",
       " '슬픈': 609,\n",
       " '원': 610,\n",
       " '삶': 611,\n",
       " '만난': 612,\n",
       " '언젠간': 613,\n",
       " '잠시': 614,\n",
       " '그분': 615,\n",
       " '가능성': 616,\n",
       " '어려워': 617,\n",
       " '솔직': 618,\n",
       " '능력': 619,\n",
       " '편': 620,\n",
       " '크': 621,\n",
       " '차이': 622,\n",
       " '장': 623,\n",
       " '환승': 624,\n",
       " '인의': 625,\n",
       " '소리': 626,\n",
       " '벌써': 627,\n",
       " '위해': 628,\n",
       " '다행': 629,\n",
       " '아닌데': 630,\n",
       " '번호': 631,\n",
       " '일찍': 632,\n",
       " '형': 633,\n",
       " '지났': 634,\n",
       " '풀': 635,\n",
       " '최고': 636,\n",
       " '며': 637,\n",
       " '잊혀': 638,\n",
       " '행동': 639,\n",
       " '나중': 640,\n",
       " '낫': 641,\n",
       " '카페': 642,\n",
       " '얼마': 643,\n",
       " '알아보': 644,\n",
       " '언제나': 645,\n",
       " '임': 646,\n",
       " '포기': 647,\n",
       " '세': 648,\n",
       " '곧': 649,\n",
       " '만이': 650,\n",
       " '월과': 651,\n",
       " '거짓말': 652,\n",
       " '도전': 653,\n",
       " '귀': 654,\n",
       " '여기': 655,\n",
       " '설레': 656,\n",
       " '떠나': 657,\n",
       " '여유': 658,\n",
       " '잠깐': 659,\n",
       " '소식': 660,\n",
       " '마주치': 661,\n",
       " '다를': 662,\n",
       " '주기도': 663,\n",
       " '보통': 664,\n",
       " '아무래도': 665,\n",
       " '응원': 666,\n",
       " '어야지': 667,\n",
       " '생활': 668,\n",
       " '폰': 669,\n",
       " '야지': 670,\n",
       " '스러워': 671,\n",
       " '맨날': 672,\n",
       " '후련': 673,\n",
       " '척': 674,\n",
       " '한다고': 675,\n",
       " '할지': 676,\n",
       " '점': 677,\n",
       " '자체': 678,\n",
       " '깊': 679,\n",
       " '이루어지': 680,\n",
       " '고생': 681,\n",
       " '생일': 682,\n",
       " '하늘': 683,\n",
       " '안녕': 684,\n",
       " '드리': 685,\n",
       " '금방': 686,\n",
       " '스타일': 687,\n",
       " '간': 688,\n",
       " '소중': 689,\n",
       " '즐기': 690,\n",
       " '별후': 691,\n",
       " '그럼': 692,\n",
       " '한데': 693,\n",
       " '칭찬': 694,\n",
       " '싸우': 695,\n",
       " '준': 696,\n",
       " '뒤': 697,\n",
       " '상담': 698,\n",
       " '시원': 699,\n",
       " '편지': 700,\n",
       " '우산': 701,\n",
       " '진정': 702,\n",
       " '변하': 703,\n",
       " '해질': 704,\n",
       " '생길': 705,\n",
       " '잊어버리': 706,\n",
       " '시험': 707,\n",
       " '기회': 708,\n",
       " '욕': 709,\n",
       " '탈': 710,\n",
       " '동안': 711,\n",
       " '종교': 712,\n",
       " '아닌지': 713,\n",
       " '적기': 714,\n",
       " '말로': 715,\n",
       " '달라지': 716,\n",
       " '옆': 717,\n",
       " '비밀': 718,\n",
       " '쯤': 719,\n",
       " '동거': 720,\n",
       " '부분': 721,\n",
       " 'sns': 722,\n",
       " '비싸': 723,\n",
       " '재미있': 724,\n",
       " '약속': 725,\n",
       " '얘기': 726,\n",
       " '여': 727,\n",
       " '실': 728,\n",
       " '주일': 729,\n",
       " '무엇': 730,\n",
       " '지나': 731,\n",
       " '프': 732,\n",
       " '부족': 733,\n",
       " '밖': 734,\n",
       " '바로': 735,\n",
       " '접': 736,\n",
       " '한지': 737,\n",
       " '려': 738,\n",
       " '우울': 739,\n",
       " '주무세요': 740,\n",
       " '드려요': 741,\n",
       " '마요': 742,\n",
       " '방학': 743,\n",
       " '미치': 744,\n",
       " '벌': 745,\n",
       " '저녁': 746,\n",
       " '자신감': 747,\n",
       " '흘렀': 748,\n",
       " '그래요': 749,\n",
       " '가요': 750,\n",
       " '취미': 751,\n",
       " '이러': 752,\n",
       " '상관': 753,\n",
       " '구': 754,\n",
       " '깨': 755,\n",
       " '작': 756,\n",
       " '자책': 757,\n",
       " '나왔': 758,\n",
       " '해졌': 759,\n",
       " '갈까': 760,\n",
       " '뭔지': 761,\n",
       " '위': 762,\n",
       " '써': 763,\n",
       " '줄까': 764,\n",
       " '여러': 765,\n",
       " '존중': 766,\n",
       " '분위기': 767,\n",
       " '미리': 768,\n",
       " '즐거운': 769,\n",
       " '통보': 770,\n",
       " '한다는': 771,\n",
       " '는다면': 772,\n",
       " '겠지': 773,\n",
       " '이번': 774,\n",
       " '과정': 775,\n",
       " '지켜보': 776,\n",
       " '야가': 777,\n",
       " '멀': 778,\n",
       " '바보': 779,\n",
       " '소개': 780,\n",
       " '멋진': 781,\n",
       " '그대로': 782,\n",
       " '인기': 783,\n",
       " '플': 784,\n",
       " '허전': 785,\n",
       " '그리고': 786,\n",
       " 'ㅠ': 787,\n",
       " '사친': 788,\n",
       " '부유층': 789,\n",
       " '차리': 790,\n",
       " '맞춰': 791,\n",
       " '손': 792,\n",
       " '문자': 793,\n",
       " '인사': 794,\n",
       " '피하': 795,\n",
       " '돌아가': 796,\n",
       " '미안': 797,\n",
       " '삭제': 798,\n",
       " '어느': 799,\n",
       " '내고': 800,\n",
       " '든지': 801,\n",
       " '아닐까요': 802,\n",
       " '배우': 803,\n",
       " '대해': 804,\n",
       " '드디어': 805,\n",
       " '이사': 806,\n",
       " '으면서': 807,\n",
       " '성': 808,\n",
       " '답장': 809,\n",
       " '서운': 810,\n",
       " '전해': 811,\n",
       " '슬프': 812,\n",
       " '까먹': 813,\n",
       " '전환': 814,\n",
       " '예쁜': 815,\n",
       " '월급': 816,\n",
       " '지쳤': 817,\n",
       " '끊': 818,\n",
       " '책': 819,\n",
       " '기간': 820,\n",
       " '숨': 821,\n",
       " '로맨틱': 822,\n",
       " '불편': 823,\n",
       " '키우': 824,\n",
       " '버렸': 825,\n",
       " '완전': 826,\n",
       " '취직': 827,\n",
       " '은가': 828,\n",
       " '헷갈리': 829,\n",
       " '흐르': 830,\n",
       " '한다면': 831,\n",
       " '더라고요': 832,\n",
       " 'ㅠㅠ': 833,\n",
       " '예민': 834,\n",
       " '꿨': 835,\n",
       " '으니': 836,\n",
       " '뭔가': 837,\n",
       " '알려': 838,\n",
       " '이성': 839,\n",
       " '어쩔': 840,\n",
       " '조언': 841,\n",
       " '따로': 842,\n",
       " '잖아요': 843,\n",
       " '쇼핑': 844,\n",
       " '엔': 845,\n",
       " '얼': 846,\n",
       " '한다': 847,\n",
       " '걸로': 848,\n",
       " '선생': 849,\n",
       " '집중': 850,\n",
       " '자유': 851,\n",
       " '현재': 852,\n",
       " '시기': 853,\n",
       " '썸녀': 854,\n",
       " '매력': 855,\n",
       " '생기': 856,\n",
       " '간다': 857,\n",
       " '당황': 858,\n",
       " '어떻': 859,\n",
       " '기본': 860,\n",
       " '짓': 861,\n",
       " '올려': 862,\n",
       " '몰랐': 863,\n",
       " '짐': 864,\n",
       " '당당': 865,\n",
       " '피해': 866,\n",
       " '돌아오': 867,\n",
       " '완벽': 868,\n",
       " '해결': 869,\n",
       " '최선': 870,\n",
       " '못한': 871,\n",
       " '그런가': 872,\n",
       " '대요': 873,\n",
       " '구불구불': 874,\n",
       " '개': 875,\n",
       " '어색': 876,\n",
       " '누가': 877,\n",
       " '사과': 878,\n",
       " '꾸준히': 879,\n",
       " '어려운': 880,\n",
       " '려면': 881,\n",
       " '자존': 882,\n",
       " '지우': 883,\n",
       " '부탁': 884,\n",
       " '어느덧': 885,\n",
       " '됩니다': 886,\n",
       " '절대': 887,\n",
       " '놓아주': 888,\n",
       " '오자키': 889,\n",
       " '협조': 890,\n",
       " '그때': 891,\n",
       " '장난': 892,\n",
       " '더라': 893,\n",
       " '기다릴': 894,\n",
       " '커플': 895,\n",
       " '자는': 896,\n",
       " '적응': 897,\n",
       " '살짝': 898,\n",
       " '화장실': 899,\n",
       " '충전': 900,\n",
       " '타이밍': 901,\n",
       " '경우': 902,\n",
       " '두려워': 903,\n",
       " '적극': 904,\n",
       " '수많': 905,\n",
       " '의사': 906,\n",
       " '치': 907,\n",
       " '군대': 908,\n",
       " '상': 909,\n",
       " '발표': 910,\n",
       " '곁': 911,\n",
       " '높': 912,\n",
       " '분간': 913,\n",
       " '그건': 914,\n",
       " '바뀌': 915,\n",
       " '본': 916,\n",
       " '고치': 917,\n",
       " '크리스마스': 918,\n",
       " '단': 919,\n",
       " '잠수': 920,\n",
       " '원래': 921,\n",
       " '일상': 922,\n",
       " '병원': 923,\n",
       " '위험': 924,\n",
       " '오랜만': 925,\n",
       " '알바': 926,\n",
       " '이기': 927,\n",
       " '샀': 928,\n",
       " '데려다': 929,\n",
       " '무': 930,\n",
       " '늘': 931,\n",
       " '복': 932,\n",
       " '꺼': 933,\n",
       " '눈치': 934,\n",
       " '할게요': 935,\n",
       " '나눠': 936,\n",
       " '됨': 937,\n",
       " '당했': 938,\n",
       " '꽃': 939,\n",
       " '떨려': 940,\n",
       " '거리': 941,\n",
       " '자리': 942,\n",
       " '한잔': 943,\n",
       " '질투': 944,\n",
       " '언젠가': 945,\n",
       " '학원': 946,\n",
       " '청소': 947,\n",
       " '감사': 948,\n",
       " '배려': 949,\n",
       " '무서워': 950,\n",
       " '걷': 951,\n",
       " '키': 952,\n",
       " '열': 953,\n",
       " '새벽': 954,\n",
       " '없애': 955,\n",
       " '연예인': 956,\n",
       " '아빠': 957,\n",
       " '출근': 958,\n",
       " '악몽': 959,\n",
       " '안경': 960,\n",
       " '이거': 961,\n",
       " '취업': 962,\n",
       " '당연': 963,\n",
       " '귀엽': 964,\n",
       " '조절': 965,\n",
       " '한동안': 966,\n",
       " '장거리': 967,\n",
       " '사실': 968,\n",
       " '견디': 969,\n",
       " '착각': 970,\n",
       " '는다는': 971,\n",
       " '흔들리': 972,\n",
       " '경험': 973,\n",
       " '인가요': 974,\n",
       " '각기': 975,\n",
       " '헤아리': 976,\n",
       " '기운': 977,\n",
       " '막': 978,\n",
       " '생': 979,\n",
       " '땐': 980,\n",
       " '그래': 981,\n",
       " '오빠': 982,\n",
       " '휴가': 983,\n",
       " '집안일': 984,\n",
       " '느껴': 985,\n",
       " '이름': 986,\n",
       " '힘든가': 987,\n",
       " '노': 988,\n",
       " '무섭': 989,\n",
       " '웨딩': 990,\n",
       " '피부': 991,\n",
       " '점심': 992,\n",
       " '적당히': 993,\n",
       " '위한': 994,\n",
       " '완전히': 995,\n",
       " '이용': 996,\n",
       " '발전': 997,\n",
       " '넘겨받': 998,\n",
       " '맘고생': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 사전 생성\n",
    "from collections import Counter\n",
    "\n",
    "voc_data = que_corpus + ans_corpus\n",
    "\n",
    "words = np.concatenate(voc_data).tolist()\n",
    "counter = Counter(words)\n",
    "counter = counter.most_common(30000-2)\n",
    "vocab = ['<pad>', '<unk>'] + [key for key, _ in counter]\n",
    "word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31848\n",
      "31848\n",
      "[2539, 182, 4106, 91]\n",
      "[3, 271, 10, 157, 10, 39, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index[word] if word in word_to_index else word_to_index['<unk>'] for word in sentence]\n",
    "\n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<unk>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "def vectorize(corpus, word_to_index):\n",
    "    data = []\n",
    "    for sen in corpus:\n",
    "        sen = get_encoded_sentence(sen, word_to_index)\n",
    "        data.append(sen)\n",
    "    return data\n",
    "\n",
    "que_train = vectorize(que_corpus, word_to_index)\n",
    "ans_train = vectorize(ans_corpus, word_to_index)\n",
    "\n",
    "print(len(que_train))\n",
    "print(len(ans_train))\n",
    "print(que_train[0])\n",
    "print(ans_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31529 319 31529 319\n"
     ]
    }
   ],
   "source": [
    "enc_tensor = tf.keras.preprocessing.sequence.pad_sequences(que_train, padding='post')\n",
    "dec_tensor = tf.keras.preprocessing.sequence.pad_sequences(ans_train, padding='post')\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(enc_tensor, dec_tensor, test_size=0.01) # test set은 1%만\n",
    "\n",
    "print(len(enc_train), len(enc_val), len(dec_train), len(dec_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 231,  218,    7,   41,   33,  113,   63,   10, 1746,  190,   27,\n",
       "        187,  296,   37,    7, 1781,   24,  144,    2,    0], dtype=int32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3,   38,  342,   27, 1746,   18, 3617,    5,   37,   31,    2,\n",
       "          4,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train\n",
    "* 위 실습의 Transformer 그대로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 하이퍼파라미터로 Transformer 인스턴스 생성\n",
    "VOCAB_SIZE = 20000\n",
    "\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate 인스턴스 선언 & Optimizer 구현\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 정의\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]\n",
    "    gold = tgt[:, 1:]\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e1f6ac2f594f0994d0e88647dcd6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea48abf8ebc4310aaf1086620d717ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f06935e7a754cbda9022fef67839685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a73d161aa749c096c30643e4efb7d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517243c172104fa6bd8c3c166cba7d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7b683dbacb4f8c87b67a5e75b27d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3726ae4858464392f16435ddd8bd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ebbbccaa0544b5adccf319b95a7d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65dac785bbcf4b69b4e668ddf51b69a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770236c754c04564ac2ad277deab12ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f5bfce2a47433d903eb34dc4085527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa68e1da2f1d4dd5b7b4d8e69ebc1db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc419cdfe3684216b66f16dd51f8e49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae26d525d6c84a8c847205b28b310de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47115b2c6b94433abea375aaaf504a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bd1d1923664243aef8cf1e21ce277c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91dcf03cedc340d5822b0b481242dbd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4cf3a1ccca740bd948dd6985b42faa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb443a0d8974c719a70655a1590da17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e76f809e9f4f46bc11e3f1928bab2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc30ced0bac244949f6400e372d136f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02f475190d94669b2a3988acdd31996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a420dff630b41f094f4341abc984329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8743c1f2faa549648ce14e69510a9594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c880f409ef412ba85864a25b6f2c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16362518e24b4884ba8eb52fa311a54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ffb8e15f77f45498f036a14f7e641d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29417996345e47ca93da52fa7e5d60c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccdf278e1c5a4701bd2a1d01032464ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f22bfc47634526aaf61ba198a9e929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=493.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 훈련시키기\n",
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate()\n",
    "\n",
    "def evaluate(sentence, model):\n",
    "    mecab = Mecab()\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    pieces = mecab.morphs(sentence)\n",
    "    \n",
    "    tokens = []\n",
    "    for sen in pieces:\n",
    "        sen= get_encoded_sentence(sen, word_to_index)\n",
    "        tokens.append(sen)\n",
    "    \n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences(tokens,\n",
    "                                                        value=word_to_index[\"<pad>\"],\n",
    "                                                        padding='pre',\n",
    "                                                        maxlen=20)\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([word_to_index[\"<start>\"]], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if word_to_index[\"<end>\"] == predicted_id:\n",
    "            result = get_decoded_sentence(ids, index_to_word)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = get_decoded_sentence(ids, index_to_word)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "def translate(sentence, model):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 지루하다, 놀러가고 싶어.\n",
      "answered: 정말 참 아 했 던 만큼 더 나 봐요 는데\n",
      "source: 오늘 일찍 일어났더니 피곤하다.\n",
      "answered: 만 더 버텨 보 세요 .\n",
      "source: 간만에 여자친구랑 데이트 하기로 했어.\n",
      "answered: 하 거나 직접 만나 보 세요 .\n",
      "source: 집에 있는다는 소리야.\n",
      "answered: 하 고 출퇴근 하 고 싶 네요 .\n"
     ]
    }
   ],
   "source": [
    "sample_sentences = [\n",
    "    \"지루하다, 놀러가고 싶어.\",\n",
    "    \"오늘 일찍 일어났더니 피곤하다.\",\n",
    "    \"간만에 여자친구랑 데이트 하기로 했어.\",\n",
    "    \"집에 있는다는 소리야.\",\n",
    "]\n",
    "\n",
    "for sentence in sample_sentences:\n",
    "    print(\"source:\", sentence)\n",
    "    print(\"answered:\", translate(sentence, transformer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
