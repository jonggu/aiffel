{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15-10. 프로젝트: 단어 level로 번역기 업그레이드하기\n",
    "* 데이터에서 상위 33000개의 샘플만 사용할 것\n",
    "* 33000개 중 3000개는 테스트 데이터로 분리할 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 정제, 정규화, 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 178009\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fire!</td>\n",
       "      <td>Au feu !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Help!</td>\n",
       "      <td>À l'aide !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jump.</td>\n",
       "      <td>Saute.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     eng         fra\n",
       "0    Go.        Va !\n",
       "1    Hi.     Salut !\n",
       "2    Hi.      Salut.\n",
       "3   Run!     Cours !\n",
       "4   Run!    Courez !\n",
       "5   Who?       Qui ?\n",
       "6   Wow!  Ça alors !\n",
       "7  Fire!    Au feu !\n",
       "8  Help!  À l'aide !\n",
       "9  Jump.      Saute."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파일 불러오기\n",
    "file_path = 'data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "\n",
    "# 세 번째 열 제거하고, 33000개만 사용\n",
    "lines = lines[['eng', 'fra']][:33000]\n",
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go .</td>\n",
       "      <td>va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hi .</td>\n",
       "      <td>salut !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hi .</td>\n",
       "      <td>salut .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>run !</td>\n",
       "      <td>cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>run !</td>\n",
       "      <td>courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>who ?</td>\n",
       "      <td>qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wow !</td>\n",
       "      <td>a alors !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fire !</td>\n",
       "      <td>au feu !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>help !</td>\n",
       "      <td>l aide !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jump .</td>\n",
       "      <td>saute .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       eng          fra\n",
       "0    go .         va ! \n",
       "1    hi .      salut ! \n",
       "2    hi .      salut . \n",
       "3   run !      cours ! \n",
       "4   run !     courez ! \n",
       "5   who ?        qui ? \n",
       "6   wow !    a alors ! \n",
       "7  fire !     au feu ! \n",
       "8  help !     l aide ! \n",
       "9  jump .      saute . "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전처리 1: 구두점을 단어와 분리 (. / , / ! / ?)\n",
    "# 전처리 2: 소문자로 변환\n",
    "for idx, val in enumerate(lines['eng']):\n",
    "    lines['eng'][idx] = val.lower()\n",
    "    # lines['eng'][idx] = lines['eng'][idx].replace('.', ' .').replace('?', ' ?').replace('!', ' !').replace(',', ' ,')\n",
    "    lines['eng'][idx] = re.sub(r\"([?.!,¿])\", r\" \\1 \", lines['eng'][idx])\n",
    "    lines['eng'][idx] = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", lines['eng'][idx])\n",
    "    lines['eng'][idx] = re.sub(r\"\\s+\", r\" \", lines['eng'][idx])\n",
    "    \n",
    "for idx, val in enumerate(lines['fra']):\n",
    "    lines['fra'][idx] = val.lower()\n",
    "    # lines['fra'][idx] = lines['fra'][idx].replace('.', ' .').replace('?', ' ?').replace('!', ' !').replace(',', ' ,')\n",
    "    lines['fra'][idx] = re.sub(r\"([?.!,¿])\", r\" \\1 \", lines['fra'][idx])\n",
    "    lines['fra'][idx] = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", lines['fra'][idx])\n",
    "    lines['fra'][idx] = re.sub(r\"\\s+\", r\" \", lines['fra'][idx])\n",
    "    \n",
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 디코더의 문장에 시작 토큰과 종료 토큰 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26739</th>\n",
       "      <td>tom drank lemonade .</td>\n",
       "      <td>&lt;sos&gt; tom a bu de la limonade .  &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20164</th>\n",
       "      <td>is that your wife ?</td>\n",
       "      <td>&lt;sos&gt; est ce votre femme ?  &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4143</th>\n",
       "      <td>it was clean .</td>\n",
       "      <td>&lt;sos&gt; ce fut net .  &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28845</th>\n",
       "      <td>hand in your papers .</td>\n",
       "      <td>&lt;sos&gt; remettez leur vos papiers .  &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22465</th>\n",
       "      <td>you did well tom .</td>\n",
       "      <td>&lt;sos&gt; vous avez bien fait tom .  &lt;eos&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          eng                                       fra\n",
       "26739   tom drank lemonade .     <sos> tom a bu de la limonade .  <eos>\n",
       "20164    is that your wife ?          <sos> est ce votre femme ?  <eos>\n",
       "4143          it was clean .                  <sos> ce fut net .  <eos>\n",
       "28845  hand in your papers .   <sos> remettez leur vos papiers .  <eos>\n",
       "22465     you did well tom .     <sos> vous avez bien fait tom .  <eos>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_token = '<sos>'\n",
    "eos_token = '<eos>'\n",
    "lines.fra = lines.fra.apply(lambda x : sos_token + ' '+ x + ' ' + eos_token)\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 케라스의 토크나이저로 텍스트를 숫자로 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[30, 1], [1132, 1], [1132, 1], [260, 32], [260, 32]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 단위로 토크나이저 생성 후 숫자로 변환\n",
    "\n",
    "# eng\n",
    "eng_tokenizer = Tokenizer(filters=\"\", lower=False) # num_words=7000\n",
    "eng_tokenizer.fit_on_texts(lines.eng)\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)\n",
    "input_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 91, 12, 2],\n",
       " [1, 1068, 12, 2],\n",
       " [1, 1068, 3, 2],\n",
       " [1, 928, 12, 2],\n",
       " [1, 2190, 12, 2]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fra\n",
    "fra_tokenizer = Tokenizer(filters=\"\", lower=False) # num_words=7000\n",
    "fra_tokenizer.fit_on_texts(lines.fra)\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)\n",
    "target_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 4662\n",
      "프랑스어 단어장의 크기 : 7326\n"
     ]
    }
   ],
   "source": [
    "# 단어장 크기 저장\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 91, 12], [1, 1068, 12], [1, 1068, 3]]\n",
      "[[91, 12, 2], [1068, 12, 2], [1068, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰, 종료 토큰 제거\n",
    "encoder_input = input_text\n",
    "\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text]\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]\n",
    "\n",
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 8\n",
      "프랑스어 시퀀스의 최대 길이 17\n"
     ]
    }
   ],
   "source": [
    "# 최대 길이\n",
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 8)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 17)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 17)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_to_index = eng_tokenizer.word_index\n",
    "index_to_eng = eng_tokenizer.index_word\n",
    "\n",
    "fra_to_index = fra_tokenizer.word_index\n",
    "index_to_fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30  1  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 91 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91 12  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)\n",
    "\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23575  7933  4082 ... 14577  1853  5291]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (30000, 8)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (30000, 17)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (30000, 17)\n",
      "영어 테스트데이터의 크기(shape) : (3000, 8)\n",
      "프랑스어 테스트 입력데이터의 크기(shape) : (3000, 17)\n",
      "프랑스어 테스트 출력데이터의 크기(shape) : (3000, 17)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input_train))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input_train))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target_train))\n",
    "\n",
    "print('영어 테스트데이터의 크기(shape) :',np.shape(encoder_input_test))\n",
    "print('프랑스어 테스트 입력데이터의 크기(shape) :',np.shape(decoder_input_test))\n",
    "print('프랑스어 테스트 출력데이터의 크기(shape) :',np.shape(decoder_target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 임베딩 층 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(eng_vocab_size, latent_dim)(encoder_inputs) # 임베딩 층\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True) # 상태값 리턴을 위해 return_state는 True\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n",
    "encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(fra_vocab_size, latent_dim) # 임베딩 층\n",
    "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 128)    596736      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    937728      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 128)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 128)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 128), (None, 131584      masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 128),  131584      masking_1[0][0]                  \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 7326)   945054      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,742,686\n",
      "Trainable params: 2,742,686\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "59/59 [==============================] - 7s 124ms/step - loss: 3.3711 - acc: 0.6023 - val_loss: 2.1646 - val_acc: 0.6131\n",
      "Epoch 2/50\n",
      "59/59 [==============================] - 6s 104ms/step - loss: 1.9858 - acc: 0.6480 - val_loss: 1.8417 - val_acc: 0.6773\n",
      "Epoch 3/50\n",
      "59/59 [==============================] - 6s 104ms/step - loss: 1.7521 - acc: 0.7260 - val_loss: 1.6758 - val_acc: 0.7356\n",
      "Epoch 4/50\n",
      "59/59 [==============================] - 6s 104ms/step - loss: 1.6251 - acc: 0.7389 - val_loss: 1.5910 - val_acc: 0.7438\n",
      "Epoch 5/50\n",
      "59/59 [==============================] - 6s 104ms/step - loss: 1.5500 - acc: 0.7440 - val_loss: 1.5295 - val_acc: 0.7461\n",
      "Epoch 6/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 1.4874 - acc: 0.7488 - val_loss: 1.4746 - val_acc: 0.7534\n",
      "Epoch 7/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 1.4233 - acc: 0.7580 - val_loss: 1.4183 - val_acc: 0.7631\n",
      "Epoch 8/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 1.3561 - acc: 0.7708 - val_loss: 1.3631 - val_acc: 0.7734\n",
      "Epoch 9/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 1.3007 - acc: 0.7793 - val_loss: 1.3105 - val_acc: 0.7816\n",
      "Epoch 10/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 1.2511 - acc: 0.7863 - val_loss: 1.2689 - val_acc: 0.7894\n",
      "Epoch 11/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 1.2045 - acc: 0.7931 - val_loss: 1.2317 - val_acc: 0.7958\n",
      "Epoch 12/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 1.1624 - acc: 0.7986 - val_loss: 1.1974 - val_acc: 0.7995\n",
      "Epoch 13/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 1.1238 - acc: 0.8036 - val_loss: 1.1646 - val_acc: 0.8034\n",
      "Epoch 14/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 1.0886 - acc: 0.8080 - val_loss: 1.1352 - val_acc: 0.8073\n",
      "Epoch 15/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 1.0544 - acc: 0.8119 - val_loss: 1.1075 - val_acc: 0.8112\n",
      "Epoch 16/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 1.0216 - acc: 0.8162 - val_loss: 1.0797 - val_acc: 0.8153\n",
      "Epoch 17/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.9901 - acc: 0.8204 - val_loss: 1.0563 - val_acc: 0.8175\n",
      "Epoch 18/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 0.9608 - acc: 0.8242 - val_loss: 1.0343 - val_acc: 0.8214\n",
      "Epoch 19/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.9323 - acc: 0.8277 - val_loss: 1.0130 - val_acc: 0.8248\n",
      "Epoch 20/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 0.9066 - acc: 0.8315 - val_loss: 0.9956 - val_acc: 0.8266\n",
      "Epoch 21/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.8815 - acc: 0.8345 - val_loss: 0.9802 - val_acc: 0.8291\n",
      "Epoch 22/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 0.8582 - acc: 0.8374 - val_loss: 0.9630 - val_acc: 0.8307\n",
      "Epoch 23/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 0.8364 - acc: 0.8402 - val_loss: 0.9486 - val_acc: 0.8319\n",
      "Epoch 24/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.8149 - acc: 0.8429 - val_loss: 0.9348 - val_acc: 0.8337\n",
      "Epoch 25/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 0.7941 - acc: 0.8457 - val_loss: 0.9157 - val_acc: 0.8365\n",
      "Epoch 26/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.7742 - acc: 0.8482 - val_loss: 0.9050 - val_acc: 0.8382\n",
      "Epoch 27/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 0.7554 - acc: 0.8506 - val_loss: 0.8912 - val_acc: 0.8405\n",
      "Epoch 28/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 0.7366 - acc: 0.8532 - val_loss: 0.8836 - val_acc: 0.8403\n",
      "Epoch 29/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.7187 - acc: 0.8558 - val_loss: 0.8685 - val_acc: 0.8438\n",
      "Epoch 30/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.7008 - acc: 0.8583 - val_loss: 0.8551 - val_acc: 0.8456\n",
      "Epoch 31/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.6839 - acc: 0.8607 - val_loss: 0.8471 - val_acc: 0.8467\n",
      "Epoch 32/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.6677 - acc: 0.8630 - val_loss: 0.8352 - val_acc: 0.8479\n",
      "Epoch 33/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.6509 - acc: 0.8656 - val_loss: 0.8280 - val_acc: 0.8485\n",
      "Epoch 34/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 0.6360 - acc: 0.8676 - val_loss: 0.8184 - val_acc: 0.8507\n",
      "Epoch 35/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 0.6201 - acc: 0.8704 - val_loss: 0.8076 - val_acc: 0.8515\n",
      "Epoch 36/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.6059 - acc: 0.8725 - val_loss: 0.7988 - val_acc: 0.8538\n",
      "Epoch 37/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 0.5906 - acc: 0.8748 - val_loss: 0.7947 - val_acc: 0.8533\n",
      "Epoch 38/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.5767 - acc: 0.8770 - val_loss: 0.7874 - val_acc: 0.8545\n",
      "Epoch 39/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.5630 - acc: 0.8793 - val_loss: 0.7831 - val_acc: 0.8550\n",
      "Epoch 40/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.5492 - acc: 0.8816 - val_loss: 0.7697 - val_acc: 0.8572\n",
      "Epoch 41/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.5359 - acc: 0.8841 - val_loss: 0.7602 - val_acc: 0.8595\n",
      "Epoch 42/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 0.5227 - acc: 0.8864 - val_loss: 0.7609 - val_acc: 0.8588\n",
      "Epoch 43/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.5103 - acc: 0.8883 - val_loss: 0.7549 - val_acc: 0.8593\n",
      "Epoch 44/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.4977 - acc: 0.8908 - val_loss: 0.7424 - val_acc: 0.8621\n",
      "Epoch 45/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.4857 - acc: 0.8927 - val_loss: 0.7446 - val_acc: 0.8608\n",
      "Epoch 46/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.4738 - acc: 0.8951 - val_loss: 0.7358 - val_acc: 0.8630\n",
      "Epoch 47/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 0.4623 - acc: 0.8972 - val_loss: 0.7272 - val_acc: 0.8644\n",
      "Epoch 48/50\n",
      "59/59 [==============================] - 6s 105ms/step - loss: 0.4512 - acc: 0.8992 - val_loss: 0.7227 - val_acc: 0.8648\n",
      "Epoch 49/50\n",
      "59/59 [==============================] - 6s 106ms/step - loss: 0.4398 - acc: 0.9013 - val_loss: 0.7189 - val_acc: 0.8656\n",
      "Epoch 50/50\n",
      "59/59 [==============================] - 6s 107ms/step - loss: 0.4295 - acc: 0.9031 - val_loss: 0.7123 - val_acc: 0.8665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff780cc8510>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=512, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 128)         596736    \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 128), (None, 128) 131584    \n",
      "=================================================================\n",
      "Total params: 728,320\n",
      "Trainable params: 728,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    937728      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 128),  131584      embedding_1[1][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 7326)   945054      lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,014,366\n",
      "Trainable params: 2,014,366\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, \n",
    "                      outputs=[decoder_outputs2] + decoder_states2)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 정수 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = fra_to_index['<sos>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 단어로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_fra[sampled_token_index]\n",
    "\n",
    "         # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<eos>' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2eng(input_seq):\n",
    "    temp = ''\n",
    "    for i in input_seq :\n",
    "        if(i!=0):\n",
    "            temp = temp + index_to_eng[i] + ' '\n",
    "    return temp\n",
    "\n",
    "def seq2fra(input_seq):\n",
    "    temp = ''\n",
    "    for i in input_seq:\n",
    "        if ((i!=0 and i!=fra_to_index['<sos>']) and i!=fra_to_index['<eos>']):\n",
    "            temp = temp + index_to_fra[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장:  don t go in there . \n",
      "정답 문장:  ne rentre pas l dedans . \n",
      "번역기가 번역한 문장:   ne le d p te pas ! \n",
      "-----------------------------------\n",
      "입력 문장:  i owe him my life . \n",
      "정답 문장:  je lui dois la vie . \n",
      "번역기가 번역한 문장:   je me dois une faveur . \n",
      "-----------------------------------\n",
      "입력 문장:  we ve gone too far . \n",
      "정답 문장:  nous sommes all es trop loin . \n",
      "번역기가 번역한 문장:   nous sommes all es trop loin . \n",
      "-----------------------------------\n",
      "입력 문장:  i remember her . \n",
      "정답 문장:  je me souviens d elle . \n",
      "번역기가 번역한 문장:   je me souviens de le dire . \n",
      "-----------------------------------\n",
      "입력 문장:  come on grow up . \n",
      "정답 문장:  enfin arr tez d agir comme des enfants ! \n",
      "번역기가 번역한 문장:   reviens ici ! \n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001]:\n",
    "    input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    \n",
    "    print(35 * \"-\")\n",
    "    print(\"입력 문장: \", seq2eng(encoder_input_train[seq_index]))\n",
    "    print(\"정답 문장: \", seq2fra(decoder_input_train[seq_index]))\n",
    "    print(\"번역기가 번역한 문장: \", decoded_sentence[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
