{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E28. 이미지 이상탐지모델 (GAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from scipy.interpolate import interp1d\n",
    "from inspect import signature\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection with GAN\n",
    "* AnoGAN: GAN을 이용해 Anomaly 데이터가 부족한 상황에서도 unsupervised 방식으로 Anomaly Detection 모델 구현이 가능하다는 아이디어\n",
    "    * https://arxiv.org/pdf/1703.05921.pdf\n",
    "        - 학습 데이터로 Anomal하지 않은 정상 (normal) 데이터만 사용\n",
    "        - GAN의 원리에 의해 Discriminator는 정상적인 이미지와 (Generator가 만들어낸) 가짜 이미지를 잘 구분하게 될 것이고,\n",
    "        - Generator는 그런 Discriminator를 속이기 위해 정상 데이터같은 이미지를 만들어낼 것\n",
    "        - AnoGAN \"잘 훈련된 GAN의 Generator $x = G(z)$ 란, latent variable z와 이미지 x 사이의 어떤 맵핑을 나타내는 함수가 아닐까? 그렇다면 Normal 데이터를 잘 만들어낼 수 있는 z의 영역을 (z를 수없이 샘플링해 가면서) 더듬더듬 찾아내면 논문의 Fig.2. (b) 와 같은 윤곽이 얻어지게 될 수 있지 않을까?\"\n",
    "        - 와 같은 생각에서 두 가지 추가적인 loss를 정의\n",
    "            - Residual Loss: $\\mathcal{L}_R(z_{\\gamma}) = \\sum |x - G(z_{\\gamma})|$\n",
    "            - Discrimination Loss: $\\mathcal{L}_D(z_{\\gamma}) = \\sum |f(x) - f(G(z_{\\gamma}))|$\n",
    "            - (f: discriminator의 중간 레이어)\n",
    "        - 최종적으로, 다음 loss를 최소화하도록 (파라미터를 찾는 게 아니라) z를 찾아나가는 것: $\\mathcal{L}(z_{\\gamma}) = (1-\\lambda)\\mathcal{L}_R(z_{\\gamma}) + \\lambda\\mathcal{L}_D(z_{\\gamma})$\n",
    "* EGBAD (feat. by BiGAN)\n",
    "    - AnoGAN의 단점: z의 latent space를 직접 찾겠다는 것은 많은 계산량을 요구\n",
    "    - latent space를 직접 모델링하는 형태의 GAN 모델이 존재: BiGAN\n",
    "    - BiGAN에는 Generator와 Discriminator 이외에도 G의 역함수 구실을 하는 E 네트워크가 존재 (=Encoder)\n",
    "    - 따라서, Discriminator는 x와 G(z) 를 비교하는 게 아니라 (x, E(x)) 와 (G(z), z) 를 비교하는 형태로 구성됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GANomaly\n",
    "* https://arxiv.org/pdf/1805.06725.pdf\n",
    "    - AnoGAN과 BiGAN Encoder에서 기본 아이디어를 가져와 잘 결합한 뒤, Encoder-Decoder-Encoder 구조의 네트워크로 다시 묶어냄\n",
    "    - GANomaly의 Generator는 일반적인 GAN과 달리 Encoder-Decoder 구조이며, 입력값이 (latent variable) z가 아니라 이미지 x\n",
    "    - 다음 3가지 loss를 최소화하는 형태로 학습 진행\n",
    "        - Adversarial Loss: $\\mathcal{L}_{adv} = \\mathbb{E}_{x \\sim p_X}\\lVert f(x) - \\mathbb{E}_{x \\sim p_X}f(F(x)) \\rVert_2$\n",
    "        - Contextual Loss: $\\mathcal{L}_{con} = \\mathbb{E}_{x \\sim p_X}\\lVert x - G(x) \\rVert_1$\n",
    "        - Encoder Loss: $ \\mathcal{L}_{enc} = \\mathbb{E}_{x \\sim p_X}\\lVert G_E(x) - E(G(x)) \\rVert_2$\n",
    "    - 전체적인 loss: $\\mathcal{L} = w_{adv}\\mathcal{L}_{adv} + w_{con}\\mathcal{L}_{con} + w_{enc}\\mathcal{L}_{enc}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-GANomaly\n",
    "* https://arxiv.org/pdf/1901.08954.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 구성\n",
    "* Fashion-MNIST: https://github.com/zalandoresearch/fashion-mnist\n",
    "    - 1채널 grayscale 데이터셋이므로 그냥은 convolution 연산이 되지 않음\n",
    "        - 채널방향 차원이 하나 늘어나도록 reshape하는 과정 필요\n",
    "    - UNet 구조의 활용을 위해서 기존의 28 X 28 사이즈의 Fashion-MNIST 데이터 이미지를 32 X 32 로 패딩처리해 줄 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "train_data = (train_data - 127.5) / 127.5\n",
    "test_data = (test_data - 127.5) / 127.5\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Fashion MNIST padding to 32 X 32\n",
    "train_data_32 = np.zeros((train_data.shape[0], 32, 32)).astype('float32')\n",
    "test_data_32 = np.zeros((test_data.shape[0], 32, 32)).astype('float32')     \n",
    "train_data_32[:, 2:30, 2:30] = train_data\n",
    "test_data_32[:, 2:30, 2:30] = test_data\n",
    "\n",
    "# 1channel data reshape\n",
    "train_data = train_data_32.reshape(train_data_32.shape[0], 32, 32, 1).astype('float32')\n",
    "test_data = test_data_32.reshape(test_data_32.shape[0], 32, 32, 1).astype('float32')\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 확인\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(train_data[i].reshape(32, 32), cmap='gray')\n",
    "    plt.title(f'index: {i}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상 데이터 (8번, bag) 제외\n",
    "def set_labels(labels):\n",
    "    new_t_labels = []\n",
    "    for old_label in labels:\n",
    "        if old_label == 8:   # Bag:8\n",
    "            new_t_labels.append([0])  # Bag을 이상치로 처리\n",
    "        else:\n",
    "            new_t_labels.append([1])  # 그 외의 경우는 정상치\n",
    "             \n",
    "    return np.array(new_t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bol_train_labels = set_labels(train_labels)\n",
    "bol_test_labels = set_labels(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 60000건의 훈련데이터 중 6000건이 제외되어 10000건의 테스트 데이터에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "normal_labels = []\n",
    "anomaly_data = []\n",
    "anomaly_labels = []\n",
    "for data, label in zip(train_data, bol_train_labels):\n",
    "    if label == 0:\n",
    "        anomaly_data.append(data)\n",
    "        anomaly_labels.append(label)\n",
    "    else:\n",
    "        normal_data.append(data)\n",
    "        normal_labels.append(label)\n",
    "        \n",
    "normal_data = np.array(normal_data)\n",
    "normal_labels = np.array(normal_labels)\n",
    "anomaly_data = np.array(anomaly_data)\n",
    "anomaly_labels = np.array(anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normal_data.shape, normal_labels.shape)\n",
    "print(anomaly_data.shape, anomaly_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = normal_data\n",
    "bol_train_labels = normal_labels\n",
    "test_data = tf.concat([test_data, anomaly_data], 0)\n",
    "bol_test_labels = tf.concat([bol_test_labels, anomaly_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bol_train_labels.shape)\n",
    "print(bol_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋이 정확하게 구성되었는지 검증\n",
    "for label in bol_train_labels:\n",
    "    if label == 0:\n",
    "        print(label)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 구성\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, bol_train_labels))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, bol_test_labels))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 검증\n",
    "# 훈련 데이터셋에는 라벨이 1인 데이터만 존재하고, 테스트 데이터에는 0과 1이 섞여 있어야 합니다.\n",
    "\n",
    "for data, label in train_dataset.take(1): # 훈련 데이터셋\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, label in test_dataset.take(1): # 테스트 데이터셋\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델과 loss 함수 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "* UNet 구조를 따릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv_layer = tf.keras.Sequential([\n",
    "            layers.Conv2D(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                          kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.conv_layer(inputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_T_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_T_block, self).__init__()\n",
    "        self.conv_T_layer = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                                   kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, concat, training=False):\n",
    "        upsample = self.conv_T_layer(inputs)\n",
    "        outputs = tf.concat([upsample, concat], -1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, num_output_channel=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(512) # 1\n",
    "        \n",
    "        self.decoder_4 = Conv_T_block(512) # 2\n",
    "        self.decoder_3 = Conv_T_block(256) # 4\n",
    "        self.decoder_2 = Conv_T_block(128) # 8\n",
    "        self.decoder_1 = Conv_T_block(64) # 16\n",
    "        \n",
    "        self.output_layer = layers.Conv2DTranspose(num_output_channel, 1, strides=2, padding='same', use_bias=False, # 32\n",
    "                                                   kernel_initializer=tf.random_normal_initializer(0., 0.02))\n",
    "                \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # gen\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        de_4 = self.decoder_4(center, en_4)\n",
    "        de_3 = self.decoder_3(de_4, en_3)\n",
    "        de_2 = self.decoder_2(de_3, en_2)\n",
    "        de_1 = self.decoder_1(de_2, en_1)\n",
    "        \n",
    "        outputs = self.output_layer(de_1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "* Generator처럼 Conv_block을 활용하며, 최종적으로 sigmoid를 거쳐 0~1 사이의 숫자를 리턴합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(100) # 1\n",
    "        \n",
    "        self.outputs = layers.Conv2D(1, 3, strides=1, padding='same',\n",
    "                                          use_bias=False, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # dis\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        outputs = self.outputs(center)\n",
    "        \n",
    "        return outputs, center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator와 Discriminator를 합해 전체 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(num_output_channel=1)\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss 함수\n",
    "* GAN 모델의 핵심은 loss 함수의 구성 방법에 달려 있습니다.\n",
    "* Skip-GANomaly는 이전 모델들과 달리 일반적인 GAN의 학습 절차와 같은 형태의 Loss 구성이 진행됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = tf.keras.losses.MeanSquaredError()\n",
    "l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(pred_real, pred_fake):\n",
    "    real_loss = cross_entropy(tf.ones_like(pred_real), pred_real)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(pred_fake), pred_fake)\n",
    "    \n",
    "    total_dis_loss = (real_loss + fake_loss) * 0.5\n",
    "    \n",
    "    # return tf.cast(total_dis_loss, tf.float32)\n",
    "    return total_dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(real_output, fake_output, input_data, gen_data, latent_first, latent_sec):\n",
    "    w_adv = 1.\n",
    "    w_context = 40.\n",
    "    w_encoder = 1.\n",
    "    \n",
    "    adv_loss = cross_entropy(real_output, fake_output)\n",
    "    context_loss = l1_loss(input_data, gen_data)\n",
    "    encoder_loss = l2_loss(latent_first, latent_sec)\n",
    "    \n",
    "    total_gen_loss = w_adv * adv_loss + \\\n",
    "                     w_context * context_loss + \\\n",
    "                     w_encoder * encoder_loss\n",
    "    \n",
    "    # return tf.cast(total_gen_loss, tf.float32)\n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 설정\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습과 평가\n",
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(images, training=True)\n",
    "        \n",
    "        pred_real, feat_real = discriminator(images, training=True)\n",
    "        pred_fake, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(pred_real, pred_fake,\n",
    "                                  images, generated_images,\n",
    "                                  feat_real, feat_fake)\n",
    "\n",
    "        disc_loss = discriminator_loss(pred_real, pred_fake)        \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'ganomaly_skip_no_norm/ckpt'\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1시간 이상 소요\n",
    "\n",
    "max_epochs = 25\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        steps += 1\n",
    "        gen_loss, disc_loss = train_step(images)\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print ('Steps : {}, \\t Total Gen Loss : {}, \\t Total Dis Loss : {}'.format(steps, gen_loss.numpy(), disc_loss.numpy()))\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_path)\n",
    "        \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 도중 저장된 체크포인트 활용\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(test_dataset, set_lambda=0.9):\n",
    "    an_scores = []\n",
    "    gt_labels = []\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(test_dataset):\n",
    "        generated_images = generator(x_batch_train, training=True)\n",
    "        _, feat_real = discriminator(x_batch_train, training=True)\n",
    "        _, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        generated_images, feat_real, feat_fake = generated_images.numpy(), feat_real.numpy(), feat_fake.numpy()        \n",
    "\n",
    "        rec = abs(x_batch_train - generated_images)\n",
    "        lat = (feat_real - feat_fake) ** 2\n",
    "\n",
    "        rec = tf.reduce_sum(rec, [1,2,3])\n",
    "        lat = tf.reduce_sum(lat, [1,2,3])\n",
    "        \n",
    "        error = (set_lambda * tf.cast(rec, tf.float32)) + ((1 - set_lambda) * tf.cast(lat, tf.float32))\n",
    "        \n",
    "        an_scores.append(error)\n",
    "        gt_labels.append(y_batch_train)\n",
    "        \n",
    "    an_scores = np.concatenate(an_scores, axis=0).reshape([-1])\n",
    "    gt_labels = np.concatenate(gt_labels, axis=0).reshape([-1])\n",
    "    \n",
    "    an_scores = (an_scores - np.amin(an_scores)) / (np.amax(an_scores) - np.amin(an_scores))\n",
    "    \n",
    "    return an_scores, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_scores, gt_labels = _evaluate(test_dataset)\n",
    "\n",
    "print(len(an_scores), len(gt_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨에 따라 anomaly score의 분포가 다르게 나타나는지 검증\n",
    "normal = []\n",
    "anormaly = []\n",
    "for score, label in zip(an_scores, gt_labels):\n",
    "    if label == 0:\n",
    "        anormaly.append(score)\n",
    "    else:\n",
    "        normal.append(score)\n",
    "\n",
    "normal = np.array(normal)\n",
    "print(normal.shape)\n",
    "anormaly = np.array(anormaly)\n",
    "print(anormaly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(normal, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.hist(anormaly, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(normal, norm.pdf(normal, np.mean(normal), np.std(normal)), 'o')\n",
    "plt.plot(anormaly, norm.pdf(anormaly, np.mean(anormaly), np.std(anormaly)), 'o')\n",
    "\n",
    "print(np.mean(normal), np.mean(anormaly))\n",
    "print(np.std(normal), np.std(anormaly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프로젝트: 개구리는 안돼요 (CIFAR-10)\n",
    "* 이상감지용 데이터셋 구축 (개구리 데이터를 학습데이터셋에서 제외하여 테스트 데이터셋에 포함)\n",
    "* Skip-GANomaly 모델의 구현\n",
    "* 모델의 학습과 검증\n",
    "* 검증 결과의 시각화 (정상-이상 데이터의 anomaly score 분포 시각화, 적절한 threshold에 따른 이삼감지율 계산, 감지 성공/실패사례 시각화 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "Number of examples:  50000\n",
      "Number of channels: 3\n",
      "Image size: 32 32\n",
      "Test data:\n",
      "Number of examples: 10000\n",
      "Number of channels: 3\n",
      "Image size: 32 32\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 데이터셋 로드\n",
    "# https://gruuuuu.github.io/machine-learning/cifar10-cnn/\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print (\"Training data:\")\n",
    "print (\"Number of examples: \", X_train.shape[0])\n",
    "print (\"Number of channels:\",X_train.shape[3]) \n",
    "print (\"Image size:\", X_train.shape[1], X_train.shape[2])\n",
    "print\n",
    "print (\"Test data:\")\n",
    "print (\"Number of examples:\", X_test.shape[0])\n",
    "print (\"Number of channels:\", X_test.shape[3])\n",
    "print (\"Image size:\", X_test.shape[1], X_test.shape[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(X_train[i].reshape(32, 32, 3), cmap='gray')\n",
    "    plt.title(f'index: {i}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0번이 개구리임을 확인. 개구리 제외\n",
    "def set_labels(labels):\n",
    "    new_t_labels = []\n",
    "    for old_label in labels:\n",
    "        if old_label == 0:\n",
    "            new_t_labels.append([0]) # 0 (frog) 는 이상치\n",
    "        else:\n",
    "            new_t_labels.append([1]) # 그 외의 경우는 정상치\n",
    "             \n",
    "    return np.array(new_t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bol_train_labels = set_labels(y_train)\n",
    "bol_test_labels = set_labels(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "normal_labels = []\n",
    "anomaly_data = []\n",
    "anomaly_labels = []\n",
    "\n",
    "for data, label in zip(X_train, bol_train_labels):\n",
    "    if label == 0:\n",
    "        anomaly_data.append(data)\n",
    "        anomaly_labels.append(label)\n",
    "    else:\n",
    "        normal_data.append(data)\n",
    "        normal_labels.append(label)\n",
    "        \n",
    "normal_data = np.array(normal_data)\n",
    "normal_labels = np.array(normal_labels)\n",
    "anomaly_data = np.array(anomaly_data)\n",
    "anomaly_labels = np.array(anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3) (45000, 1)\n",
      "(5000, 32, 32, 3) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(normal_data.shape, normal_labels.shape)\n",
    "print(anomaly_data.shape, anomaly_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = normal_data # 개구리 제외한 데이터를 training data로\n",
    "bol_train_labels = normal_labels\n",
    "test_data = tf.concat([X_test, anomaly_data], 0)\n",
    "bol_test_labels = tf.concat([bol_test_labels, anomaly_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3)\n",
      "(15000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1)\n",
      "(15000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(bol_train_labels.shape)\n",
    "print(bol_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋이 정확하게 구성되었는지 검증\n",
    "for label in bol_train_labels:\n",
    "    if label == 0:\n",
    "        print(label)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 구성\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, bol_train_labels))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, bol_test_labels))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 라벨 검증\n",
    "# 훈련 데이터셋에는 라벨이 1인 데이터만 존재하고, 테스트 데이터에는 0과 1이 섞여 있어야 합니다.\n",
    "\n",
    "for data, label in train_dataset.take(1): # 훈련 데이터셋\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in test_dataset.take(1): # 테스트 데이터셋\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 구성\n",
    "* 위 실습에서 구성한 모델 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator 채널 수만 변경\n",
    "generator = Generator(num_output_channel=3)\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(images):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(images, training=True)\n",
    "        \n",
    "        pred_real, feat_real = discriminator(images, training=True)\n",
    "        pred_fake, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(pred_real, pred_fake,\n",
    "                                  images, generated_images,\n",
    "                                  feat_real, feat_fake)\n",
    "\n",
    "        disc_loss = discriminator_loss(pred_real, pred_fake)        \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'ganomaly_skip_no_norm/cifar10/ckpt'\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 100, \t Total Gen Loss : 4981.8095703125, \t Total Dis Loss : 0.6775773167610168\n",
      "Steps : 200, \t Total Gen Loss : 3739.433837890625, \t Total Dis Loss : 0.4182469844818115\n",
      "Steps : 300, \t Total Gen Loss : 3712.87744140625, \t Total Dis Loss : 0.2312907874584198\n",
      "Steps : 400, \t Total Gen Loss : 3441.894775390625, \t Total Dis Loss : 0.1678539216518402\n",
      "Steps : 500, \t Total Gen Loss : 3319.87353515625, \t Total Dis Loss : 0.0710604265332222\n",
      "Steps : 600, \t Total Gen Loss : 4439.92724609375, \t Total Dis Loss : 0.063031867146492\n",
      "Steps : 700, \t Total Gen Loss : 4388.45849609375, \t Total Dis Loss : 0.030544094741344452\n",
      "Steps : 800, \t Total Gen Loss : 3301.0322265625, \t Total Dis Loss : 0.02501210756599903\n",
      "Steps : 900, \t Total Gen Loss : 3492.27978515625, \t Total Dis Loss : 0.017471566796302795\n",
      "Steps : 1000, \t Total Gen Loss : 3251.72509765625, \t Total Dis Loss : 0.010676702484488487\n",
      "Steps : 1100, \t Total Gen Loss : 3598.67724609375, \t Total Dis Loss : 0.23388394713401794\n",
      "Steps : 1200, \t Total Gen Loss : 3518.86474609375, \t Total Dis Loss : 0.035445570945739746\n",
      "Steps : 1300, \t Total Gen Loss : 3446.6748046875, \t Total Dis Loss : 0.03253680095076561\n",
      "Steps : 1400, \t Total Gen Loss : 4113.42138671875, \t Total Dis Loss : 0.010679368861019611\n",
      "Steps : 1500, \t Total Gen Loss : 3990.797119140625, \t Total Dis Loss : 0.017086558043956757\n",
      "Steps : 1600, \t Total Gen Loss : 3697.73828125, \t Total Dis Loss : 0.026742015033960342\n",
      "Steps : 1700, \t Total Gen Loss : 3454.121337890625, \t Total Dis Loss : 0.0068160854279994965\n",
      "Steps : 1800, \t Total Gen Loss : 4065.04736328125, \t Total Dis Loss : 0.011725978925824165\n",
      "Steps : 1900, \t Total Gen Loss : 3682.044189453125, \t Total Dis Loss : 0.004753241315484047\n",
      "Steps : 2000, \t Total Gen Loss : 3293.0478515625, \t Total Dis Loss : 0.005242300219833851\n",
      "Steps : 2100, \t Total Gen Loss : 3564.466064453125, \t Total Dis Loss : 0.004948974587023258\n",
      "Steps : 2200, \t Total Gen Loss : 3403.715087890625, \t Total Dis Loss : 0.008859241381287575\n",
      "Steps : 2300, \t Total Gen Loss : 3809.011474609375, \t Total Dis Loss : 0.0030586044304072857\n",
      "Steps : 2400, \t Total Gen Loss : 3227.165283203125, \t Total Dis Loss : 0.005176533944904804\n",
      "Steps : 2500, \t Total Gen Loss : 3163.904541015625, \t Total Dis Loss : 0.01761545054614544\n",
      "Steps : 2600, \t Total Gen Loss : 3484.217041015625, \t Total Dis Loss : 0.01854238659143448\n",
      "Steps : 2700, \t Total Gen Loss : 3212.407470703125, \t Total Dis Loss : 0.07303221523761749\n",
      "Steps : 2800, \t Total Gen Loss : 3024.660888671875, \t Total Dis Loss : 0.0027345698326826096\n",
      "Steps : 2900, \t Total Gen Loss : 4335.8662109375, \t Total Dis Loss : 0.0021848995238542557\n",
      "Steps : 3000, \t Total Gen Loss : 3833.18701171875, \t Total Dis Loss : 0.0037627650890499353\n",
      "Steps : 3100, \t Total Gen Loss : 3340.969482421875, \t Total Dis Loss : 0.0030534383840858936\n",
      "Steps : 3200, \t Total Gen Loss : 4065.027099609375, \t Total Dis Loss : 0.0012822987046092749\n",
      "Steps : 3300, \t Total Gen Loss : 3928.98681640625, \t Total Dis Loss : 0.0017990362830460072\n",
      "Steps : 3400, \t Total Gen Loss : 3574.854736328125, \t Total Dis Loss : 0.0010670637711882591\n",
      "Steps : 3500, \t Total Gen Loss : 3757.985595703125, \t Total Dis Loss : 0.0010284341406077147\n",
      "Steps : 3600, \t Total Gen Loss : 3598.02783203125, \t Total Dis Loss : 0.001522134873084724\n",
      "Steps : 3700, \t Total Gen Loss : 3437.958984375, \t Total Dis Loss : 0.009697630070149899\n",
      "Steps : 3800, \t Total Gen Loss : 3996.108154296875, \t Total Dis Loss : 0.0012165396474301815\n",
      "Steps : 3900, \t Total Gen Loss : 3607.154541015625, \t Total Dis Loss : 0.0009487250936217606\n",
      "Steps : 4000, \t Total Gen Loss : 3528.220947265625, \t Total Dis Loss : 0.000847451388835907\n",
      "Steps : 4100, \t Total Gen Loss : 3627.298583984375, \t Total Dis Loss : 0.008441551588475704\n",
      "Steps : 4200, \t Total Gen Loss : 3901.988037109375, \t Total Dis Loss : 0.001290618907660246\n",
      "Steps : 4300, \t Total Gen Loss : 3356.32177734375, \t Total Dis Loss : 0.0031489436514675617\n",
      "Steps : 4400, \t Total Gen Loss : 4000.959228515625, \t Total Dis Loss : 0.06739924103021622\n",
      "Steps : 4500, \t Total Gen Loss : 4095.86328125, \t Total Dis Loss : 0.000996293849311769\n",
      "Steps : 4600, \t Total Gen Loss : 3339.210693359375, \t Total Dis Loss : 0.0017109396867454052\n",
      "Steps : 4700, \t Total Gen Loss : 3232.3125, \t Total Dis Loss : 0.0037724769208580256\n",
      "Steps : 4800, \t Total Gen Loss : 4270.38427734375, \t Total Dis Loss : 0.0010183111298829317\n",
      "Steps : 4900, \t Total Gen Loss : 3522.880859375, \t Total Dis Loss : 0.0018711675656959414\n",
      "Steps : 5000, \t Total Gen Loss : 3917.157470703125, \t Total Dis Loss : 0.003877941519021988\n",
      "Steps : 5100, \t Total Gen Loss : 3404.4541015625, \t Total Dis Loss : 0.0003866858605761081\n",
      "Steps : 5200, \t Total Gen Loss : 3021.54150390625, \t Total Dis Loss : 0.0004581109678838402\n",
      "Steps : 5300, \t Total Gen Loss : 3130.28955078125, \t Total Dis Loss : 0.0031913891434669495\n",
      "Steps : 5400, \t Total Gen Loss : 4178.578125, \t Total Dis Loss : 0.008741882629692554\n",
      "Steps : 5500, \t Total Gen Loss : 3469.159423828125, \t Total Dis Loss : 0.0008849614532664418\n",
      "Steps : 5600, \t Total Gen Loss : 3915.7744140625, \t Total Dis Loss : 0.0012334089260548353\n",
      "Time for epoch 1 is 83.91597294807434 sec\n",
      "Steps : 5700, \t Total Gen Loss : 2882.021240234375, \t Total Dis Loss : 0.0013693782966583967\n",
      "Steps : 5800, \t Total Gen Loss : 3601.726806640625, \t Total Dis Loss : 0.001001348253339529\n",
      "Steps : 5900, \t Total Gen Loss : 3675.231689453125, \t Total Dis Loss : 0.0013186819851398468\n",
      "Steps : 6000, \t Total Gen Loss : 4158.14404296875, \t Total Dis Loss : 0.007627478800714016\n",
      "Steps : 6100, \t Total Gen Loss : 4236.9501953125, \t Total Dis Loss : 0.0024058539420366287\n",
      "Steps : 6200, \t Total Gen Loss : 3516.737548828125, \t Total Dis Loss : 0.0005916388472542167\n",
      "Steps : 6300, \t Total Gen Loss : 3685.404052734375, \t Total Dis Loss : 0.0013517355546355247\n",
      "Steps : 6400, \t Total Gen Loss : 3428.276611328125, \t Total Dis Loss : 0.003147721290588379\n",
      "Steps : 6500, \t Total Gen Loss : 3693.985107421875, \t Total Dis Loss : 0.007476241327822208\n",
      "Steps : 6600, \t Total Gen Loss : 3339.29052734375, \t Total Dis Loss : 0.0013507611583918333\n",
      "Steps : 6700, \t Total Gen Loss : 3784.027099609375, \t Total Dis Loss : 0.0015856759855523705\n",
      "Steps : 6800, \t Total Gen Loss : 3627.54638671875, \t Total Dis Loss : 0.0032030530273914337\n",
      "Steps : 6900, \t Total Gen Loss : 3105.93603515625, \t Total Dis Loss : 0.005640886723995209\n",
      "Steps : 7000, \t Total Gen Loss : 3642.387939453125, \t Total Dis Loss : 0.00048744038213044405\n",
      "Steps : 7100, \t Total Gen Loss : 3193.855712890625, \t Total Dis Loss : 0.0005543070146813989\n",
      "Steps : 7200, \t Total Gen Loss : 3710.242431640625, \t Total Dis Loss : 0.0006883856258355081\n",
      "Steps : 7300, \t Total Gen Loss : 3861.173583984375, \t Total Dis Loss : 0.00338305183686316\n",
      "Steps : 7400, \t Total Gen Loss : 3437.213134765625, \t Total Dis Loss : 0.001382380840368569\n",
      "Steps : 7500, \t Total Gen Loss : 3635.5078125, \t Total Dis Loss : 0.0011386929545551538\n",
      "Steps : 7600, \t Total Gen Loss : 3852.508544921875, \t Total Dis Loss : 0.0009699271176941693\n",
      "Steps : 7700, \t Total Gen Loss : 2913.15185546875, \t Total Dis Loss : 0.0010114165488630533\n",
      "Steps : 7800, \t Total Gen Loss : 3532.7939453125, \t Total Dis Loss : 0.001687112613581121\n",
      "Steps : 7900, \t Total Gen Loss : 3537.270751953125, \t Total Dis Loss : 0.00047819630708545446\n",
      "Steps : 8000, \t Total Gen Loss : 3670.238525390625, \t Total Dis Loss : 0.0004030930867884308\n",
      "Steps : 8100, \t Total Gen Loss : 3342.85400390625, \t Total Dis Loss : 0.0010348516516387463\n",
      "Steps : 8200, \t Total Gen Loss : 3836.278564453125, \t Total Dis Loss : 0.00040178169729188085\n",
      "Steps : 8300, \t Total Gen Loss : 3241.514404296875, \t Total Dis Loss : 0.004292249213904142\n",
      "Steps : 8400, \t Total Gen Loss : 4224.2138671875, \t Total Dis Loss : 0.003299150848761201\n",
      "Steps : 8500, \t Total Gen Loss : 3548.724853515625, \t Total Dis Loss : 0.0010059395572170615\n",
      "Steps : 8600, \t Total Gen Loss : 3681.66357421875, \t Total Dis Loss : 0.0006142861675471067\n",
      "Steps : 8700, \t Total Gen Loss : 3895.0517578125, \t Total Dis Loss : 0.0037484592758119106\n",
      "Steps : 8800, \t Total Gen Loss : 3901.27587890625, \t Total Dis Loss : 0.0010739233111962676\n",
      "Steps : 8900, \t Total Gen Loss : 4010.20947265625, \t Total Dis Loss : 0.0013487075921148062\n",
      "Steps : 9000, \t Total Gen Loss : 3233.64208984375, \t Total Dis Loss : 0.0009518744191154838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 9100, \t Total Gen Loss : 3518.2099609375, \t Total Dis Loss : 0.0006506233476102352\n",
      "Steps : 9200, \t Total Gen Loss : 3674.73583984375, \t Total Dis Loss : 0.0005094291409477592\n",
      "Steps : 9300, \t Total Gen Loss : 3639.770263671875, \t Total Dis Loss : 0.00024200264306273311\n",
      "Steps : 9400, \t Total Gen Loss : 3233.017578125, \t Total Dis Loss : 0.0004430263943504542\n",
      "Steps : 9500, \t Total Gen Loss : 3511.532470703125, \t Total Dis Loss : 0.0006146423984318972\n",
      "Steps : 9600, \t Total Gen Loss : 3541.77392578125, \t Total Dis Loss : 0.00016708702605683357\n",
      "Steps : 9700, \t Total Gen Loss : 3598.787109375, \t Total Dis Loss : 0.0005376410554163158\n",
      "Steps : 9800, \t Total Gen Loss : 3736.106201171875, \t Total Dis Loss : 0.00025273801293224096\n",
      "Steps : 9900, \t Total Gen Loss : 3357.651123046875, \t Total Dis Loss : 0.022952113300561905\n",
      "Steps : 10000, \t Total Gen Loss : 3573.2099609375, \t Total Dis Loss : 0.00039164110785350204\n",
      "Steps : 10100, \t Total Gen Loss : 3714.958251953125, \t Total Dis Loss : 0.0010786461643874645\n",
      "Steps : 10200, \t Total Gen Loss : 3795.655517578125, \t Total Dis Loss : 0.00018107908545061946\n",
      "Steps : 10300, \t Total Gen Loss : 4652.427734375, \t Total Dis Loss : 0.0003971592232119292\n",
      "Steps : 10400, \t Total Gen Loss : 2921.073974609375, \t Total Dis Loss : 0.00046046535135246813\n",
      "Steps : 10500, \t Total Gen Loss : 3079.103271484375, \t Total Dis Loss : 0.0004340861923992634\n",
      "Steps : 10600, \t Total Gen Loss : 3279.283203125, \t Total Dis Loss : 0.001095196814276278\n",
      "Steps : 10700, \t Total Gen Loss : 3544.921630859375, \t Total Dis Loss : 0.0042007253505289555\n",
      "Steps : 10800, \t Total Gen Loss : 3638.763671875, \t Total Dis Loss : 0.0014375485479831696\n",
      "Steps : 10900, \t Total Gen Loss : 3752.28759765625, \t Total Dis Loss : 0.0009436229011043906\n",
      "Steps : 11000, \t Total Gen Loss : 3529.962890625, \t Total Dis Loss : 0.00044961593812331557\n",
      "Steps : 11100, \t Total Gen Loss : 3472.02783203125, \t Total Dis Loss : 0.001072470797225833\n",
      "Steps : 11200, \t Total Gen Loss : 3525.87939453125, \t Total Dis Loss : 0.0004009823314845562\n",
      "Time for epoch 2 is 74.63071846961975 sec\n",
      "Steps : 11300, \t Total Gen Loss : 3448.71826171875, \t Total Dis Loss : 0.0004792944819200784\n",
      "Steps : 11400, \t Total Gen Loss : 3025.1923828125, \t Total Dis Loss : 0.0012118309969082475\n",
      "Steps : 11500, \t Total Gen Loss : 3695.538818359375, \t Total Dis Loss : 0.017090683802962303\n",
      "Steps : 11600, \t Total Gen Loss : 3452.175537109375, \t Total Dis Loss : 0.0005502923741005361\n",
      "Steps : 11700, \t Total Gen Loss : 3509.94091796875, \t Total Dis Loss : 0.01372718345373869\n",
      "Steps : 11800, \t Total Gen Loss : 3329.020751953125, \t Total Dis Loss : 0.0015560423489660025\n",
      "Steps : 11900, \t Total Gen Loss : 3841.5068359375, \t Total Dis Loss : 0.0001553144829813391\n",
      "Steps : 12000, \t Total Gen Loss : 3756.3291015625, \t Total Dis Loss : 0.0001781313621904701\n",
      "Steps : 12100, \t Total Gen Loss : 3676.12646484375, \t Total Dis Loss : 0.00033011508639901876\n",
      "Steps : 12200, \t Total Gen Loss : 3731.2919921875, \t Total Dis Loss : 0.0001248576445505023\n",
      "Steps : 12300, \t Total Gen Loss : 3403.683837890625, \t Total Dis Loss : 0.00023453019093722105\n",
      "Steps : 12400, \t Total Gen Loss : 3861.457275390625, \t Total Dis Loss : 0.00014049172750674188\n",
      "Steps : 12500, \t Total Gen Loss : 3858.6455078125, \t Total Dis Loss : 0.00017931076581589878\n",
      "Steps : 12600, \t Total Gen Loss : 4398.38671875, \t Total Dis Loss : 0.00023257789143826813\n",
      "Steps : 12700, \t Total Gen Loss : 3905.29150390625, \t Total Dis Loss : 0.0021102423779666424\n",
      "Steps : 12800, \t Total Gen Loss : 4206.2314453125, \t Total Dis Loss : 0.0013935411116108298\n",
      "Steps : 12900, \t Total Gen Loss : 3178.506591796875, \t Total Dis Loss : 0.00043950614053756\n",
      "Steps : 13000, \t Total Gen Loss : 3189.966796875, \t Total Dis Loss : 0.0002268080279463902\n",
      "Steps : 13100, \t Total Gen Loss : 4240.7451171875, \t Total Dis Loss : 0.00015023601008579135\n",
      "Steps : 13200, \t Total Gen Loss : 3468.65185546875, \t Total Dis Loss : 0.00014521782577503473\n",
      "Steps : 13300, \t Total Gen Loss : 4438.4765625, \t Total Dis Loss : 0.00019805124611593783\n",
      "Steps : 13400, \t Total Gen Loss : 3874.154052734375, \t Total Dis Loss : 0.000751894898712635\n",
      "Steps : 13500, \t Total Gen Loss : 4394.61083984375, \t Total Dis Loss : 0.06840778887271881\n",
      "Steps : 13600, \t Total Gen Loss : 3350.29345703125, \t Total Dis Loss : 0.04967770352959633\n",
      "Steps : 13700, \t Total Gen Loss : 3959.544189453125, \t Total Dis Loss : 0.001303357072174549\n",
      "Steps : 13800, \t Total Gen Loss : 3900.934814453125, \t Total Dis Loss : 0.000737441354431212\n",
      "Steps : 13900, \t Total Gen Loss : 3415.228271484375, \t Total Dis Loss : 0.0002235519204987213\n",
      "Steps : 14000, \t Total Gen Loss : 3602.95947265625, \t Total Dis Loss : 0.00043183364323340356\n",
      "Steps : 14100, \t Total Gen Loss : 3850.10107421875, \t Total Dis Loss : 0.0005407073185779154\n",
      "Steps : 14200, \t Total Gen Loss : 3728.04638671875, \t Total Dis Loss : 0.0015931014204397798\n",
      "Steps : 14300, \t Total Gen Loss : 3599.9609375, \t Total Dis Loss : 0.00030137074645608664\n",
      "Steps : 14400, \t Total Gen Loss : 3552.173583984375, \t Total Dis Loss : 0.0007343784673139453\n",
      "Steps : 14500, \t Total Gen Loss : 4091.849853515625, \t Total Dis Loss : 0.001250233966857195\n",
      "Steps : 14600, \t Total Gen Loss : 3609.84814453125, \t Total Dis Loss : 0.00021540152374655008\n",
      "Steps : 14700, \t Total Gen Loss : 3541.74853515625, \t Total Dis Loss : 0.0007793525001034141\n",
      "Steps : 14800, \t Total Gen Loss : 3626.29150390625, \t Total Dis Loss : 0.0005711622652597725\n",
      "Steps : 14900, \t Total Gen Loss : 3525.3525390625, \t Total Dis Loss : 0.0010791656095534563\n",
      "Steps : 15000, \t Total Gen Loss : 3946.122802734375, \t Total Dis Loss : 0.0024222666397690773\n",
      "Steps : 15100, \t Total Gen Loss : 3920.08349609375, \t Total Dis Loss : 0.0003547917876858264\n",
      "Steps : 15200, \t Total Gen Loss : 3687.6826171875, \t Total Dis Loss : 0.0008055918733589351\n",
      "Steps : 15300, \t Total Gen Loss : 4090.277099609375, \t Total Dis Loss : 0.0006021611043252051\n",
      "Steps : 15400, \t Total Gen Loss : 3804.2890625, \t Total Dis Loss : 0.00024239244521595538\n",
      "Steps : 15500, \t Total Gen Loss : 3622.6298828125, \t Total Dis Loss : 0.0001076313346857205\n",
      "Steps : 15600, \t Total Gen Loss : 3759.37548828125, \t Total Dis Loss : 0.0013337540440261364\n",
      "Steps : 15700, \t Total Gen Loss : 3509.15576171875, \t Total Dis Loss : 0.0006483493489213288\n",
      "Steps : 15800, \t Total Gen Loss : 3772.03173828125, \t Total Dis Loss : 0.0004919118364341557\n",
      "Steps : 15900, \t Total Gen Loss : 4064.86279296875, \t Total Dis Loss : 0.000593147415202111\n",
      "Steps : 16000, \t Total Gen Loss : 3616.1728515625, \t Total Dis Loss : 0.0003338792303111404\n",
      "Steps : 16100, \t Total Gen Loss : 3200.83837890625, \t Total Dis Loss : 0.0005458379164338112\n",
      "Steps : 16200, \t Total Gen Loss : 3370.89111328125, \t Total Dis Loss : 0.0024024141021072865\n",
      "Steps : 16300, \t Total Gen Loss : 3846.492431640625, \t Total Dis Loss : 0.00045692664571106434\n",
      "Steps : 16400, \t Total Gen Loss : 3447.480712890625, \t Total Dis Loss : 0.00013670496991835535\n",
      "Steps : 16500, \t Total Gen Loss : 3821.71435546875, \t Total Dis Loss : 7.423958595609292e-05\n",
      "Steps : 16600, \t Total Gen Loss : 3803.41015625, \t Total Dis Loss : 0.0006326777511276305\n",
      "Steps : 16700, \t Total Gen Loss : 3255.0185546875, \t Total Dis Loss : 0.0002588704810477793\n",
      "Steps : 16800, \t Total Gen Loss : 4322.2626953125, \t Total Dis Loss : 0.00022710178745910525\n",
      "Time for epoch 3 is 74.77904319763184 sec\n",
      "Steps : 16900, \t Total Gen Loss : 3796.986328125, \t Total Dis Loss : 0.00023344936198554933\n",
      "Steps : 17000, \t Total Gen Loss : 3280.0712890625, \t Total Dis Loss : 7.609486056026071e-05\n",
      "Steps : 17100, \t Total Gen Loss : 3286.082275390625, \t Total Dis Loss : 8.401095692534e-05\n",
      "Steps : 17200, \t Total Gen Loss : 3359.1015625, \t Total Dis Loss : 0.00012577403686009347\n",
      "Steps : 17300, \t Total Gen Loss : 3395.891845703125, \t Total Dis Loss : 0.005404985044151545\n",
      "Steps : 17400, \t Total Gen Loss : 3835.978271484375, \t Total Dis Loss : 0.001363544724881649\n",
      "Steps : 17500, \t Total Gen Loss : 3572.09765625, \t Total Dis Loss : 0.00033827219158411026\n",
      "Steps : 17600, \t Total Gen Loss : 3682.6875, \t Total Dis Loss : 0.0002956406679004431\n",
      "Steps : 17700, \t Total Gen Loss : 3603.265625, \t Total Dis Loss : 0.00036227464443072677\n",
      "Steps : 17800, \t Total Gen Loss : 3870.76220703125, \t Total Dis Loss : 0.0003767985908780247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 17900, \t Total Gen Loss : 3239.28564453125, \t Total Dis Loss : 0.0005552556249313056\n",
      "Steps : 18000, \t Total Gen Loss : 3744.7080078125, \t Total Dis Loss : 0.0006752283661626279\n",
      "Steps : 18100, \t Total Gen Loss : 3957.933837890625, \t Total Dis Loss : 0.00044839721522293985\n",
      "Steps : 18200, \t Total Gen Loss : 4073.938720703125, \t Total Dis Loss : 0.00031129096169024706\n",
      "Steps : 18300, \t Total Gen Loss : 3534.225341796875, \t Total Dis Loss : 0.0003090713871642947\n",
      "Steps : 18400, \t Total Gen Loss : 3918.33203125, \t Total Dis Loss : 0.00013029832916799933\n",
      "Steps : 18500, \t Total Gen Loss : 3557.644775390625, \t Total Dis Loss : 0.0003889047948177904\n",
      "Steps : 18600, \t Total Gen Loss : 3588.73388671875, \t Total Dis Loss : 8.848650759318843e-05\n",
      "Steps : 18700, \t Total Gen Loss : 3106.4599609375, \t Total Dis Loss : 0.000424671801738441\n",
      "Steps : 18800, \t Total Gen Loss : 3170.353271484375, \t Total Dis Loss : 0.00047199393156915903\n",
      "Steps : 18900, \t Total Gen Loss : 4078.21337890625, \t Total Dis Loss : 0.00022466504015028477\n",
      "Steps : 19000, \t Total Gen Loss : 4108.49658203125, \t Total Dis Loss : 9.35366697376594e-05\n",
      "Steps : 19100, \t Total Gen Loss : 3045.554443359375, \t Total Dis Loss : 0.06839177012443542\n",
      "Steps : 19200, \t Total Gen Loss : 3129.35009765625, \t Total Dis Loss : 0.0018663516966626048\n",
      "Steps : 19300, \t Total Gen Loss : 3364.741943359375, \t Total Dis Loss : 0.00025397707941010594\n",
      "Steps : 19400, \t Total Gen Loss : 3219.06494140625, \t Total Dis Loss : 0.0016498132608830929\n",
      "Steps : 19500, \t Total Gen Loss : 3950.767578125, \t Total Dis Loss : 0.011238339357078075\n",
      "Steps : 19600, \t Total Gen Loss : 3773.23583984375, \t Total Dis Loss : 0.0003617876791395247\n",
      "Steps : 19700, \t Total Gen Loss : 3717.99072265625, \t Total Dis Loss : 0.0003477331483736634\n",
      "Steps : 19800, \t Total Gen Loss : 3692.11474609375, \t Total Dis Loss : 0.00034953278372995555\n",
      "Steps : 19900, \t Total Gen Loss : 3491.65087890625, \t Total Dis Loss : 0.0004233288927935064\n",
      "Steps : 20000, \t Total Gen Loss : 3798.82275390625, \t Total Dis Loss : 0.00020727122318930924\n",
      "Steps : 20100, \t Total Gen Loss : 3202.27490234375, \t Total Dis Loss : 0.00020520386169664562\n",
      "Steps : 20200, \t Total Gen Loss : 3215.230224609375, \t Total Dis Loss : 0.0003324185963720083\n",
      "Steps : 20300, \t Total Gen Loss : 3959.27001953125, \t Total Dis Loss : 0.0008617864223197103\n",
      "Steps : 20400, \t Total Gen Loss : 3286.114501953125, \t Total Dis Loss : 0.00012835353845730424\n",
      "Steps : 20500, \t Total Gen Loss : 3569.171142578125, \t Total Dis Loss : 0.00019925591186620295\n",
      "Steps : 20600, \t Total Gen Loss : 4169.181640625, \t Total Dis Loss : 0.00015890932991169393\n",
      "Steps : 20700, \t Total Gen Loss : 3581.854248046875, \t Total Dis Loss : 0.000464416021713987\n",
      "Steps : 20800, \t Total Gen Loss : 3757.942138671875, \t Total Dis Loss : 0.00017233641119673848\n",
      "Steps : 20900, \t Total Gen Loss : 3624.615234375, \t Total Dis Loss : 0.0001956928608706221\n",
      "Steps : 21000, \t Total Gen Loss : 3825.4697265625, \t Total Dis Loss : 0.00019868201343342662\n",
      "Steps : 21100, \t Total Gen Loss : 3598.302734375, \t Total Dis Loss : 0.00012572800915222615\n",
      "Steps : 21200, \t Total Gen Loss : 3474.01953125, \t Total Dis Loss : 0.00026337974122725427\n",
      "Steps : 21300, \t Total Gen Loss : 3056.58837890625, \t Total Dis Loss : 0.00013146954006515443\n",
      "Steps : 21400, \t Total Gen Loss : 3837.575439453125, \t Total Dis Loss : 5.9697111282730475e-05\n",
      "Steps : 21500, \t Total Gen Loss : 3315.356689453125, \t Total Dis Loss : 0.0009408222394995391\n",
      "Steps : 21600, \t Total Gen Loss : 3653.83154296875, \t Total Dis Loss : 0.00010586684220470488\n",
      "Steps : 21700, \t Total Gen Loss : 3569.1533203125, \t Total Dis Loss : 0.00011168647324666381\n",
      "Steps : 21800, \t Total Gen Loss : 3932.615234375, \t Total Dis Loss : 0.08175254613161087\n",
      "Steps : 21900, \t Total Gen Loss : 3309.396728515625, \t Total Dis Loss : 0.00011503348650876433\n",
      "Steps : 22000, \t Total Gen Loss : 4045.501220703125, \t Total Dis Loss : 0.00044545435230247676\n",
      "Steps : 22100, \t Total Gen Loss : 3249.727294921875, \t Total Dis Loss : 6.720347300870344e-05\n",
      "Steps : 22200, \t Total Gen Loss : 3375.5205078125, \t Total Dis Loss : 0.00022092470317147672\n",
      "Steps : 22300, \t Total Gen Loss : 3691.483642578125, \t Total Dis Loss : 0.0002791969745885581\n",
      "Steps : 22400, \t Total Gen Loss : 3831.677734375, \t Total Dis Loss : 0.0001730277726892382\n",
      "Steps : 22500, \t Total Gen Loss : 3559.482177734375, \t Total Dis Loss : 0.0003139587352052331\n",
      "Time for epoch 4 is 74.95161533355713 sec\n",
      "Steps : 22600, \t Total Gen Loss : 3631.38916015625, \t Total Dis Loss : 0.0005004100385122001\n",
      "Steps : 22700, \t Total Gen Loss : 3460.072021484375, \t Total Dis Loss : 0.00021960982121527195\n",
      "Steps : 22800, \t Total Gen Loss : 3462.981689453125, \t Total Dis Loss : 0.0001418389001628384\n",
      "Steps : 22900, \t Total Gen Loss : 4220.47900390625, \t Total Dis Loss : 0.00020071867038495839\n",
      "Steps : 23000, \t Total Gen Loss : 4235.3916015625, \t Total Dis Loss : 0.0017276409780606627\n",
      "Steps : 23100, \t Total Gen Loss : 3380.421875, \t Total Dis Loss : 0.00016268424224108458\n",
      "Steps : 23200, \t Total Gen Loss : 4245.1015625, \t Total Dis Loss : 0.001917349174618721\n",
      "Steps : 23300, \t Total Gen Loss : 3220.514892578125, \t Total Dis Loss : 0.0004550900775939226\n",
      "Steps : 23400, \t Total Gen Loss : 3400.54833984375, \t Total Dis Loss : 0.000302870204905048\n",
      "Steps : 23500, \t Total Gen Loss : 3752.531005859375, \t Total Dis Loss : 0.00034279844840057194\n",
      "Steps : 23600, \t Total Gen Loss : 3261.734619140625, \t Total Dis Loss : 0.0006982922786846757\n",
      "Steps : 23700, \t Total Gen Loss : 3737.51025390625, \t Total Dis Loss : 0.00016030312690418214\n",
      "Steps : 23800, \t Total Gen Loss : 3159.017578125, \t Total Dis Loss : 0.0002322848013136536\n",
      "Steps : 23900, \t Total Gen Loss : 3706.002197265625, \t Total Dis Loss : 0.00015664917009416968\n",
      "Steps : 24000, \t Total Gen Loss : 3882.652587890625, \t Total Dis Loss : 0.0001470819697715342\n",
      "Steps : 24100, \t Total Gen Loss : 4016.650634765625, \t Total Dis Loss : 0.0004972118767909706\n",
      "Steps : 24200, \t Total Gen Loss : 3528.266357421875, \t Total Dis Loss : 0.000183402793481946\n",
      "Steps : 24300, \t Total Gen Loss : 4010.35693359375, \t Total Dis Loss : 0.00026858673663809896\n",
      "Steps : 24400, \t Total Gen Loss : 3095.881591796875, \t Total Dis Loss : 0.006989228539168835\n",
      "Steps : 24500, \t Total Gen Loss : 4104.23388671875, \t Total Dis Loss : 0.00022321019787341356\n",
      "Steps : 24600, \t Total Gen Loss : 3549.575439453125, \t Total Dis Loss : 0.0003834518720395863\n",
      "Steps : 24700, \t Total Gen Loss : 3792.603271484375, \t Total Dis Loss : 0.0002054614742519334\n",
      "Steps : 24800, \t Total Gen Loss : 3638.45458984375, \t Total Dis Loss : 8.368848648387939e-05\n",
      "Steps : 24900, \t Total Gen Loss : 3887.238525390625, \t Total Dis Loss : 0.00033255090238526464\n",
      "Steps : 25000, \t Total Gen Loss : 3233.568603515625, \t Total Dis Loss : 0.00012731138849630952\n",
      "Steps : 25100, \t Total Gen Loss : 3355.5380859375, \t Total Dis Loss : 0.00021245816606096923\n",
      "Steps : 25200, \t Total Gen Loss : 3579.81591796875, \t Total Dis Loss : 0.000300322164548561\n",
      "Steps : 25300, \t Total Gen Loss : 3828.017333984375, \t Total Dis Loss : 0.000498181558214128\n",
      "Steps : 25400, \t Total Gen Loss : 4212.10693359375, \t Total Dis Loss : 0.00016429006063845009\n",
      "Steps : 25500, \t Total Gen Loss : 3742.184814453125, \t Total Dis Loss : 0.0001363595947623253\n",
      "Steps : 25600, \t Total Gen Loss : 3570.8603515625, \t Total Dis Loss : 0.0006019459688104689\n",
      "Steps : 25700, \t Total Gen Loss : 3757.423583984375, \t Total Dis Loss : 0.00018962238391395658\n",
      "Steps : 25800, \t Total Gen Loss : 4235.591796875, \t Total Dis Loss : 6.347039015963674e-05\n",
      "Steps : 25900, \t Total Gen Loss : 3399.9765625, \t Total Dis Loss : 0.0004218432877678424\n",
      "Steps : 26000, \t Total Gen Loss : 3383.78515625, \t Total Dis Loss : 7.584709965158254e-05\n",
      "Steps : 26100, \t Total Gen Loss : 3705.777099609375, \t Total Dis Loss : 6.923721230123192e-05\n",
      "Steps : 26200, \t Total Gen Loss : 3498.576171875, \t Total Dis Loss : 0.3312734365463257\n",
      "Steps : 26300, \t Total Gen Loss : 3444.953857421875, \t Total Dis Loss : 0.0011789649724960327\n",
      "Steps : 26400, \t Total Gen Loss : 3443.891357421875, \t Total Dis Loss : 0.0005122849834151566\n",
      "Steps : 26500, \t Total Gen Loss : 3681.837890625, \t Total Dis Loss : 0.0011281311744824052\n",
      "Steps : 26600, \t Total Gen Loss : 3534.791259765625, \t Total Dis Loss : 0.0005474628414958715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 26700, \t Total Gen Loss : 3421.54345703125, \t Total Dis Loss : 0.00011101391282863915\n",
      "Steps : 26800, \t Total Gen Loss : 3515.4697265625, \t Total Dis Loss : 0.00024553644470870495\n",
      "Steps : 26900, \t Total Gen Loss : 3642.657470703125, \t Total Dis Loss : 0.0005667299265041947\n",
      "Steps : 27000, \t Total Gen Loss : 3437.370849609375, \t Total Dis Loss : 0.000364270294085145\n",
      "Steps : 27100, \t Total Gen Loss : 3534.746826171875, \t Total Dis Loss : 0.00010940876381937414\n",
      "Steps : 27200, \t Total Gen Loss : 3896.02392578125, \t Total Dis Loss : 0.00026171570061706007\n",
      "Steps : 27300, \t Total Gen Loss : 3127.933837890625, \t Total Dis Loss : 0.00011620558507274836\n",
      "Steps : 27400, \t Total Gen Loss : 3736.396484375, \t Total Dis Loss : 9.279981895815581e-05\n",
      "Steps : 27500, \t Total Gen Loss : 3882.33740234375, \t Total Dis Loss : 0.00035009998828172684\n",
      "Steps : 27600, \t Total Gen Loss : 3676.748291015625, \t Total Dis Loss : 0.00010094272147398442\n",
      "Steps : 27700, \t Total Gen Loss : 3494.990478515625, \t Total Dis Loss : 0.000369630433851853\n",
      "Steps : 27800, \t Total Gen Loss : 3729.7158203125, \t Total Dis Loss : 0.00039926014142110944\n",
      "Steps : 27900, \t Total Gen Loss : 3526.232421875, \t Total Dis Loss : 0.00262598623521626\n",
      "Steps : 28000, \t Total Gen Loss : 3774.695068359375, \t Total Dis Loss : 0.00025299107073806226\n",
      "Steps : 28100, \t Total Gen Loss : 3423.9609375, \t Total Dis Loss : 0.00016248930478468537\n",
      "Time for epoch 5 is 75.08075404167175 sec\n",
      "Steps : 28200, \t Total Gen Loss : 3782.07958984375, \t Total Dis Loss : 0.004489853512495756\n",
      "Steps : 28300, \t Total Gen Loss : 3834.10107421875, \t Total Dis Loss : 0.0002347923582419753\n",
      "Steps : 28400, \t Total Gen Loss : 3347.212646484375, \t Total Dis Loss : 0.00017829574062488973\n",
      "Steps : 28500, \t Total Gen Loss : 4202.4755859375, \t Total Dis Loss : 0.0005346164689399302\n",
      "Steps : 28600, \t Total Gen Loss : 3853.275390625, \t Total Dis Loss : 8.084753790171817e-05\n",
      "Steps : 28700, \t Total Gen Loss : 3866.93408203125, \t Total Dis Loss : 0.00012596792657859623\n",
      "Steps : 28800, \t Total Gen Loss : 3634.64453125, \t Total Dis Loss : 0.00011024256673408672\n",
      "Steps : 28900, \t Total Gen Loss : 3068.417724609375, \t Total Dis Loss : 3.52638671756722e-05\n",
      "Steps : 29000, \t Total Gen Loss : 3744.635009765625, \t Total Dis Loss : 0.0005493939388543367\n",
      "Steps : 29100, \t Total Gen Loss : 3979.88916015625, \t Total Dis Loss : 3.450014628469944e-05\n",
      "Steps : 29200, \t Total Gen Loss : 3266.228515625, \t Total Dis Loss : 8.874199556885287e-05\n",
      "Steps : 29300, \t Total Gen Loss : 3535.65771484375, \t Total Dis Loss : 0.00012761549442075193\n",
      "Steps : 29400, \t Total Gen Loss : 4582.68212890625, \t Total Dis Loss : 4.010184056824073e-05\n",
      "Steps : 29500, \t Total Gen Loss : 3580.428955078125, \t Total Dis Loss : 4.997936775907874e-05\n",
      "Steps : 29600, \t Total Gen Loss : 3427.336181640625, \t Total Dis Loss : 5.095768210594542e-05\n",
      "Steps : 29700, \t Total Gen Loss : 3611.55517578125, \t Total Dis Loss : 0.00021120668679941446\n",
      "Steps : 29800, \t Total Gen Loss : 3669.324462890625, \t Total Dis Loss : 5.206563582760282e-05\n",
      "Steps : 29900, \t Total Gen Loss : 3110.239013671875, \t Total Dis Loss : 0.00032040313817560673\n",
      "Steps : 30000, \t Total Gen Loss : 4117.1044921875, \t Total Dis Loss : 0.00258076936006546\n",
      "Steps : 30100, \t Total Gen Loss : 3805.501953125, \t Total Dis Loss : 0.0002601134474389255\n",
      "Steps : 30200, \t Total Gen Loss : 3541.958251953125, \t Total Dis Loss : 0.0019664091523736715\n",
      "Steps : 30300, \t Total Gen Loss : 4297.396484375, \t Total Dis Loss : 0.0004361816099844873\n",
      "Steps : 30400, \t Total Gen Loss : 3773.18115234375, \t Total Dis Loss : 0.0005455680657178164\n",
      "Steps : 30500, \t Total Gen Loss : 4267.2900390625, \t Total Dis Loss : 0.0002805747208185494\n",
      "Steps : 30600, \t Total Gen Loss : 3641.680908203125, \t Total Dis Loss : 0.00123394385445863\n",
      "Steps : 30700, \t Total Gen Loss : 3555.801025390625, \t Total Dis Loss : 6.960616883588955e-05\n",
      "Steps : 30800, \t Total Gen Loss : 3527.853515625, \t Total Dis Loss : 0.00010465592640684918\n",
      "Steps : 30900, \t Total Gen Loss : 4103.455078125, \t Total Dis Loss : 0.00016574689652770758\n",
      "Steps : 31000, \t Total Gen Loss : 3860.384765625, \t Total Dis Loss : 0.00018681278743315488\n",
      "Steps : 31100, \t Total Gen Loss : 3659.23583984375, \t Total Dis Loss : 9.292202594224364e-05\n",
      "Steps : 31200, \t Total Gen Loss : 3678.233642578125, \t Total Dis Loss : 0.00011268256639596075\n",
      "Steps : 31300, \t Total Gen Loss : 3246.056396484375, \t Total Dis Loss : 0.0001983851834665984\n",
      "Steps : 31400, \t Total Gen Loss : 3897.123779296875, \t Total Dis Loss : 3.40355618391186e-05\n",
      "Steps : 31500, \t Total Gen Loss : 3119.54443359375, \t Total Dis Loss : 0.0006404882296919823\n",
      "Steps : 31600, \t Total Gen Loss : 3702.4140625, \t Total Dis Loss : 4.653566793422215e-05\n",
      "Steps : 31700, \t Total Gen Loss : 3319.290283203125, \t Total Dis Loss : 6.363702414091676e-05\n",
      "Steps : 31800, \t Total Gen Loss : 4065.09375, \t Total Dis Loss : 6.344735447783023e-05\n",
      "Steps : 31900, \t Total Gen Loss : 3616.948974609375, \t Total Dis Loss : 3.0182654882082716e-05\n",
      "Steps : 32000, \t Total Gen Loss : 3514.281982421875, \t Total Dis Loss : 2.9759728931821883e-05\n",
      "Steps : 32100, \t Total Gen Loss : 3736.741943359375, \t Total Dis Loss : 2.6060271920869127e-05\n",
      "Steps : 32200, \t Total Gen Loss : 3933.220947265625, \t Total Dis Loss : 4.839645407628268e-05\n",
      "Steps : 32300, \t Total Gen Loss : 3292.251220703125, \t Total Dis Loss : 3.8650177884846926e-05\n",
      "Steps : 32400, \t Total Gen Loss : 3515.782470703125, \t Total Dis Loss : 3.30869406752754e-05\n",
      "Steps : 32500, \t Total Gen Loss : 4101.9638671875, \t Total Dis Loss : 1.0535966794122942e-05\n",
      "Steps : 32600, \t Total Gen Loss : 4552.8662109375, \t Total Dis Loss : 3.1366631446871907e-05\n",
      "Steps : 32700, \t Total Gen Loss : 3312.2333984375, \t Total Dis Loss : 7.288593042176217e-05\n",
      "Steps : 32800, \t Total Gen Loss : 4011.375, \t Total Dis Loss : 0.00013287313049659133\n",
      "Steps : 32900, \t Total Gen Loss : 4011.09326171875, \t Total Dis Loss : 0.00020570562628563493\n",
      "Steps : 33000, \t Total Gen Loss : 3386.56787109375, \t Total Dis Loss : 0.00012429169146344066\n",
      "Steps : 33100, \t Total Gen Loss : 3142.2275390625, \t Total Dis Loss : 0.00010324353934265673\n",
      "Steps : 33200, \t Total Gen Loss : 3176.72998046875, \t Total Dis Loss : 6.696394848404452e-05\n",
      "Steps : 33300, \t Total Gen Loss : 4052.841796875, \t Total Dis Loss : 0.0001843716308940202\n",
      "Steps : 33400, \t Total Gen Loss : 3171.741455078125, \t Total Dis Loss : 0.00011394738976377994\n",
      "Steps : 33500, \t Total Gen Loss : 3313.5302734375, \t Total Dis Loss : 0.00014907962759025395\n",
      "Steps : 33600, \t Total Gen Loss : 3547.736328125, \t Total Dis Loss : 5.011304892832413e-05\n",
      "Steps : 33700, \t Total Gen Loss : 3682.51220703125, \t Total Dis Loss : 0.005900138523429632\n",
      "Time for epoch 6 is 74.99361252784729 sec\n",
      "Steps : 33800, \t Total Gen Loss : 3499.646240234375, \t Total Dis Loss : 5.233764386503026e-05\n",
      "Steps : 33900, \t Total Gen Loss : 3855.18115234375, \t Total Dis Loss : 2.573919300630223e-05\n",
      "Steps : 34000, \t Total Gen Loss : 3813.7861328125, \t Total Dis Loss : 4.733676905743778e-05\n",
      "Steps : 34100, \t Total Gen Loss : 3473.373779296875, \t Total Dis Loss : 4.4072832679376006e-05\n",
      "Steps : 34200, \t Total Gen Loss : 3604.954833984375, \t Total Dis Loss : 2.574126483523287e-05\n",
      "Steps : 34300, \t Total Gen Loss : 3735.018310546875, \t Total Dis Loss : 0.0002633094845805317\n",
      "Steps : 34400, \t Total Gen Loss : 3253.7314453125, \t Total Dis Loss : 0.0009711958118714392\n",
      "Steps : 34500, \t Total Gen Loss : 3269.968505859375, \t Total Dis Loss : 0.000563188863452524\n",
      "Steps : 34600, \t Total Gen Loss : 3468.215576171875, \t Total Dis Loss : 0.0002202570904046297\n",
      "Steps : 34700, \t Total Gen Loss : 3563.887939453125, \t Total Dis Loss : 7.90149497333914e-05\n",
      "Steps : 34800, \t Total Gen Loss : 4056.257080078125, \t Total Dis Loss : 0.005157174542546272\n",
      "Steps : 34900, \t Total Gen Loss : 3462.28076171875, \t Total Dis Loss : 0.00025378481950610876\n",
      "Steps : 35000, \t Total Gen Loss : 3532.36376953125, \t Total Dis Loss : 0.00013508685515262187\n",
      "Steps : 35100, \t Total Gen Loss : 3837.16845703125, \t Total Dis Loss : 0.00018705638649407774\n",
      "Steps : 35200, \t Total Gen Loss : 3373.701904296875, \t Total Dis Loss : 0.0003962306072935462\n",
      "Steps : 35300, \t Total Gen Loss : 3349.0927734375, \t Total Dis Loss : 6.1032824305584654e-05\n",
      "Steps : 35400, \t Total Gen Loss : 3885.770263671875, \t Total Dis Loss : 6.022449451847933e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 35500, \t Total Gen Loss : 3191.16357421875, \t Total Dis Loss : 0.00011339296906953678\n",
      "Steps : 35600, \t Total Gen Loss : 3074.029541015625, \t Total Dis Loss : 0.0001773591066012159\n",
      "Steps : 35700, \t Total Gen Loss : 3338.826171875, \t Total Dis Loss : 2.3188316845335066e-05\n",
      "Steps : 35800, \t Total Gen Loss : 3611.132080078125, \t Total Dis Loss : 0.012384051457047462\n",
      "Steps : 35900, \t Total Gen Loss : 3206.59765625, \t Total Dis Loss : 0.00012530021194834262\n",
      "Steps : 36000, \t Total Gen Loss : 3481.4189453125, \t Total Dis Loss : 0.0007750854128971696\n",
      "Steps : 36100, \t Total Gen Loss : 3786.572998046875, \t Total Dis Loss : 7.325624756049365e-05\n",
      "Steps : 36200, \t Total Gen Loss : 3551.41455078125, \t Total Dis Loss : 6.301884423010051e-05\n",
      "Steps : 36300, \t Total Gen Loss : 3716.925048828125, \t Total Dis Loss : 7.620178075740114e-05\n",
      "Steps : 36400, \t Total Gen Loss : 3594.37353515625, \t Total Dis Loss : 0.0008048240561038256\n",
      "Steps : 36500, \t Total Gen Loss : 3365.883544921875, \t Total Dis Loss : 4.535759217105806e-05\n",
      "Steps : 36600, \t Total Gen Loss : 3463.91552734375, \t Total Dis Loss : 4.506171171669848e-05\n",
      "Steps : 36700, \t Total Gen Loss : 3864.465576171875, \t Total Dis Loss : 5.593773312284611e-05\n",
      "Steps : 36800, \t Total Gen Loss : 3227.10888671875, \t Total Dis Loss : 4.6474524424411356e-05\n",
      "Steps : 36900, \t Total Gen Loss : 3788.181640625, \t Total Dis Loss : 0.00017454917542636395\n",
      "Steps : 37000, \t Total Gen Loss : 3558.42333984375, \t Total Dis Loss : 9.152430720860139e-05\n",
      "Steps : 37100, \t Total Gen Loss : 3098.870849609375, \t Total Dis Loss : 1.7324940927210264e-05\n",
      "Steps : 37200, \t Total Gen Loss : 3425.568603515625, \t Total Dis Loss : 3.354573709657416e-05\n",
      "Steps : 37300, \t Total Gen Loss : 3684.086669921875, \t Total Dis Loss : 8.6279193055816e-05\n",
      "Steps : 37400, \t Total Gen Loss : 3627.46533203125, \t Total Dis Loss : 9.837809921009466e-05\n",
      "Steps : 37500, \t Total Gen Loss : 3279.44921875, \t Total Dis Loss : 1.8227043256047182e-05\n",
      "Steps : 37600, \t Total Gen Loss : 3353.476318359375, \t Total Dis Loss : 0.0004403240163810551\n",
      "Steps : 37700, \t Total Gen Loss : 3779.4912109375, \t Total Dis Loss : 0.0001152212789747864\n",
      "Steps : 37800, \t Total Gen Loss : 4091.27587890625, \t Total Dis Loss : 7.869324326748028e-05\n",
      "Steps : 37900, \t Total Gen Loss : 3459.561279296875, \t Total Dis Loss : 9.2801041319035e-05\n",
      "Steps : 38000, \t Total Gen Loss : 3851.62158203125, \t Total Dis Loss : 0.00019342040468472987\n",
      "Steps : 38100, \t Total Gen Loss : 2941.61279296875, \t Total Dis Loss : 0.00017649272922426462\n",
      "Steps : 38200, \t Total Gen Loss : 3406.78955078125, \t Total Dis Loss : 0.0008406228735111654\n",
      "Steps : 38300, \t Total Gen Loss : 3934.14453125, \t Total Dis Loss : 0.0001393643324263394\n",
      "Steps : 38400, \t Total Gen Loss : 3545.962158203125, \t Total Dis Loss : 0.00027546094497665763\n",
      "Steps : 38500, \t Total Gen Loss : 3543.33447265625, \t Total Dis Loss : 0.0011646763887256384\n",
      "Steps : 38600, \t Total Gen Loss : 3090.719970703125, \t Total Dis Loss : 0.00011951345368288457\n",
      "Steps : 38700, \t Total Gen Loss : 3557.3525390625, \t Total Dis Loss : 8.368617272935808e-05\n",
      "Steps : 38800, \t Total Gen Loss : 3948.3779296875, \t Total Dis Loss : 7.552088209195063e-05\n",
      "Steps : 38900, \t Total Gen Loss : 3132.56005859375, \t Total Dis Loss : 5.968711047898978e-05\n",
      "Steps : 39000, \t Total Gen Loss : 3825.166015625, \t Total Dis Loss : 0.00022058132162783295\n",
      "Steps : 39100, \t Total Gen Loss : 3640.073974609375, \t Total Dis Loss : 0.00019286636961624026\n",
      "Steps : 39200, \t Total Gen Loss : 4033.052734375, \t Total Dis Loss : 0.00014896703942213207\n",
      "Steps : 39300, \t Total Gen Loss : 3587.82861328125, \t Total Dis Loss : 0.00014370937424246222\n",
      "Time for epoch 7 is 74.88843584060669 sec\n",
      "Steps : 39400, \t Total Gen Loss : 3675.3115234375, \t Total Dis Loss : 0.00035601225681602955\n",
      "Steps : 39500, \t Total Gen Loss : 3621.38232421875, \t Total Dis Loss : 6.135754665592685e-05\n",
      "Steps : 39600, \t Total Gen Loss : 3498.77685546875, \t Total Dis Loss : 1.0892004866036586e-05\n",
      "Steps : 39700, \t Total Gen Loss : 3381.4638671875, \t Total Dis Loss : 7.511342118959874e-05\n",
      "Steps : 39800, \t Total Gen Loss : 3114.55126953125, \t Total Dis Loss : 2.8806331101804972e-05\n",
      "Steps : 39900, \t Total Gen Loss : 3269.2099609375, \t Total Dis Loss : 0.00033447681926190853\n",
      "Steps : 40000, \t Total Gen Loss : 3197.990966796875, \t Total Dis Loss : 0.0001300976873608306\n",
      "Steps : 40100, \t Total Gen Loss : 3607.331298828125, \t Total Dis Loss : 0.00011822098167613149\n",
      "Steps : 40200, \t Total Gen Loss : 3601.9951171875, \t Total Dis Loss : 6.453243258874863e-05\n",
      "Steps : 40300, \t Total Gen Loss : 3880.73291015625, \t Total Dis Loss : 7.203761924756691e-05\n",
      "Steps : 40400, \t Total Gen Loss : 3859.535400390625, \t Total Dis Loss : 2.321596912224777e-05\n",
      "Steps : 40500, \t Total Gen Loss : 3531.28076171875, \t Total Dis Loss : 4.022209395770915e-05\n",
      "Steps : 40600, \t Total Gen Loss : 3728.068603515625, \t Total Dis Loss : 2.783839954645373e-05\n",
      "Steps : 40700, \t Total Gen Loss : 3780.178955078125, \t Total Dis Loss : 0.0002912769268732518\n",
      "Steps : 40800, \t Total Gen Loss : 4318.8408203125, \t Total Dis Loss : 0.0003178472979925573\n",
      "Steps : 40900, \t Total Gen Loss : 4193.09130859375, \t Total Dis Loss : 0.00020172807853668928\n",
      "Steps : 41000, \t Total Gen Loss : 4142.9287109375, \t Total Dis Loss : 2.8886075597256422e-05\n",
      "Steps : 41100, \t Total Gen Loss : 3592.065673828125, \t Total Dis Loss : 7.13038316462189e-05\n",
      "Steps : 41200, \t Total Gen Loss : 3491.32177734375, \t Total Dis Loss : 2.7337406208971515e-05\n",
      "Steps : 41300, \t Total Gen Loss : 3737.5439453125, \t Total Dis Loss : 4.5209690142655745e-05\n",
      "Steps : 41400, \t Total Gen Loss : 3930.677001953125, \t Total Dis Loss : 2.239199420728255e-05\n",
      "Steps : 41500, \t Total Gen Loss : 3353.303466796875, \t Total Dis Loss : 2.7032454454456456e-05\n",
      "Steps : 41600, \t Total Gen Loss : 3690.42724609375, \t Total Dis Loss : 3.439396823523566e-05\n",
      "Steps : 41700, \t Total Gen Loss : 3524.585693359375, \t Total Dis Loss : 2.0362816940178163e-05\n",
      "Steps : 41800, \t Total Gen Loss : 3550.370361328125, \t Total Dis Loss : 5.0203612772747874e-05\n",
      "Steps : 41900, \t Total Gen Loss : 3296.751953125, \t Total Dis Loss : 6.873081019875826e-06\n",
      "Steps : 42000, \t Total Gen Loss : 4059.230712890625, \t Total Dis Loss : 0.0003410822828300297\n",
      "Steps : 42100, \t Total Gen Loss : 3638.65966796875, \t Total Dis Loss : 5.840430821990594e-05\n",
      "Steps : 42200, \t Total Gen Loss : 3063.679443359375, \t Total Dis Loss : 6.385134474840015e-05\n",
      "Steps : 42300, \t Total Gen Loss : 3449.2998046875, \t Total Dis Loss : 0.3297887146472931\n",
      "Steps : 42400, \t Total Gen Loss : 3360.00146484375, \t Total Dis Loss : 0.0002737877075560391\n",
      "Steps : 42500, \t Total Gen Loss : 3535.757568359375, \t Total Dis Loss : 4.456387978279963e-05\n",
      "Steps : 42600, \t Total Gen Loss : 3812.424560546875, \t Total Dis Loss : 7.561968959635124e-05\n",
      "Steps : 42700, \t Total Gen Loss : 3428.20263671875, \t Total Dis Loss : 2.8639828087761998e-05\n",
      "Steps : 42800, \t Total Gen Loss : 3121.980712890625, \t Total Dis Loss : 0.00016115074686240405\n",
      "Steps : 42900, \t Total Gen Loss : 3728.502685546875, \t Total Dis Loss : 0.00013745341857429594\n",
      "Steps : 43000, \t Total Gen Loss : 3704.4033203125, \t Total Dis Loss : 3.468030263320543e-05\n",
      "Steps : 43100, \t Total Gen Loss : 3989.928955078125, \t Total Dis Loss : 7.359341543633491e-05\n",
      "Steps : 43200, \t Total Gen Loss : 3533.056396484375, \t Total Dis Loss : 5.3023406508145854e-05\n",
      "Steps : 43300, \t Total Gen Loss : 3378.017822265625, \t Total Dis Loss : 4.028982948511839e-05\n",
      "Steps : 43400, \t Total Gen Loss : 3278.97216796875, \t Total Dis Loss : 8.57436825754121e-05\n",
      "Steps : 43500, \t Total Gen Loss : 3764.284912109375, \t Total Dis Loss : 4.6673045289935544e-05\n",
      "Steps : 43600, \t Total Gen Loss : 3923.05615234375, \t Total Dis Loss : 1.860615702753421e-05\n",
      "Steps : 43700, \t Total Gen Loss : 3767.874267578125, \t Total Dis Loss : 2.370461334066931e-05\n",
      "Steps : 43800, \t Total Gen Loss : 3636.5322265625, \t Total Dis Loss : 3.39011057803873e-05\n",
      "Steps : 43900, \t Total Gen Loss : 3328.28759765625, \t Total Dis Loss : 0.00021559788729064167\n",
      "Steps : 44000, \t Total Gen Loss : 3823.3876953125, \t Total Dis Loss : 0.00012483636965043843\n",
      "Steps : 44100, \t Total Gen Loss : 3905.451904296875, \t Total Dis Loss : 2.9855653338017873e-05\n",
      "Steps : 44200, \t Total Gen Loss : 3131.23583984375, \t Total Dis Loss : 2.9413704396574758e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 44300, \t Total Gen Loss : 3210.724365234375, \t Total Dis Loss : 1.2070908269379288e-05\n",
      "Steps : 44400, \t Total Gen Loss : 3254.631103515625, \t Total Dis Loss : 9.385468729306012e-06\n",
      "Steps : 44500, \t Total Gen Loss : 3400.48095703125, \t Total Dis Loss : 5.149648495716974e-05\n",
      "Steps : 44600, \t Total Gen Loss : 3429.75244140625, \t Total Dis Loss : 1.2355345461401157e-05\n",
      "Steps : 44700, \t Total Gen Loss : 3070.5947265625, \t Total Dis Loss : 2.247941847599577e-05\n",
      "Steps : 44800, \t Total Gen Loss : 3986.242919921875, \t Total Dis Loss : 0.0002571761142462492\n",
      "Steps : 44900, \t Total Gen Loss : 3881.951416015625, \t Total Dis Loss : 2.9438873752951622e-05\n",
      "Steps : 45000, \t Total Gen Loss : 3555.351318359375, \t Total Dis Loss : 3.423765883781016e-05\n",
      "Time for epoch 8 is 74.81211018562317 sec\n",
      "Steps : 45100, \t Total Gen Loss : 3646.308349609375, \t Total Dis Loss : 1.566576975164935e-05\n",
      "Steps : 45200, \t Total Gen Loss : 3687.973388671875, \t Total Dis Loss : 8.243112097261474e-05\n",
      "Steps : 45300, \t Total Gen Loss : 3734.65234375, \t Total Dis Loss : 4.235547748976387e-05\n",
      "Steps : 45400, \t Total Gen Loss : 3782.1669921875, \t Total Dis Loss : 2.499603942851536e-05\n",
      "Steps : 45500, \t Total Gen Loss : 3396.389404296875, \t Total Dis Loss : 1.8991113392985426e-05\n",
      "Steps : 45600, \t Total Gen Loss : 3877.923583984375, \t Total Dis Loss : 3.251113957958296e-05\n",
      "Steps : 45700, \t Total Gen Loss : 3380.70068359375, \t Total Dis Loss : 2.4416380256297998e-05\n",
      "Steps : 45800, \t Total Gen Loss : 3879.9736328125, \t Total Dis Loss : 0.00011066985462093726\n",
      "Steps : 45900, \t Total Gen Loss : 3660.969482421875, \t Total Dis Loss : 2.1252646547509357e-05\n",
      "Steps : 46000, \t Total Gen Loss : 3690.48095703125, \t Total Dis Loss : 9.66199968388537e-06\n",
      "Steps : 46100, \t Total Gen Loss : 3244.7021484375, \t Total Dis Loss : 0.000770416168961674\n",
      "Steps : 46200, \t Total Gen Loss : 4036.199951171875, \t Total Dis Loss : 0.00026143324794247746\n",
      "Steps : 46300, \t Total Gen Loss : 3374.269775390625, \t Total Dis Loss : 0.0003795502125285566\n",
      "Steps : 46400, \t Total Gen Loss : 3512.41845703125, \t Total Dis Loss : 5.2404859161470085e-05\n",
      "Steps : 46500, \t Total Gen Loss : 3183.028076171875, \t Total Dis Loss : 5.899342068005353e-05\n",
      "Steps : 46600, \t Total Gen Loss : 3492.30078125, \t Total Dis Loss : 6.479989679064602e-05\n",
      "Steps : 46700, \t Total Gen Loss : 3847.76171875, \t Total Dis Loss : 4.505717151914723e-05\n",
      "Steps : 46800, \t Total Gen Loss : 3245.2998046875, \t Total Dis Loss : 3.6701323551824316e-05\n",
      "Steps : 46900, \t Total Gen Loss : 3668.267333984375, \t Total Dis Loss : 0.004444170277565718\n",
      "Steps : 47000, \t Total Gen Loss : 4119.3994140625, \t Total Dis Loss : 4.35305482824333e-05\n",
      "Steps : 47100, \t Total Gen Loss : 4412.40673828125, \t Total Dis Loss : 9.15586861083284e-05\n",
      "Steps : 47200, \t Total Gen Loss : 3616.248046875, \t Total Dis Loss : 6.849077180959284e-05\n",
      "Steps : 47300, \t Total Gen Loss : 3813.4443359375, \t Total Dis Loss : 0.0007388670928776264\n",
      "Steps : 47400, \t Total Gen Loss : 3317.209228515625, \t Total Dis Loss : 0.00044596492080017924\n",
      "Steps : 47500, \t Total Gen Loss : 3171.201171875, \t Total Dis Loss : 6.216718611540273e-05\n",
      "Steps : 47600, \t Total Gen Loss : 3750.0546875, \t Total Dis Loss : 9.438737470190972e-05\n",
      "Steps : 47700, \t Total Gen Loss : 3620.000732421875, \t Total Dis Loss : 4.357759826234542e-05\n",
      "Steps : 47800, \t Total Gen Loss : 3360.43505859375, \t Total Dis Loss : 5.1898019592044875e-05\n",
      "Steps : 47900, \t Total Gen Loss : 3702.797607421875, \t Total Dis Loss : 9.958229202311486e-05\n",
      "Steps : 48000, \t Total Gen Loss : 3494.160888671875, \t Total Dis Loss : 0.00012806749145966023\n",
      "Steps : 48100, \t Total Gen Loss : 3133.49853515625, \t Total Dis Loss : 7.697464752709493e-05\n",
      "Steps : 48200, \t Total Gen Loss : 3364.168212890625, \t Total Dis Loss : 0.0001993008190765977\n",
      "Steps : 48300, \t Total Gen Loss : 3509.966064453125, \t Total Dis Loss : 0.00012646957475226372\n",
      "Steps : 48400, \t Total Gen Loss : 3218.974853515625, \t Total Dis Loss : 5.779935236205347e-05\n",
      "Steps : 48500, \t Total Gen Loss : 3155.499755859375, \t Total Dis Loss : 0.00016437687736470252\n",
      "Steps : 48600, \t Total Gen Loss : 3625.720703125, \t Total Dis Loss : 0.00014727446250617504\n",
      "Steps : 48700, \t Total Gen Loss : 3508.36669921875, \t Total Dis Loss : 4.5777800551149994e-05\n",
      "Steps : 48800, \t Total Gen Loss : 3772.004150390625, \t Total Dis Loss : 2.715809205255937e-05\n",
      "Steps : 48900, \t Total Gen Loss : 3446.3388671875, \t Total Dis Loss : 0.00011260998871875927\n",
      "Steps : 49000, \t Total Gen Loss : 3759.6376953125, \t Total Dis Loss : 7.111333252396435e-05\n",
      "Steps : 49100, \t Total Gen Loss : 3604.637451171875, \t Total Dis Loss : 5.287725798552856e-05\n",
      "Steps : 49200, \t Total Gen Loss : 4279.564453125, \t Total Dis Loss : 8.214869012590498e-06\n",
      "Steps : 49300, \t Total Gen Loss : 3569.340576171875, \t Total Dis Loss : 1.718442581477575e-05\n",
      "Steps : 49400, \t Total Gen Loss : 3663.443603515625, \t Total Dis Loss : 7.954236207297072e-05\n",
      "Steps : 49500, \t Total Gen Loss : 3342.022705078125, \t Total Dis Loss : 3.740807733265683e-05\n",
      "Steps : 49600, \t Total Gen Loss : 2769.993896484375, \t Total Dis Loss : 3.966401709476486e-05\n",
      "Steps : 49700, \t Total Gen Loss : 3459.635009765625, \t Total Dis Loss : 5.811323717352934e-05\n",
      "Steps : 49800, \t Total Gen Loss : 3283.587890625, \t Total Dis Loss : 0.0004698736884165555\n",
      "Steps : 49900, \t Total Gen Loss : 3901.49853515625, \t Total Dis Loss : 0.00014980233390815556\n",
      "Steps : 50000, \t Total Gen Loss : 3184.63037109375, \t Total Dis Loss : 2.5063793145818636e-05\n",
      "Steps : 50100, \t Total Gen Loss : 3766.467041015625, \t Total Dis Loss : 3.767739690374583e-05\n",
      "Steps : 50200, \t Total Gen Loss : 3405.0595703125, \t Total Dis Loss : 0.00012118547601858154\n",
      "Steps : 50300, \t Total Gen Loss : 3158.2939453125, \t Total Dis Loss : 0.008009144105017185\n",
      "Steps : 50400, \t Total Gen Loss : 3786.51708984375, \t Total Dis Loss : 0.000522808579262346\n",
      "Steps : 50500, \t Total Gen Loss : 3647.087158203125, \t Total Dis Loss : 0.0001913719461299479\n",
      "Steps : 50600, \t Total Gen Loss : 3493.18310546875, \t Total Dis Loss : 0.00047700764844194055\n",
      "Time for epoch 9 is 76.20476722717285 sec\n",
      "Steps : 50700, \t Total Gen Loss : 3347.666015625, \t Total Dis Loss : 9.102591866394505e-05\n",
      "Steps : 50800, \t Total Gen Loss : 3691.946533203125, \t Total Dis Loss : 4.050324423587881e-05\n",
      "Steps : 50900, \t Total Gen Loss : 3435.461181640625, \t Total Dis Loss : 0.005311279557645321\n",
      "Steps : 51000, \t Total Gen Loss : 3565.896484375, \t Total Dis Loss : 0.00016476922610308975\n",
      "Steps : 51100, \t Total Gen Loss : 3556.50927734375, \t Total Dis Loss : 3.486609784886241e-05\n",
      "Steps : 51200, \t Total Gen Loss : 3743.899658203125, \t Total Dis Loss : 0.009686006233096123\n",
      "Steps : 51300, \t Total Gen Loss : 3631.365234375, \t Total Dis Loss : 1.8468335838406347e-05\n",
      "Steps : 51400, \t Total Gen Loss : 3258.125, \t Total Dis Loss : 3.2108779123518616e-05\n",
      "Steps : 51500, \t Total Gen Loss : 3088.8388671875, \t Total Dis Loss : 0.00023516322835348547\n",
      "Steps : 51600, \t Total Gen Loss : 3535.02490234375, \t Total Dis Loss : 0.0001849330437835306\n",
      "Steps : 51700, \t Total Gen Loss : 4306.85400390625, \t Total Dis Loss : 0.0024162172339856625\n",
      "Steps : 51800, \t Total Gen Loss : 3736.681640625, \t Total Dis Loss : 7.039328920654953e-05\n",
      "Steps : 51900, \t Total Gen Loss : 3726.621337890625, \t Total Dis Loss : 5.701556074200198e-05\n",
      "Steps : 52000, \t Total Gen Loss : 3639.138916015625, \t Total Dis Loss : 0.001434813253581524\n",
      "Steps : 52100, \t Total Gen Loss : 4070.674560546875, \t Total Dis Loss : 0.00011717625602614135\n",
      "Steps : 52200, \t Total Gen Loss : 3203.510986328125, \t Total Dis Loss : 0.0001503009261796251\n",
      "Steps : 52300, \t Total Gen Loss : 3362.1259765625, \t Total Dis Loss : 5.667303776135668e-05\n",
      "Steps : 52400, \t Total Gen Loss : 3198.62158203125, \t Total Dis Loss : 0.00010701101564336568\n",
      "Steps : 52500, \t Total Gen Loss : 4203.423828125, \t Total Dis Loss : 7.500431820517406e-05\n",
      "Steps : 52600, \t Total Gen Loss : 3394.677490234375, \t Total Dis Loss : 3.371945058461279e-05\n",
      "Steps : 52700, \t Total Gen Loss : 3433.6591796875, \t Total Dis Loss : 9.477202547714114e-05\n",
      "Steps : 52800, \t Total Gen Loss : 3543.302734375, \t Total Dis Loss : 4.981116217095405e-05\n",
      "Steps : 52900, \t Total Gen Loss : 3754.1494140625, \t Total Dis Loss : 0.00010520554496906698\n",
      "Steps : 53000, \t Total Gen Loss : 2936.271240234375, \t Total Dis Loss : 0.00010075206955661997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 53100, \t Total Gen Loss : 4073.001708984375, \t Total Dis Loss : 4.442262070369907e-05\n",
      "Steps : 53200, \t Total Gen Loss : 3847.766357421875, \t Total Dis Loss : 5.796761251986027e-05\n",
      "Steps : 53300, \t Total Gen Loss : 3525.023681640625, \t Total Dis Loss : 4.5363489334704354e-05\n",
      "Steps : 53400, \t Total Gen Loss : 3319.4326171875, \t Total Dis Loss : 1.9475755834719166e-05\n",
      "Steps : 53500, \t Total Gen Loss : 3038.24169921875, \t Total Dis Loss : 1.3666084669239353e-05\n",
      "Steps : 53600, \t Total Gen Loss : 3751.68798828125, \t Total Dis Loss : 3.745697904378176e-05\n",
      "Steps : 53700, \t Total Gen Loss : 3872.3271484375, \t Total Dis Loss : 4.8721205530455336e-05\n",
      "Steps : 53800, \t Total Gen Loss : 3415.718017578125, \t Total Dis Loss : 3.881604425259866e-05\n",
      "Steps : 53900, \t Total Gen Loss : 3773.1171875, \t Total Dis Loss : 2.4108065190375783e-05\n",
      "Steps : 54000, \t Total Gen Loss : 3948.27001953125, \t Total Dis Loss : 2.568172749306541e-05\n",
      "Steps : 54100, \t Total Gen Loss : 3947.60693359375, \t Total Dis Loss : 2.1248499251669273e-05\n",
      "Steps : 54200, \t Total Gen Loss : 3615.622802734375, \t Total Dis Loss : 8.759813499636948e-05\n",
      "Steps : 54300, \t Total Gen Loss : 3075.463134765625, \t Total Dis Loss : 4.4691616494674236e-05\n",
      "Steps : 54400, \t Total Gen Loss : 3839.4580078125, \t Total Dis Loss : 0.0005362567026168108\n",
      "Steps : 54500, \t Total Gen Loss : 3573.7900390625, \t Total Dis Loss : 4.784729389939457e-05\n",
      "Steps : 54600, \t Total Gen Loss : 3358.49853515625, \t Total Dis Loss : 3.0690407584188506e-05\n",
      "Steps : 54700, \t Total Gen Loss : 3887.989013671875, \t Total Dis Loss : 2.7460217097541317e-05\n",
      "Steps : 54800, \t Total Gen Loss : 3433.735595703125, \t Total Dis Loss : 0.0001012283973977901\n",
      "Steps : 54900, \t Total Gen Loss : 3472.868896484375, \t Total Dis Loss : 0.0004788812075275928\n",
      "Steps : 55000, \t Total Gen Loss : 3635.91259765625, \t Total Dis Loss : 6.1014074162812904e-05\n",
      "Steps : 55100, \t Total Gen Loss : 3434.8857421875, \t Total Dis Loss : 1.8978596926899627e-05\n",
      "Steps : 55200, \t Total Gen Loss : 3877.93212890625, \t Total Dis Loss : 0.0001856346061686054\n",
      "Steps : 55300, \t Total Gen Loss : 3843.751708984375, \t Total Dis Loss : 1.2541953325271606\n",
      "Steps : 55400, \t Total Gen Loss : 3234.6748046875, \t Total Dis Loss : 0.03646230697631836\n",
      "Steps : 55500, \t Total Gen Loss : 3480.91552734375, \t Total Dis Loss : 0.0001529616565676406\n",
      "Steps : 55600, \t Total Gen Loss : 3088.70849609375, \t Total Dis Loss : 0.00012314242485444993\n",
      "Steps : 55700, \t Total Gen Loss : 3253.1357421875, \t Total Dis Loss : 3.329171522636898e-05\n",
      "Steps : 55800, \t Total Gen Loss : 3491.21484375, \t Total Dis Loss : 4.3339066905900836e-05\n",
      "Steps : 55900, \t Total Gen Loss : 3608.911865234375, \t Total Dis Loss : 5.134059756528586e-05\n",
      "Steps : 56000, \t Total Gen Loss : 3926.642333984375, \t Total Dis Loss : 8.886616706149653e-05\n",
      "Steps : 56100, \t Total Gen Loss : 3717.14453125, \t Total Dis Loss : 9.148222306976095e-05\n",
      "Steps : 56200, \t Total Gen Loss : 3251.9296875, \t Total Dis Loss : 7.181397813837975e-05\n",
      "Time for epoch 10 is 79.08809542655945 sec\n",
      "Steps : 56300, \t Total Gen Loss : 3165.62255859375, \t Total Dis Loss : 4.565375274978578e-05\n",
      "Steps : 56400, \t Total Gen Loss : 3842.580810546875, \t Total Dis Loss : 3.7620662624249235e-05\n",
      "Steps : 56500, \t Total Gen Loss : 3624.3896484375, \t Total Dis Loss : 1.945879921549931e-05\n",
      "Steps : 56600, \t Total Gen Loss : 3612.075439453125, \t Total Dis Loss : 5.022461118642241e-05\n",
      "Steps : 56700, \t Total Gen Loss : 3680.668701171875, \t Total Dis Loss : 7.086067489581183e-05\n",
      "Steps : 56800, \t Total Gen Loss : 3479.203125, \t Total Dis Loss : 0.0002505119191482663\n",
      "Steps : 56900, \t Total Gen Loss : 3422.07275390625, \t Total Dis Loss : 4.0045557398116216e-05\n",
      "Steps : 57000, \t Total Gen Loss : 3345.841064453125, \t Total Dis Loss : 4.4651114876614884e-05\n",
      "Steps : 57100, \t Total Gen Loss : 3916.3759765625, \t Total Dis Loss : 6.947178189875558e-05\n",
      "Steps : 57200, \t Total Gen Loss : 3833.621826171875, \t Total Dis Loss : 3.3336400520056486e-05\n",
      "Steps : 57300, \t Total Gen Loss : 3529.508544921875, \t Total Dis Loss : 0.0002804284740705043\n",
      "Steps : 57400, \t Total Gen Loss : 3608.71337890625, \t Total Dis Loss : 3.72649883502163e-05\n",
      "Steps : 57500, \t Total Gen Loss : 3090.399169921875, \t Total Dis Loss : 6.382903666235507e-05\n",
      "Steps : 57600, \t Total Gen Loss : 3620.578857421875, \t Total Dis Loss : 5.0244190788362175e-05\n",
      "Steps : 57700, \t Total Gen Loss : 3371.986328125, \t Total Dis Loss : 0.00017419249343220145\n",
      "Steps : 57800, \t Total Gen Loss : 3286.354736328125, \t Total Dis Loss : 1.6568727005505934e-05\n",
      "Steps : 57900, \t Total Gen Loss : 3537.789794921875, \t Total Dis Loss : 0.00010981793457176536\n",
      "Steps : 58000, \t Total Gen Loss : 3630.28564453125, \t Total Dis Loss : 4.560446905088611e-05\n",
      "Steps : 58100, \t Total Gen Loss : 3626.7626953125, \t Total Dis Loss : 4.383760460768826e-05\n",
      "Steps : 58200, \t Total Gen Loss : 3575.77099609375, \t Total Dis Loss : 0.00031262339325621724\n",
      "Steps : 58300, \t Total Gen Loss : 3961.39208984375, \t Total Dis Loss : 7.375398126896471e-05\n",
      "Steps : 58400, \t Total Gen Loss : 3833.03076171875, \t Total Dis Loss : 0.0002814059844240546\n",
      "Steps : 58500, \t Total Gen Loss : 3520.246337890625, \t Total Dis Loss : 3.254889088566415e-05\n",
      "Steps : 58600, \t Total Gen Loss : 3852.66162109375, \t Total Dis Loss : 1.5053760762384627e-05\n",
      "Steps : 58700, \t Total Gen Loss : 3694.724853515625, \t Total Dis Loss : 0.00016899226466193795\n",
      "Steps : 58800, \t Total Gen Loss : 3426.58447265625, \t Total Dis Loss : 2.0597592083504423e-05\n",
      "Steps : 58900, \t Total Gen Loss : 3642.341796875, \t Total Dis Loss : 0.007455243729054928\n",
      "Steps : 59000, \t Total Gen Loss : 3873.73876953125, \t Total Dis Loss : 6.280582601903006e-05\n",
      "Steps : 59100, \t Total Gen Loss : 3591.10595703125, \t Total Dis Loss : 6.44600804662332e-05\n",
      "Steps : 59200, \t Total Gen Loss : 3650.372802734375, \t Total Dis Loss : 5.712593701900914e-05\n",
      "Steps : 59300, \t Total Gen Loss : 3409.543701171875, \t Total Dis Loss : 6.573289283551276e-05\n",
      "Steps : 59400, \t Total Gen Loss : 3109.622802734375, \t Total Dis Loss : 0.0002859768574126065\n",
      "Steps : 59500, \t Total Gen Loss : 3883.26953125, \t Total Dis Loss : 9.297626820625737e-05\n",
      "Steps : 59600, \t Total Gen Loss : 3860.442626953125, \t Total Dis Loss : 4.3529144022613764e-05\n",
      "Steps : 59700, \t Total Gen Loss : 3554.2314453125, \t Total Dis Loss : 0.00017224869225174189\n",
      "Steps : 59800, \t Total Gen Loss : 3438.98583984375, \t Total Dis Loss : 6.864824536023661e-05\n",
      "Steps : 59900, \t Total Gen Loss : 3532.78466796875, \t Total Dis Loss : 2.2360651200870052e-05\n",
      "Steps : 60000, \t Total Gen Loss : 2879.435791015625, \t Total Dis Loss : 2.6573183276923373e-05\n",
      "Steps : 60100, \t Total Gen Loss : 3220.490966796875, \t Total Dis Loss : 2.574056816229131e-05\n",
      "Steps : 60200, \t Total Gen Loss : 3262.098876953125, \t Total Dis Loss : 2.695102011784911e-05\n",
      "Steps : 60300, \t Total Gen Loss : 3388.802734375, \t Total Dis Loss : 2.267578565806616e-05\n",
      "Steps : 60400, \t Total Gen Loss : 3719.610595703125, \t Total Dis Loss : 0.00913982093334198\n",
      "Steps : 60500, \t Total Gen Loss : 3013.08984375, \t Total Dis Loss : 1.0667346941772848e-05\n",
      "Steps : 60600, \t Total Gen Loss : 4160.44189453125, \t Total Dis Loss : 1.95661104953615e-05\n",
      "Steps : 60700, \t Total Gen Loss : 3611.212158203125, \t Total Dis Loss : 8.007498399820179e-05\n",
      "Steps : 60800, \t Total Gen Loss : 3651.18017578125, \t Total Dis Loss : 2.7354124540579505e-05\n",
      "Steps : 60900, \t Total Gen Loss : 3296.90673828125, \t Total Dis Loss : 2.9515504138544202e-05\n",
      "Steps : 61000, \t Total Gen Loss : 3518.669189453125, \t Total Dis Loss : 0.00031785015016794205\n",
      "Steps : 61100, \t Total Gen Loss : 3320.35791015625, \t Total Dis Loss : 1.0191753972321749e-05\n",
      "Steps : 61200, \t Total Gen Loss : 3322.322265625, \t Total Dis Loss : 4.241229180479422e-05\n",
      "Steps : 61300, \t Total Gen Loss : 3647.435791015625, \t Total Dis Loss : 0.00016252024215646088\n",
      "Steps : 61400, \t Total Gen Loss : 3859.532470703125, \t Total Dis Loss : 0.00013330267393030226\n",
      "Steps : 61500, \t Total Gen Loss : 3309.159912109375, \t Total Dis Loss : 8.968899783212692e-05\n",
      "Steps : 61600, \t Total Gen Loss : 3606.970947265625, \t Total Dis Loss : 2.454099740134552e-05\n",
      "Steps : 61700, \t Total Gen Loss : 3868.418701171875, \t Total Dis Loss : 1.5501496818615124e-05\n",
      "Steps : 61800, \t Total Gen Loss : 3402.98095703125, \t Total Dis Loss : 1.9066794266109355e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 11 is 74.97103595733643 sec\n",
      "Steps : 61900, \t Total Gen Loss : 3507.174072265625, \t Total Dis Loss : 2.021022555709351e-05\n",
      "Steps : 62000, \t Total Gen Loss : 3659.32958984375, \t Total Dis Loss : 9.745895113155711e-06\n",
      "Steps : 62100, \t Total Gen Loss : 3377.12109375, \t Total Dis Loss : 3.0461413189186715e-05\n",
      "Steps : 62200, \t Total Gen Loss : 4718.05078125, \t Total Dis Loss : 1.0354222467867658e-05\n",
      "Steps : 62300, \t Total Gen Loss : 3923.691162109375, \t Total Dis Loss : 3.165995076415129e-05\n",
      "Steps : 62400, \t Total Gen Loss : 3576.2177734375, \t Total Dis Loss : 7.182310946518555e-05\n",
      "Steps : 62500, \t Total Gen Loss : 3792.73779296875, \t Total Dis Loss : 2.8995815227972344e-05\n",
      "Steps : 62600, \t Total Gen Loss : 3583.947998046875, \t Total Dis Loss : 2.29397392104147e-05\n",
      "Steps : 62700, \t Total Gen Loss : 3540.760498046875, \t Total Dis Loss : 4.1351573599968106e-05\n",
      "Steps : 62800, \t Total Gen Loss : 3556.51953125, \t Total Dis Loss : 4.4583350245375186e-05\n",
      "Steps : 62900, \t Total Gen Loss : 3630.8974609375, \t Total Dis Loss : 1.950617297552526e-05\n",
      "Steps : 63000, \t Total Gen Loss : 3617.54443359375, \t Total Dis Loss : 6.901807864778675e-06\n",
      "Steps : 63100, \t Total Gen Loss : 3563.763427734375, \t Total Dis Loss : 1.2434985364961904e-05\n",
      "Steps : 63200, \t Total Gen Loss : 3939.04736328125, \t Total Dis Loss : 5.91916250414215e-06\n",
      "Steps : 63300, \t Total Gen Loss : 3395.28271484375, \t Total Dis Loss : 2.786878394545056e-05\n",
      "Steps : 63400, \t Total Gen Loss : 3763.3623046875, \t Total Dis Loss : 0.00018921752052847296\n",
      "Steps : 63500, \t Total Gen Loss : 4153.40869140625, \t Total Dis Loss : 1.6902913557714783e-05\n",
      "Steps : 63600, \t Total Gen Loss : 3758.528076171875, \t Total Dis Loss : 0.0004851986304856837\n",
      "Steps : 63700, \t Total Gen Loss : 3240.05859375, \t Total Dis Loss : 6.477830174844712e-05\n",
      "Steps : 63800, \t Total Gen Loss : 3443.478515625, \t Total Dis Loss : 6.003597445669584e-05\n",
      "Steps : 63900, \t Total Gen Loss : 3181.8828125, \t Total Dis Loss : 1.0506038961466402e-05\n",
      "Steps : 64000, \t Total Gen Loss : 3905.35888671875, \t Total Dis Loss : 0.00011096915841335431\n",
      "Steps : 64100, \t Total Gen Loss : 3831.755859375, \t Total Dis Loss : 0.0004960891092196107\n",
      "Steps : 64200, \t Total Gen Loss : 3387.95947265625, \t Total Dis Loss : 2.891243639169261e-05\n",
      "Steps : 64300, \t Total Gen Loss : 3505.63525390625, \t Total Dis Loss : 3.0273582524387166e-05\n",
      "Steps : 64400, \t Total Gen Loss : 3306.425537109375, \t Total Dis Loss : 0.0002064924337901175\n",
      "Steps : 64500, \t Total Gen Loss : 3360.656494140625, \t Total Dis Loss : 0.002552935155108571\n",
      "Steps : 64600, \t Total Gen Loss : 3633.47412109375, \t Total Dis Loss : 0.00018043641466647387\n",
      "Steps : 64700, \t Total Gen Loss : 3284.45068359375, \t Total Dis Loss : 0.00021024713350925595\n",
      "Steps : 64800, \t Total Gen Loss : 3804.631103515625, \t Total Dis Loss : 8.364066889043897e-05\n",
      "Steps : 64900, \t Total Gen Loss : 3778.9736328125, \t Total Dis Loss : 0.00019022944616153836\n",
      "Steps : 65000, \t Total Gen Loss : 3367.0947265625, \t Total Dis Loss : 4.1119870729744434e-05\n",
      "Steps : 65100, \t Total Gen Loss : 3536.534423828125, \t Total Dis Loss : 0.00022593248286284506\n",
      "Steps : 65200, \t Total Gen Loss : 3388.89892578125, \t Total Dis Loss : 4.3816035031341016e-05\n",
      "Steps : 65300, \t Total Gen Loss : 3328.502197265625, \t Total Dis Loss : 0.001095097279176116\n",
      "Steps : 65400, \t Total Gen Loss : 3380.51904296875, \t Total Dis Loss : 4.798089503310621e-05\n",
      "Steps : 65500, \t Total Gen Loss : 3463.279541015625, \t Total Dis Loss : 1.859843177953735e-05\n",
      "Steps : 65600, \t Total Gen Loss : 3765.3505859375, \t Total Dis Loss : 4.93558072776068e-05\n",
      "Steps : 65700, \t Total Gen Loss : 3330.545654296875, \t Total Dis Loss : 0.0003090091049671173\n",
      "Steps : 65800, \t Total Gen Loss : 3557.62841796875, \t Total Dis Loss : 0.00010213819768978283\n",
      "Steps : 65900, \t Total Gen Loss : 3315.459716796875, \t Total Dis Loss : 2.4762974135228433e-05\n",
      "Steps : 66000, \t Total Gen Loss : 4157.9619140625, \t Total Dis Loss : 1.3263127584650647e-05\n",
      "Steps : 66100, \t Total Gen Loss : 3691.25634765625, \t Total Dis Loss : 4.45867954113055e-05\n",
      "Steps : 66200, \t Total Gen Loss : 4081.732666015625, \t Total Dis Loss : 0.0001097168933483772\n",
      "Steps : 66300, \t Total Gen Loss : 3932.316650390625, \t Total Dis Loss : 1.5472138329641894e-05\n",
      "Steps : 66400, \t Total Gen Loss : 3273.1376953125, \t Total Dis Loss : 0.00019699348194990307\n",
      "Steps : 66500, \t Total Gen Loss : 3977.733154296875, \t Total Dis Loss : 2.4083023163257167e-05\n",
      "Steps : 66600, \t Total Gen Loss : 3814.330810546875, \t Total Dis Loss : 1.2560723007482011e-05\n",
      "Steps : 66700, \t Total Gen Loss : 3308.579833984375, \t Total Dis Loss : 6.574256258318201e-05\n",
      "Steps : 66800, \t Total Gen Loss : 3782.708251953125, \t Total Dis Loss : 2.0491830582614057e-05\n",
      "Steps : 66900, \t Total Gen Loss : 3291.90283203125, \t Total Dis Loss : 2.7993653930025175e-05\n",
      "Steps : 67000, \t Total Gen Loss : 3534.211181640625, \t Total Dis Loss : 4.6452536480501294e-05\n",
      "Steps : 67100, \t Total Gen Loss : 3511.4404296875, \t Total Dis Loss : 7.331898814300075e-05\n",
      "Steps : 67200, \t Total Gen Loss : 3027.296875, \t Total Dis Loss : 4.169172461843118e-05\n",
      "Steps : 67300, \t Total Gen Loss : 4307.77880859375, \t Total Dis Loss : 7.399716560030356e-05\n",
      "Steps : 67400, \t Total Gen Loss : 3930.4365234375, \t Total Dis Loss : 1.186488225357607e-05\n",
      "Steps : 67500, \t Total Gen Loss : 3099.930908203125, \t Total Dis Loss : 4.21796903538052e-05\n",
      "Time for epoch 12 is 75.83545017242432 sec\n",
      "Steps : 67600, \t Total Gen Loss : 4209.38671875, \t Total Dis Loss : 1.491172588430345e-05\n",
      "Steps : 67700, \t Total Gen Loss : 4113.61572265625, \t Total Dis Loss : 8.593621896579862e-05\n",
      "Steps : 67800, \t Total Gen Loss : 3425.793212890625, \t Total Dis Loss : 1.3734881576965563e-05\n",
      "Steps : 67900, \t Total Gen Loss : 4470.50244140625, \t Total Dis Loss : 5.1704031648114324e-05\n",
      "Steps : 68000, \t Total Gen Loss : 3459.34326171875, \t Total Dis Loss : 1.553422953293193e-05\n",
      "Steps : 68100, \t Total Gen Loss : 3520.967529296875, \t Total Dis Loss : 1.4240473319659941e-05\n",
      "Steps : 68200, \t Total Gen Loss : 3770.4033203125, \t Total Dis Loss : 1.5930423614918254e-05\n",
      "Steps : 68300, \t Total Gen Loss : 3626.503662109375, \t Total Dis Loss : 0.00010353492689318955\n",
      "Steps : 68400, \t Total Gen Loss : 3980.489013671875, \t Total Dis Loss : 5.257026350591332e-05\n",
      "Steps : 68500, \t Total Gen Loss : 3571.076171875, \t Total Dis Loss : 2.8149299396318384e-05\n",
      "Steps : 68600, \t Total Gen Loss : 3253.072265625, \t Total Dis Loss : 1.5374660506495275e-05\n",
      "Steps : 68700, \t Total Gen Loss : 3735.887939453125, \t Total Dis Loss : 6.78452033753274e-06\n",
      "Steps : 68800, \t Total Gen Loss : 3919.3125, \t Total Dis Loss : 0.0007958285277709365\n",
      "Steps : 68900, \t Total Gen Loss : 3470.946044921875, \t Total Dis Loss : 0.00017222976021002978\n",
      "Steps : 69000, \t Total Gen Loss : 3772.8251953125, \t Total Dis Loss : 9.067958671948873e-06\n",
      "Steps : 69100, \t Total Gen Loss : 3366.5654296875, \t Total Dis Loss : 3.397736145416275e-05\n",
      "Steps : 69200, \t Total Gen Loss : 3184.59912109375, \t Total Dis Loss : 1.2050034456478897e-05\n",
      "Steps : 69300, \t Total Gen Loss : 3922.4453125, \t Total Dis Loss : 0.00019194002379663289\n",
      "Steps : 69400, \t Total Gen Loss : 3563.363037109375, \t Total Dis Loss : 1.6803045582491904e-05\n",
      "Steps : 69500, \t Total Gen Loss : 3928.28076171875, \t Total Dis Loss : 5.0812039262382314e-05\n",
      "Steps : 69600, \t Total Gen Loss : 3060.049560546875, \t Total Dis Loss : 8.892585174180567e-05\n",
      "Steps : 69700, \t Total Gen Loss : 3615.03369140625, \t Total Dis Loss : 0.00013932927686255425\n",
      "Steps : 69800, \t Total Gen Loss : 3926.850341796875, \t Total Dis Loss : 1.7447480786358938e-05\n",
      "Steps : 69900, \t Total Gen Loss : 3217.866943359375, \t Total Dis Loss : 0.0001505877880845219\n",
      "Steps : 70000, \t Total Gen Loss : 3289.30517578125, \t Total Dis Loss : 8.891598554328084e-05\n",
      "Steps : 70100, \t Total Gen Loss : 4356.56689453125, \t Total Dis Loss : 1.2055021215928718e-05\n",
      "Steps : 70200, \t Total Gen Loss : 3791.1025390625, \t Total Dis Loss : 5.494867582456209e-05\n",
      "Steps : 70300, \t Total Gen Loss : 3471.17724609375, \t Total Dis Loss : 0.0001135167694883421\n",
      "Steps : 70400, \t Total Gen Loss : 3296.620849609375, \t Total Dis Loss : 0.0001426946255378425\n",
      "Steps : 70500, \t Total Gen Loss : 3537.278564453125, \t Total Dis Loss : 2.3728895030217245e-05\n",
      "Steps : 70600, \t Total Gen Loss : 3377.33544921875, \t Total Dis Loss : 9.982871415559202e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 70700, \t Total Gen Loss : 3447.673095703125, \t Total Dis Loss : 8.374858589377254e-05\n",
      "Steps : 70800, \t Total Gen Loss : 2993.7470703125, \t Total Dis Loss : 5.199558290769346e-05\n",
      "Steps : 70900, \t Total Gen Loss : 3800.5146484375, \t Total Dis Loss : 4.434376023709774e-05\n",
      "Steps : 71000, \t Total Gen Loss : 3746.093994140625, \t Total Dis Loss : 4.20420037698932e-05\n",
      "Steps : 71100, \t Total Gen Loss : 3840.1064453125, \t Total Dis Loss : 2.3193682864075527e-05\n",
      "Steps : 71200, \t Total Gen Loss : 3707.3974609375, \t Total Dis Loss : 3.5191362258046865e-05\n",
      "Steps : 71300, \t Total Gen Loss : 2955.763427734375, \t Total Dis Loss : 8.063498171395622e-06\n",
      "Steps : 71400, \t Total Gen Loss : 3392.80859375, \t Total Dis Loss : 7.728307537036017e-06\n",
      "Steps : 71500, \t Total Gen Loss : 3140.037841796875, \t Total Dis Loss : 1.7850101357907988e-05\n",
      "Steps : 71600, \t Total Gen Loss : 3773.742919921875, \t Total Dis Loss : 6.605220551136881e-05\n",
      "Steps : 71700, \t Total Gen Loss : 3009.825927734375, \t Total Dis Loss : 0.00012791322660632432\n",
      "Steps : 71800, \t Total Gen Loss : 3697.23583984375, \t Total Dis Loss : 7.313139212783426e-05\n",
      "Steps : 71900, \t Total Gen Loss : 3178.271240234375, \t Total Dis Loss : 5.7009139709407464e-05\n",
      "Steps : 72000, \t Total Gen Loss : 3335.71435546875, \t Total Dis Loss : 0.003473048796877265\n",
      "Steps : 72100, \t Total Gen Loss : 3316.186279296875, \t Total Dis Loss : 4.909304334432818e-05\n",
      "Steps : 72200, \t Total Gen Loss : 3372.3134765625, \t Total Dis Loss : 0.00018047353660222143\n",
      "Steps : 72300, \t Total Gen Loss : 3312.47998046875, \t Total Dis Loss : 4.897252802038565e-05\n",
      "Steps : 72400, \t Total Gen Loss : 3747.16064453125, \t Total Dis Loss : 4.933085438096896e-05\n",
      "Steps : 72500, \t Total Gen Loss : 4112.900390625, \t Total Dis Loss : 4.3606734834611416e-05\n",
      "Steps : 72600, \t Total Gen Loss : 3102.159423828125, \t Total Dis Loss : 2.827394928317517e-05\n",
      "Steps : 72700, \t Total Gen Loss : 3813.98876953125, \t Total Dis Loss : 0.0012278166832402349\n",
      "Steps : 72800, \t Total Gen Loss : 3772.7060546875, \t Total Dis Loss : 0.001149365329183638\n",
      "Steps : 72900, \t Total Gen Loss : 3413.5888671875, \t Total Dis Loss : 4.202632771921344e-05\n",
      "Steps : 73000, \t Total Gen Loss : 3634.70556640625, \t Total Dis Loss : 1.165818321169354e-05\n",
      "Steps : 73100, \t Total Gen Loss : 3704.863525390625, \t Total Dis Loss : 2.4192084310925566e-05\n",
      "Time for epoch 13 is 76.28380703926086 sec\n",
      "Steps : 73200, \t Total Gen Loss : 3296.654296875, \t Total Dis Loss : 6.894658781675389e-06\n",
      "Steps : 73300, \t Total Gen Loss : 3373.85205078125, \t Total Dis Loss : 0.0003293162153568119\n",
      "Steps : 73400, \t Total Gen Loss : 4198.76220703125, \t Total Dis Loss : 0.0001424695656169206\n",
      "Steps : 73500, \t Total Gen Loss : 3424.27587890625, \t Total Dis Loss : 0.0032526387367397547\n",
      "Steps : 73600, \t Total Gen Loss : 3568.92041015625, \t Total Dis Loss : 7.412531704176217e-05\n",
      "Steps : 73700, \t Total Gen Loss : 3908.829345703125, \t Total Dis Loss : 0.0001322831813013181\n",
      "Steps : 73800, \t Total Gen Loss : 4174.86865234375, \t Total Dis Loss : 0.00014916015788912773\n",
      "Steps : 73900, \t Total Gen Loss : 3683.14404296875, \t Total Dis Loss : 0.0001131625467678532\n",
      "Steps : 74000, \t Total Gen Loss : 3490.81005859375, \t Total Dis Loss : 4.437390816747211e-05\n",
      "Steps : 74100, \t Total Gen Loss : 3853.36572265625, \t Total Dis Loss : 0.001961774891242385\n",
      "Steps : 74200, \t Total Gen Loss : 2789.705810546875, \t Total Dis Loss : 0.0004941698280163109\n",
      "Steps : 74300, \t Total Gen Loss : 3621.049560546875, \t Total Dis Loss : 2.6861362130148336e-05\n",
      "Steps : 74400, \t Total Gen Loss : 3793.381591796875, \t Total Dis Loss : 1.4377790648723021e-05\n",
      "Steps : 74500, \t Total Gen Loss : 3765.9072265625, \t Total Dis Loss : 1.3513058547687251e-05\n",
      "Steps : 74600, \t Total Gen Loss : 3749.998779296875, \t Total Dis Loss : 3.7836369301658124e-05\n",
      "Steps : 74700, \t Total Gen Loss : 3601.400146484375, \t Total Dis Loss : 3.751769691007212e-05\n",
      "Steps : 74800, \t Total Gen Loss : 3995.2490234375, \t Total Dis Loss : 9.783259883988649e-06\n",
      "Steps : 74900, \t Total Gen Loss : 3429.685791015625, \t Total Dis Loss : 2.1168105377000757e-05\n",
      "Steps : 75000, \t Total Gen Loss : 3546.4921875, \t Total Dis Loss : 1.3355514965951443e-05\n",
      "Steps : 75100, \t Total Gen Loss : 3344.947509765625, \t Total Dis Loss : 3.6463115975493565e-05\n",
      "Steps : 75200, \t Total Gen Loss : 3541.524658203125, \t Total Dis Loss : 1.4022813047631644e-05\n",
      "Steps : 75300, \t Total Gen Loss : 3766.853271484375, \t Total Dis Loss : 2.004892121476587e-05\n",
      "Steps : 75400, \t Total Gen Loss : 3448.6083984375, \t Total Dis Loss : 6.950591341592371e-05\n",
      "Steps : 75500, \t Total Gen Loss : 3611.410888671875, \t Total Dis Loss : 0.00016679904365446419\n",
      "Steps : 75600, \t Total Gen Loss : 3764.237548828125, \t Total Dis Loss : 8.974794582172763e-06\n",
      "Steps : 75700, \t Total Gen Loss : 3598.181396484375, \t Total Dis Loss : 1.0066430149890948e-05\n",
      "Steps : 75800, \t Total Gen Loss : 3781.48095703125, \t Total Dis Loss : 6.113524432294071e-05\n",
      "Steps : 75900, \t Total Gen Loss : 3968.248779296875, \t Total Dis Loss : 5.7642668252810836e-05\n",
      "Steps : 76000, \t Total Gen Loss : 3732.890869140625, \t Total Dis Loss : 0.000543312169611454\n",
      "Steps : 76100, \t Total Gen Loss : 3559.44091796875, \t Total Dis Loss : 0.00018514909606892616\n",
      "Steps : 76200, \t Total Gen Loss : 3802.05126953125, \t Total Dis Loss : 2.0663106624851935e-05\n",
      "Steps : 76300, \t Total Gen Loss : 2731.944091796875, \t Total Dis Loss : 2.0866642444161698e-05\n",
      "Steps : 76400, \t Total Gen Loss : 3785.091796875, \t Total Dis Loss : 6.378589750966057e-05\n",
      "Steps : 76500, \t Total Gen Loss : 3810.1123046875, \t Total Dis Loss : 3.2957854273263365e-05\n",
      "Steps : 76600, \t Total Gen Loss : 3907.167724609375, \t Total Dis Loss : 4.4457025069277734e-05\n",
      "Steps : 76700, \t Total Gen Loss : 3996.214111328125, \t Total Dis Loss : 2.7367106667952612e-05\n",
      "Steps : 76800, \t Total Gen Loss : 3895.008544921875, \t Total Dis Loss : 2.851369936252013e-05\n",
      "Steps : 76900, \t Total Gen Loss : 3216.044189453125, \t Total Dis Loss : 0.00010268329060636461\n",
      "Steps : 77000, \t Total Gen Loss : 3785.431396484375, \t Total Dis Loss : 2.8964646844542585e-05\n",
      "Steps : 77100, \t Total Gen Loss : 3733.630615234375, \t Total Dis Loss : 9.480951121076941e-06\n",
      "Steps : 77200, \t Total Gen Loss : 3558.73828125, \t Total Dis Loss : 1.8653900042409077e-05\n",
      "Steps : 77300, \t Total Gen Loss : 3701.744384765625, \t Total Dis Loss : 7.838235615054145e-05\n",
      "Steps : 77400, \t Total Gen Loss : 3834.064697265625, \t Total Dis Loss : 1.8173021089751273e-05\n",
      "Steps : 77500, \t Total Gen Loss : 3671.208984375, \t Total Dis Loss : 3.71138630725909e-05\n",
      "Steps : 77600, \t Total Gen Loss : 3553.774658203125, \t Total Dis Loss : 1.3644307728100102e-05\n",
      "Steps : 77700, \t Total Gen Loss : 3569.359375, \t Total Dis Loss : 3.5604985896497965e-05\n",
      "Steps : 77800, \t Total Gen Loss : 3177.706787109375, \t Total Dis Loss : 0.00047924573300406337\n",
      "Steps : 77900, \t Total Gen Loss : 3928.2470703125, \t Total Dis Loss : 6.9988782342989e-05\n",
      "Steps : 78000, \t Total Gen Loss : 3383.819091796875, \t Total Dis Loss : 5.9441088524181396e-05\n",
      "Steps : 78100, \t Total Gen Loss : 3902.497802734375, \t Total Dis Loss : 1.2302232789807022e-05\n",
      "Steps : 78200, \t Total Gen Loss : 3217.54443359375, \t Total Dis Loss : 1.9773520762100816e-05\n",
      "Steps : 78300, \t Total Gen Loss : 3692.392333984375, \t Total Dis Loss : 2.1674677554983646e-05\n",
      "Steps : 78400, \t Total Gen Loss : 3340.206298828125, \t Total Dis Loss : 1.3155367014405783e-05\n",
      "Steps : 78500, \t Total Gen Loss : 3408.71630859375, \t Total Dis Loss : 5.6121461966540664e-05\n",
      "Steps : 78600, \t Total Gen Loss : 3755.09814453125, \t Total Dis Loss : 7.902514335000888e-05\n",
      "Steps : 78700, \t Total Gen Loss : 3717.12744140625, \t Total Dis Loss : 0.0005159564898349345\n",
      "Time for epoch 14 is 76.71157741546631 sec\n",
      "Steps : 78800, \t Total Gen Loss : 3545.25244140625, \t Total Dis Loss : 0.0002161363954655826\n",
      "Steps : 78900, \t Total Gen Loss : 3216.302490234375, \t Total Dis Loss : 0.00011749799887184054\n",
      "Steps : 79000, \t Total Gen Loss : 3516.95068359375, \t Total Dis Loss : 6.850123463664204e-05\n",
      "Steps : 79100, \t Total Gen Loss : 3399.209716796875, \t Total Dis Loss : 6.862328882561997e-05\n",
      "Steps : 79200, \t Total Gen Loss : 3902.96142578125, \t Total Dis Loss : 0.00013011934061069041\n",
      "Steps : 79300, \t Total Gen Loss : 3098.918701171875, \t Total Dis Loss : 9.839718586590607e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 79400, \t Total Gen Loss : 3650.748779296875, \t Total Dis Loss : 3.1658069929108024e-05\n",
      "Steps : 79500, \t Total Gen Loss : 3792.8447265625, \t Total Dis Loss : 9.30558453546837e-05\n",
      "Steps : 79600, \t Total Gen Loss : 3633.408935546875, \t Total Dis Loss : 1.2083696674380917e-05\n",
      "Steps : 79700, \t Total Gen Loss : 3376.470703125, \t Total Dis Loss : 2.4715451218071394e-05\n",
      "Steps : 79800, \t Total Gen Loss : 3727.981689453125, \t Total Dis Loss : 0.0002913610078394413\n",
      "Steps : 79900, \t Total Gen Loss : 3986.337158203125, \t Total Dis Loss : 0.00011519210238475353\n",
      "Steps : 80000, \t Total Gen Loss : 3269.3662109375, \t Total Dis Loss : 3.173595177941024e-05\n",
      "Steps : 80100, \t Total Gen Loss : 3617.950927734375, \t Total Dis Loss : 0.0001375592255499214\n",
      "Steps : 80200, \t Total Gen Loss : 3970.7060546875, \t Total Dis Loss : 2.3078178855939768e-05\n",
      "Steps : 80300, \t Total Gen Loss : 3193.873779296875, \t Total Dis Loss : 1.8916216504294425e-05\n",
      "Steps : 80400, \t Total Gen Loss : 3581.260009765625, \t Total Dis Loss : 2.789995960483793e-05\n",
      "Steps : 80500, \t Total Gen Loss : 3653.145751953125, \t Total Dis Loss : 2.6787674869410694e-05\n",
      "Steps : 80600, \t Total Gen Loss : 3869.860107421875, \t Total Dis Loss : 4.372630883153761e-06\n",
      "Steps : 80700, \t Total Gen Loss : 3923.333251953125, \t Total Dis Loss : 0.004311565775424242\n",
      "Steps : 80800, \t Total Gen Loss : 4142.3681640625, \t Total Dis Loss : 2.8622527679544874e-05\n",
      "Steps : 80900, \t Total Gen Loss : 3561.840087890625, \t Total Dis Loss : 6.306541035883129e-05\n",
      "Steps : 81000, \t Total Gen Loss : 3875.609375, \t Total Dis Loss : 2.8003154511679895e-05\n",
      "Steps : 81100, \t Total Gen Loss : 3283.734619140625, \t Total Dis Loss : 9.856345059233718e-06\n",
      "Steps : 81200, \t Total Gen Loss : 3740.0126953125, \t Total Dis Loss : 0.00013530142314266413\n",
      "Steps : 81300, \t Total Gen Loss : 3302.283447265625, \t Total Dis Loss : 3.972972626797855e-05\n",
      "Steps : 81400, \t Total Gen Loss : 3356.40673828125, \t Total Dis Loss : 2.494722866686061e-05\n",
      "Steps : 81500, \t Total Gen Loss : 3517.199951171875, \t Total Dis Loss : 1.227393022418255e-05\n",
      "Steps : 81600, \t Total Gen Loss : 3638.04052734375, \t Total Dis Loss : 1.57579797814833e-05\n",
      "Steps : 81700, \t Total Gen Loss : 3425.303466796875, \t Total Dis Loss : 7.24609344615601e-05\n",
      "Steps : 81800, \t Total Gen Loss : 3522.078125, \t Total Dis Loss : 2.4643095457577147e-05\n",
      "Steps : 81900, \t Total Gen Loss : 3440.81103515625, \t Total Dis Loss : 0.0007531155133619905\n",
      "Steps : 82000, \t Total Gen Loss : 3433.8544921875, \t Total Dis Loss : 0.00022620153322350234\n",
      "Steps : 82100, \t Total Gen Loss : 3497.670166015625, \t Total Dis Loss : 3.438587737036869e-05\n",
      "Steps : 82200, \t Total Gen Loss : 3521.25146484375, \t Total Dis Loss : 9.234167373506352e-05\n",
      "Steps : 82300, \t Total Gen Loss : 4051.21923828125, \t Total Dis Loss : 0.0017920060781762004\n",
      "Steps : 82400, \t Total Gen Loss : 3417.438232421875, \t Total Dis Loss : 0.00014026975259184837\n",
      "Steps : 82500, \t Total Gen Loss : 3906.569091796875, \t Total Dis Loss : 1.5431091014761478e-05\n",
      "Steps : 82600, \t Total Gen Loss : 3924.767333984375, \t Total Dis Loss : 7.610534521518275e-05\n",
      "Steps : 82700, \t Total Gen Loss : 3822.06396484375, \t Total Dis Loss : 3.341681804158725e-05\n",
      "Steps : 82800, \t Total Gen Loss : 4157.44970703125, \t Total Dis Loss : 1.5565636203973554e-05\n",
      "Steps : 82900, \t Total Gen Loss : 3478.281005859375, \t Total Dis Loss : 2.311657772224862e-05\n",
      "Steps : 83000, \t Total Gen Loss : 3462.9736328125, \t Total Dis Loss : 0.0012786182342097163\n",
      "Steps : 83100, \t Total Gen Loss : 3723.852294921875, \t Total Dis Loss : 0.0001585425197845325\n",
      "Steps : 83200, \t Total Gen Loss : 3835.32470703125, \t Total Dis Loss : 5.0261067372048274e-05\n",
      "Steps : 83300, \t Total Gen Loss : 3612.378173828125, \t Total Dis Loss : 0.0009257705532945693\n",
      "Steps : 83400, \t Total Gen Loss : 4034.727783203125, \t Total Dis Loss : 6.500197196146473e-05\n",
      "Steps : 83500, \t Total Gen Loss : 3713.858642578125, \t Total Dis Loss : 1.0748578461061697e-05\n",
      "Steps : 83600, \t Total Gen Loss : 3907.06396484375, \t Total Dis Loss : 1.5656030882382765e-05\n",
      "Steps : 83700, \t Total Gen Loss : 3868.71630859375, \t Total Dis Loss : 1.1222293323953636e-05\n",
      "Steps : 83800, \t Total Gen Loss : 3545.42236328125, \t Total Dis Loss : 1.8436936670696014e-06\n",
      "Steps : 83900, \t Total Gen Loss : 3774.02099609375, \t Total Dis Loss : 2.3385282474919222e-05\n",
      "Steps : 84000, \t Total Gen Loss : 4002.902099609375, \t Total Dis Loss : 7.564851694041863e-05\n",
      "Steps : 84100, \t Total Gen Loss : 3542.783447265625, \t Total Dis Loss : 0.00012982604675926268\n",
      "Steps : 84200, \t Total Gen Loss : 3584.166015625, \t Total Dis Loss : 0.00020663280156441033\n",
      "Steps : 84300, \t Total Gen Loss : 3908.87939453125, \t Total Dis Loss : 6.257908535189927e-05\n",
      "Time for epoch 15 is 75.4257926940918 sec\n",
      "Steps : 84400, \t Total Gen Loss : 3829.707275390625, \t Total Dis Loss : 0.00021001658751629293\n",
      "Steps : 84500, \t Total Gen Loss : 3428.872802734375, \t Total Dis Loss : 6.7419110564515e-05\n",
      "Steps : 84600, \t Total Gen Loss : 3897.219482421875, \t Total Dis Loss : 1.2176176824141294e-05\n",
      "Steps : 84700, \t Total Gen Loss : 3288.94921875, \t Total Dis Loss : 1.431444616173394e-05\n",
      "Steps : 84800, \t Total Gen Loss : 3342.33544921875, \t Total Dis Loss : 1.4703055057907477e-05\n",
      "Steps : 84900, \t Total Gen Loss : 3710.897705078125, \t Total Dis Loss : 0.00043032507528550923\n",
      "Steps : 85000, \t Total Gen Loss : 3759.716064453125, \t Total Dis Loss : 1.6634568964946084e-05\n",
      "Steps : 85100, \t Total Gen Loss : 4105.765625, \t Total Dis Loss : 9.38292796490714e-05\n",
      "Steps : 85200, \t Total Gen Loss : 2877.94921875, \t Total Dis Loss : 2.07866687560454e-05\n",
      "Steps : 85300, \t Total Gen Loss : 4012.17041015625, \t Total Dis Loss : 1.1706795703503303e-05\n",
      "Steps : 85400, \t Total Gen Loss : 3649.35791015625, \t Total Dis Loss : 3.189963899785653e-05\n",
      "Steps : 85500, \t Total Gen Loss : 3208.14794921875, \t Total Dis Loss : 7.645925506949425e-05\n",
      "Steps : 85600, \t Total Gen Loss : 3883.1884765625, \t Total Dis Loss : 0.00039391033351421356\n",
      "Steps : 85700, \t Total Gen Loss : 3453.43505859375, \t Total Dis Loss : 9.50643079704605e-05\n",
      "Steps : 85800, \t Total Gen Loss : 3239.638427734375, \t Total Dis Loss : 3.956429281970486e-05\n",
      "Steps : 85900, \t Total Gen Loss : 3810.962890625, \t Total Dis Loss : 0.0001950994337676093\n",
      "Steps : 86000, \t Total Gen Loss : 3434.91259765625, \t Total Dis Loss : 1.0008887329604477e-05\n",
      "Steps : 86100, \t Total Gen Loss : 3639.112060546875, \t Total Dis Loss : 0.010110633447766304\n",
      "Steps : 86200, \t Total Gen Loss : 3928.084228515625, \t Total Dis Loss : 5.649617378367111e-05\n",
      "Steps : 86300, \t Total Gen Loss : 4087.860107421875, \t Total Dis Loss : 9.835233868216164e-06\n",
      "Steps : 86400, \t Total Gen Loss : 3679.775146484375, \t Total Dis Loss : 1.0565699994913302e-05\n",
      "Steps : 86500, \t Total Gen Loss : 3373.0771484375, \t Total Dis Loss : 7.3125556809827685e-06\n",
      "Steps : 86600, \t Total Gen Loss : 3456.758544921875, \t Total Dis Loss : 1.5512019672314636e-05\n",
      "Steps : 86700, \t Total Gen Loss : 3406.43359375, \t Total Dis Loss : 5.560058070841478e-06\n",
      "Steps : 86800, \t Total Gen Loss : 3503.047119140625, \t Total Dis Loss : 5.14869207108859e-05\n",
      "Steps : 86900, \t Total Gen Loss : 3558.24365234375, \t Total Dis Loss : 2.1868443582206964e-05\n",
      "Steps : 87000, \t Total Gen Loss : 3469.857177734375, \t Total Dis Loss : 2.4999808374559507e-05\n",
      "Steps : 87100, \t Total Gen Loss : 3968.401611328125, \t Total Dis Loss : 5.665791832143441e-06\n",
      "Steps : 87200, \t Total Gen Loss : 3014.532470703125, \t Total Dis Loss : 5.3021863095636945e-06\n",
      "Steps : 87300, \t Total Gen Loss : 3961.526123046875, \t Total Dis Loss : 8.077957318164408e-06\n",
      "Steps : 87400, \t Total Gen Loss : 3872.00146484375, \t Total Dis Loss : 6.592136287508765e-06\n",
      "Steps : 87500, \t Total Gen Loss : 3861.214599609375, \t Total Dis Loss : 2.266125193273183e-05\n",
      "Steps : 87600, \t Total Gen Loss : 3706.159912109375, \t Total Dis Loss : 5.4982639994705096e-05\n",
      "Steps : 87700, \t Total Gen Loss : 3082.91748046875, \t Total Dis Loss : 4.009952681371942e-05\n",
      "Steps : 87800, \t Total Gen Loss : 3447.376708984375, \t Total Dis Loss : 0.0006507050711661577\n",
      "Steps : 87900, \t Total Gen Loss : 3070.976318359375, \t Total Dis Loss : 1.2032100130454637e-05\n",
      "Steps : 88000, \t Total Gen Loss : 3757.25146484375, \t Total Dis Loss : 7.226160232676193e-05\n",
      "Steps : 88100, \t Total Gen Loss : 3614.448974609375, \t Total Dis Loss : 0.0001191878181998618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 88200, \t Total Gen Loss : 3791.546630859375, \t Total Dis Loss : 6.636157922912389e-05\n",
      "Steps : 88300, \t Total Gen Loss : 3690.532470703125, \t Total Dis Loss : 7.026900129858404e-05\n",
      "Steps : 88400, \t Total Gen Loss : 3720.814208984375, \t Total Dis Loss : 1.9023180357180536e-05\n",
      "Steps : 88500, \t Total Gen Loss : 3425.86328125, \t Total Dis Loss : 1.3089436833979562e-05\n",
      "Steps : 88600, \t Total Gen Loss : 3251.442626953125, \t Total Dis Loss : 0.00013769113866146654\n",
      "Steps : 88700, \t Total Gen Loss : 3627.74267578125, \t Total Dis Loss : 0.00010310365905752406\n",
      "Steps : 88800, \t Total Gen Loss : 3784.18359375, \t Total Dis Loss : 2.2631891624769196e-05\n",
      "Steps : 88900, \t Total Gen Loss : 3702.937255859375, \t Total Dis Loss : 0.00015909253852441907\n",
      "Steps : 89000, \t Total Gen Loss : 3401.464111328125, \t Total Dis Loss : 0.0001948494027601555\n",
      "Steps : 89100, \t Total Gen Loss : 4087.46728515625, \t Total Dis Loss : 0.0001292196538997814\n",
      "Steps : 89200, \t Total Gen Loss : 3703.69140625, \t Total Dis Loss : 0.00011165304749738425\n",
      "Steps : 89300, \t Total Gen Loss : 3058.12744140625, \t Total Dis Loss : 0.00022921228082850575\n",
      "Steps : 89400, \t Total Gen Loss : 4034.354736328125, \t Total Dis Loss : 2.45480150624644e-05\n",
      "Steps : 89500, \t Total Gen Loss : 3619.00244140625, \t Total Dis Loss : 0.00018621016351971775\n",
      "Steps : 89600, \t Total Gen Loss : 3854.99072265625, \t Total Dis Loss : 3.336576628498733e-05\n",
      "Steps : 89700, \t Total Gen Loss : 3494.3515625, \t Total Dis Loss : 0.0001741510786814615\n",
      "Steps : 89800, \t Total Gen Loss : 4093.156982421875, \t Total Dis Loss : 0.0005944540025666356\n",
      "Steps : 89900, \t Total Gen Loss : 3586.948486328125, \t Total Dis Loss : 4.290209471946582e-05\n",
      "Steps : 90000, \t Total Gen Loss : 3441.529541015625, \t Total Dis Loss : 2.5155033654300496e-05\n",
      "Time for epoch 16 is 74.99596238136292 sec\n",
      "Steps : 90100, \t Total Gen Loss : 4476.14111328125, \t Total Dis Loss : 0.0008780006319284439\n",
      "Steps : 90200, \t Total Gen Loss : 2905.58740234375, \t Total Dis Loss : 9.86405648291111e-05\n",
      "Steps : 90300, \t Total Gen Loss : 3520.730224609375, \t Total Dis Loss : 5.300771226757206e-05\n",
      "Steps : 90400, \t Total Gen Loss : 3531.996337890625, \t Total Dis Loss : 0.03689775988459587\n",
      "Steps : 90500, \t Total Gen Loss : 3814.404541015625, \t Total Dis Loss : 6.384630069078412e-06\n",
      "Steps : 90600, \t Total Gen Loss : 4364.2177734375, \t Total Dis Loss : 4.4963777327211574e-05\n",
      "Steps : 90700, \t Total Gen Loss : 3224.42822265625, \t Total Dis Loss : 8.589488061261363e-06\n",
      "Steps : 90800, \t Total Gen Loss : 3513.343505859375, \t Total Dis Loss : 1.4390757314686198e-05\n",
      "Steps : 90900, \t Total Gen Loss : 3290.488525390625, \t Total Dis Loss : 3.443716559559107e-05\n",
      "Steps : 91000, \t Total Gen Loss : 3699.8134765625, \t Total Dis Loss : 2.012982076848857e-05\n",
      "Steps : 91100, \t Total Gen Loss : 3704.63330078125, \t Total Dis Loss : 8.193008397938684e-06\n",
      "Steps : 91200, \t Total Gen Loss : 3523.239990234375, \t Total Dis Loss : 0.000386404397431761\n",
      "Steps : 91300, \t Total Gen Loss : 3252.123291015625, \t Total Dis Loss : 0.002366066677495837\n",
      "Steps : 91400, \t Total Gen Loss : 3291.33349609375, \t Total Dis Loss : 0.0001336632703896612\n",
      "Steps : 91500, \t Total Gen Loss : 3738.40283203125, \t Total Dis Loss : 4.480432107811794e-05\n",
      "Steps : 91600, \t Total Gen Loss : 3811.564208984375, \t Total Dis Loss : 0.0014654884580522776\n",
      "Steps : 91700, \t Total Gen Loss : 4001.7666015625, \t Total Dis Loss : 0.0004573954502120614\n",
      "Steps : 91800, \t Total Gen Loss : 3530.29443359375, \t Total Dis Loss : 1.6154604963958263e-05\n",
      "Steps : 91900, \t Total Gen Loss : 3724.4296875, \t Total Dis Loss : 6.53851602692157e-05\n",
      "Steps : 92000, \t Total Gen Loss : 3902.455810546875, \t Total Dis Loss : 1.4531989108945709e-05\n",
      "Steps : 92100, \t Total Gen Loss : 3514.630859375, \t Total Dis Loss : 7.334802830882836e-06\n",
      "Steps : 92200, \t Total Gen Loss : 3627.378173828125, \t Total Dis Loss : 0.00013722262519877404\n",
      "Steps : 92300, \t Total Gen Loss : 3305.41064453125, \t Total Dis Loss : 3.7617082853103057e-05\n",
      "Steps : 92400, \t Total Gen Loss : 3530.488037109375, \t Total Dis Loss : 0.00012209327542223036\n",
      "Steps : 92500, \t Total Gen Loss : 3669.756103515625, \t Total Dis Loss : 5.797021003672853e-05\n",
      "Steps : 92600, \t Total Gen Loss : 3150.010986328125, \t Total Dis Loss : 0.0012189231347292662\n",
      "Steps : 92700, \t Total Gen Loss : 3639.0859375, \t Total Dis Loss : 2.348287853237707e-05\n",
      "Steps : 92800, \t Total Gen Loss : 3965.895751953125, \t Total Dis Loss : 1.9763392629101872e-05\n",
      "Steps : 92900, \t Total Gen Loss : 3267.18212890625, \t Total Dis Loss : 8.846080163493752e-05\n",
      "Steps : 93000, \t Total Gen Loss : 3413.531005859375, \t Total Dis Loss : 6.104605563450605e-05\n",
      "Steps : 93100, \t Total Gen Loss : 3583.372314453125, \t Total Dis Loss : 0.00022461864864453673\n",
      "Steps : 93200, \t Total Gen Loss : 3649.242919921875, \t Total Dis Loss : 4.264449671609327e-05\n",
      "Steps : 93300, \t Total Gen Loss : 3497.45166015625, \t Total Dis Loss : 4.909334165859036e-05\n",
      "Steps : 93400, \t Total Gen Loss : 4003.969482421875, \t Total Dis Loss : 0.00022232851188164204\n",
      "Steps : 93500, \t Total Gen Loss : 3943.788818359375, \t Total Dis Loss : 4.26652877649758e-05\n",
      "Steps : 93600, \t Total Gen Loss : 3703.742919921875, \t Total Dis Loss : 2.5006651412695646e-05\n",
      "Steps : 93700, \t Total Gen Loss : 3318.646484375, \t Total Dis Loss : 0.00022970243298914284\n",
      "Steps : 93800, \t Total Gen Loss : 3503.799072265625, \t Total Dis Loss : 2.914941887866007e-06\n",
      "Steps : 93900, \t Total Gen Loss : 3717.133056640625, \t Total Dis Loss : 8.99483075045282e-06\n",
      "Steps : 94000, \t Total Gen Loss : 3252.998291015625, \t Total Dis Loss : 6.092975127103273e-06\n",
      "Steps : 94100, \t Total Gen Loss : 3653.63037109375, \t Total Dis Loss : 5.250805406831205e-05\n",
      "Steps : 94200, \t Total Gen Loss : 3539.275634765625, \t Total Dis Loss : 1.536262288936996e-06\n",
      "Steps : 94300, \t Total Gen Loss : 3674.74853515625, \t Total Dis Loss : 0.0003275102353654802\n",
      "Steps : 94400, \t Total Gen Loss : 3430.99365234375, \t Total Dis Loss : 5.4078114771982655e-05\n",
      "Steps : 94500, \t Total Gen Loss : 3884.448974609375, \t Total Dis Loss : 1.8507247659727e-05\n",
      "Steps : 94600, \t Total Gen Loss : 4388.49462890625, \t Total Dis Loss : 8.370684554392938e-06\n",
      "Steps : 94700, \t Total Gen Loss : 3688.6669921875, \t Total Dis Loss : 0.0002392184396740049\n",
      "Steps : 94800, \t Total Gen Loss : 3598.676025390625, \t Total Dis Loss : 3.916491550626233e-05\n",
      "Steps : 94900, \t Total Gen Loss : 3508.85009765625, \t Total Dis Loss : 5.6302804296137765e-05\n",
      "Steps : 95000, \t Total Gen Loss : 3388.932861328125, \t Total Dis Loss : 4.2717456381069496e-05\n",
      "Steps : 95100, \t Total Gen Loss : 3328.279052734375, \t Total Dis Loss : 2.0729999960167333e-05\n",
      "Steps : 95200, \t Total Gen Loss : 3606.63818359375, \t Total Dis Loss : 5.9409067034721375e-06\n",
      "Steps : 95300, \t Total Gen Loss : 3103.789306640625, \t Total Dis Loss : 8.069005707511678e-05\n",
      "Steps : 95400, \t Total Gen Loss : 3650.556884765625, \t Total Dis Loss : 1.689720738795586e-05\n",
      "Steps : 95500, \t Total Gen Loss : 3243.96484375, \t Total Dis Loss : 1.714969039312564e-05\n",
      "Steps : 95600, \t Total Gen Loss : 3535.55126953125, \t Total Dis Loss : 7.122693205019459e-06\n",
      "Time for epoch 17 is 75.43343544006348 sec\n",
      "Steps : 95700, \t Total Gen Loss : 3628.371337890625, \t Total Dis Loss : 7.061209180392325e-05\n",
      "Steps : 95800, \t Total Gen Loss : 3662.466796875, \t Total Dis Loss : 1.0400769497209694e-05\n",
      "Steps : 95900, \t Total Gen Loss : 3771.71533203125, \t Total Dis Loss : 9.53317794483155e-06\n",
      "Steps : 96000, \t Total Gen Loss : 3948.8671875, \t Total Dis Loss : 0.00015753225306980312\n",
      "Steps : 96100, \t Total Gen Loss : 2805.2470703125, \t Total Dis Loss : 8.107893336273264e-06\n",
      "Steps : 96200, \t Total Gen Loss : 2988.794921875, \t Total Dis Loss : 2.2071679268265143e-05\n",
      "Steps : 96300, \t Total Gen Loss : 3702.155517578125, \t Total Dis Loss : 3.3921623980859295e-05\n",
      "Steps : 96400, \t Total Gen Loss : 3710.3759765625, \t Total Dis Loss : 4.707938205683604e-05\n",
      "Steps : 96500, \t Total Gen Loss : 3426.873291015625, \t Total Dis Loss : 4.012804856756702e-05\n",
      "Steps : 96600, \t Total Gen Loss : 3883.420166015625, \t Total Dis Loss : 8.058343155425973e-06\n",
      "Steps : 96700, \t Total Gen Loss : 3360.728271484375, \t Total Dis Loss : 0.0003284294216427952\n",
      "Steps : 96800, \t Total Gen Loss : 4424.876953125, \t Total Dis Loss : 2.204149859608151e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 96900, \t Total Gen Loss : 3501.046142578125, \t Total Dis Loss : 2.957986725959927e-05\n",
      "Steps : 97000, \t Total Gen Loss : 3972.5556640625, \t Total Dis Loss : 3.950143218389712e-05\n",
      "Steps : 97100, \t Total Gen Loss : 3321.7138671875, \t Total Dis Loss : 1.4755467418581247e-05\n",
      "Steps : 97200, \t Total Gen Loss : 3510.5029296875, \t Total Dis Loss : 8.433735638391227e-05\n",
      "Steps : 97300, \t Total Gen Loss : 3507.481201171875, \t Total Dis Loss : 2.40887857216876e-05\n",
      "Steps : 97400, \t Total Gen Loss : 3989.432861328125, \t Total Dis Loss : 8.986853208625689e-05\n",
      "Steps : 97500, \t Total Gen Loss : 3706.392578125, \t Total Dis Loss : 0.0002722361823543906\n",
      "Steps : 97600, \t Total Gen Loss : 3489.235595703125, \t Total Dis Loss : 9.110388782573864e-05\n",
      "Steps : 97700, \t Total Gen Loss : 4291.32177734375, \t Total Dis Loss : 7.736259431112558e-05\n",
      "Steps : 97800, \t Total Gen Loss : 3377.794677734375, \t Total Dis Loss : 7.636092050233856e-05\n",
      "Steps : 97900, \t Total Gen Loss : 3241.1396484375, \t Total Dis Loss : 2.938189027190674e-05\n",
      "Steps : 98000, \t Total Gen Loss : 4078.42724609375, \t Total Dis Loss : 5.176846752874553e-05\n",
      "Steps : 98100, \t Total Gen Loss : 3364.3759765625, \t Total Dis Loss : 2.1278252461343072e-05\n",
      "Steps : 98200, \t Total Gen Loss : 3613.390380859375, \t Total Dis Loss : 0.00011142914445372298\n",
      "Steps : 98300, \t Total Gen Loss : 3373.002197265625, \t Total Dis Loss : 6.108128218329512e-06\n",
      "Steps : 98400, \t Total Gen Loss : 3222.06982421875, \t Total Dis Loss : 2.947654684248846e-05\n",
      "Steps : 98500, \t Total Gen Loss : 3111.473876953125, \t Total Dis Loss : 5.0553124310681596e-05\n",
      "Steps : 98600, \t Total Gen Loss : 4061.778076171875, \t Total Dis Loss : 4.38302886323072e-05\n",
      "Steps : 98700, \t Total Gen Loss : 3612.0068359375, \t Total Dis Loss : 3.077047222177498e-05\n",
      "Steps : 98800, \t Total Gen Loss : 3840.68505859375, \t Total Dis Loss : 5.745581438532099e-05\n",
      "Steps : 98900, \t Total Gen Loss : 3660.92236328125, \t Total Dis Loss : 1.6335572581738234e-05\n",
      "Steps : 99000, \t Total Gen Loss : 3901.011962890625, \t Total Dis Loss : 2.8445409043342806e-05\n",
      "Steps : 99100, \t Total Gen Loss : 3726.909912109375, \t Total Dis Loss : 1.1757840184145607e-05\n",
      "Steps : 99200, \t Total Gen Loss : 3650.9404296875, \t Total Dis Loss : 7.482337878172984e-06\n",
      "Steps : 99300, \t Total Gen Loss : 3628.2490234375, \t Total Dis Loss : 7.553045179520268e-06\n",
      "Steps : 99400, \t Total Gen Loss : 3012.4384765625, \t Total Dis Loss : 0.0005355369648896158\n",
      "Steps : 99500, \t Total Gen Loss : 3827.023193359375, \t Total Dis Loss : 1.2698750651907176e-05\n",
      "Steps : 99600, \t Total Gen Loss : 3872.961181640625, \t Total Dis Loss : 8.8133652752731e-05\n",
      "Steps : 99700, \t Total Gen Loss : 4421.5283203125, \t Total Dis Loss : 0.0001449247938580811\n",
      "Steps : 99800, \t Total Gen Loss : 3440.05908203125, \t Total Dis Loss : 3.023494537046645e-05\n",
      "Steps : 99900, \t Total Gen Loss : 3316.173095703125, \t Total Dis Loss : 2.0544195649563335e-05\n",
      "Steps : 100000, \t Total Gen Loss : 3659.345947265625, \t Total Dis Loss : 2.8654298148467205e-05\n",
      "Steps : 100100, \t Total Gen Loss : 3397.63134765625, \t Total Dis Loss : 3.211038711015135e-05\n",
      "Steps : 100200, \t Total Gen Loss : 3953.174560546875, \t Total Dis Loss : 8.038234227569774e-06\n",
      "Steps : 100300, \t Total Gen Loss : 3510.8359375, \t Total Dis Loss : 0.0016637060325592756\n",
      "Steps : 100400, \t Total Gen Loss : 3432.1259765625, \t Total Dis Loss : 5.2075309213250875e-05\n",
      "Steps : 100500, \t Total Gen Loss : 3335.938232421875, \t Total Dis Loss : 7.161519897636026e-05\n",
      "Steps : 100600, \t Total Gen Loss : 3457.37353515625, \t Total Dis Loss : 4.490517676458694e-05\n",
      "Steps : 100700, \t Total Gen Loss : 3484.96435546875, \t Total Dis Loss : 3.4076871088473126e-05\n",
      "Steps : 100800, \t Total Gen Loss : 4293.2119140625, \t Total Dis Loss : 3.5269120417069644e-05\n",
      "Steps : 100900, \t Total Gen Loss : 3467.593994140625, \t Total Dis Loss : 9.052579844137654e-05\n",
      "Steps : 101000, \t Total Gen Loss : 3400.956787109375, \t Total Dis Loss : 2.2843365513836034e-05\n",
      "Steps : 101100, \t Total Gen Loss : 3367.399169921875, \t Total Dis Loss : 0.00012640526983886957\n",
      "Steps : 101200, \t Total Gen Loss : 3376.018310546875, \t Total Dis Loss : 9.600386874808464e-06\n",
      "Time for epoch 18 is 75.79365944862366 sec\n",
      "Steps : 101300, \t Total Gen Loss : 4055.223876953125, \t Total Dis Loss : 6.318826763163088e-06\n",
      "Steps : 101400, \t Total Gen Loss : 4240.83544921875, \t Total Dis Loss : 2.1905286757828435e-06\n",
      "Steps : 101500, \t Total Gen Loss : 3675.58349609375, \t Total Dis Loss : 1.7329128240817226e-05\n",
      "Steps : 101600, \t Total Gen Loss : 3595.89208984375, \t Total Dis Loss : 1.8513104805606417e-05\n",
      "Steps : 101700, \t Total Gen Loss : 3338.73681640625, \t Total Dis Loss : 0.00025520010967738926\n",
      "Steps : 101800, \t Total Gen Loss : 3706.55224609375, \t Total Dis Loss : 0.00012516886636149138\n",
      "Steps : 101900, \t Total Gen Loss : 3268.06591796875, \t Total Dis Loss : 0.00025198719231411815\n",
      "Steps : 102000, \t Total Gen Loss : 3427.6591796875, \t Total Dis Loss : 3.280842429376207e-05\n",
      "Steps : 102100, \t Total Gen Loss : 3812.845703125, \t Total Dis Loss : 1.628382051421795e-05\n",
      "Steps : 102200, \t Total Gen Loss : 3329.82861328125, \t Total Dis Loss : 0.00019104237435385585\n",
      "Steps : 102300, \t Total Gen Loss : 4055.48583984375, \t Total Dis Loss : 7.14129491825588e-05\n",
      "Steps : 102400, \t Total Gen Loss : 3428.53173828125, \t Total Dis Loss : 6.0887654399266466e-05\n",
      "Steps : 102500, \t Total Gen Loss : 3659.033935546875, \t Total Dis Loss : 6.892793317092583e-05\n",
      "Steps : 102600, \t Total Gen Loss : 3333.575927734375, \t Total Dis Loss : 0.00012933750986121595\n",
      "Steps : 102700, \t Total Gen Loss : 3535.25390625, \t Total Dis Loss : 0.00019320867431815714\n",
      "Steps : 102800, \t Total Gen Loss : 3960.64111328125, \t Total Dis Loss : 0.00012073893594788387\n",
      "Steps : 102900, \t Total Gen Loss : 3726.99853515625, \t Total Dis Loss : 1.6260075426544063e-05\n",
      "Steps : 103000, \t Total Gen Loss : 4321.087890625, \t Total Dis Loss : 4.513363819569349e-05\n",
      "Steps : 103100, \t Total Gen Loss : 4212.73388671875, \t Total Dis Loss : 1.4983423170633614e-05\n",
      "Steps : 103200, \t Total Gen Loss : 3460.237548828125, \t Total Dis Loss : 9.52356422203593e-05\n",
      "Steps : 103300, \t Total Gen Loss : 3730.21826171875, \t Total Dis Loss : 0.00011370065476512536\n",
      "Steps : 103400, \t Total Gen Loss : 3781.385009765625, \t Total Dis Loss : 2.2242675186134875e-05\n",
      "Steps : 103500, \t Total Gen Loss : 3878.742919921875, \t Total Dis Loss : 7.81704147811979e-05\n",
      "Steps : 103600, \t Total Gen Loss : 3820.7763671875, \t Total Dis Loss : 0.00014984553854446858\n",
      "Steps : 103700, \t Total Gen Loss : 3132.029296875, \t Total Dis Loss : 0.00047828140668570995\n",
      "Steps : 103800, \t Total Gen Loss : 2851.26025390625, \t Total Dis Loss : 8.650898962514475e-05\n",
      "Steps : 103900, \t Total Gen Loss : 3745.1591796875, \t Total Dis Loss : 9.089598461287096e-05\n",
      "Steps : 104000, \t Total Gen Loss : 4248.35693359375, \t Total Dis Loss : 1.5673947928007692e-05\n",
      "Steps : 104100, \t Total Gen Loss : 3652.883544921875, \t Total Dis Loss : 3.8744947232771665e-05\n",
      "Steps : 104200, \t Total Gen Loss : 3604.27001953125, \t Total Dis Loss : 3.3697811886668205e-05\n",
      "Steps : 104300, \t Total Gen Loss : 3646.197509765625, \t Total Dis Loss : 0.00010121011291630566\n",
      "Steps : 104400, \t Total Gen Loss : 3322.94482421875, \t Total Dis Loss : 1.9457327653071843e-05\n",
      "Steps : 104500, \t Total Gen Loss : 3599.591552734375, \t Total Dis Loss : 0.00071186717832461\n",
      "Steps : 104600, \t Total Gen Loss : 4263.0380859375, \t Total Dis Loss : 3.6639492464018986e-05\n",
      "Steps : 104700, \t Total Gen Loss : 3921.31787109375, \t Total Dis Loss : 1.410195545759052e-05\n",
      "Steps : 104800, \t Total Gen Loss : 4156.060546875, \t Total Dis Loss : 5.0721522711683065e-05\n",
      "Steps : 104900, \t Total Gen Loss : 3204.364990234375, \t Total Dis Loss : 1.3205924915382639e-05\n",
      "Steps : 105000, \t Total Gen Loss : 3188.4853515625, \t Total Dis Loss : 0.0002000797976506874\n",
      "Steps : 105100, \t Total Gen Loss : 3926.870361328125, \t Total Dis Loss : 8.31475990707986e-05\n",
      "Steps : 105200, \t Total Gen Loss : 3542.549560546875, \t Total Dis Loss : 4.0595903556095436e-05\n",
      "Steps : 105300, \t Total Gen Loss : 3469.57470703125, \t Total Dis Loss : 1.0440435289638117e-05\n",
      "Steps : 105400, \t Total Gen Loss : 3504.693359375, \t Total Dis Loss : 6.140113328001462e-06\n",
      "Steps : 105500, \t Total Gen Loss : 3588.545166015625, \t Total Dis Loss : 5.770661118731368e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 105600, \t Total Gen Loss : 3538.44775390625, \t Total Dis Loss : 3.078444569837302e-05\n",
      "Steps : 105700, \t Total Gen Loss : 3402.709228515625, \t Total Dis Loss : 0.00010488302359590307\n",
      "Steps : 105800, \t Total Gen Loss : 3251.04345703125, \t Total Dis Loss : 2.536719875934068e-05\n",
      "Steps : 105900, \t Total Gen Loss : 3417.39306640625, \t Total Dis Loss : 1.1802682820416521e-05\n",
      "Steps : 106000, \t Total Gen Loss : 3776.463623046875, \t Total Dis Loss : 3.662507515400648e-05\n",
      "Steps : 106100, \t Total Gen Loss : 3803.073974609375, \t Total Dis Loss : 1.9053153664572164e-05\n",
      "Steps : 106200, \t Total Gen Loss : 3762.5107421875, \t Total Dis Loss : 1.0991460840159561e-05\n",
      "Steps : 106300, \t Total Gen Loss : 3316.316162109375, \t Total Dis Loss : 7.618668314535171e-05\n",
      "Steps : 106400, \t Total Gen Loss : 3859.596923828125, \t Total Dis Loss : 7.785735760990065e-06\n",
      "Steps : 106500, \t Total Gen Loss : 3066.77294921875, \t Total Dis Loss : 9.997151209972799e-06\n",
      "Steps : 106600, \t Total Gen Loss : 3115.481201171875, \t Total Dis Loss : 2.3409877030644566e-05\n",
      "Steps : 106700, \t Total Gen Loss : 3206.306640625, \t Total Dis Loss : 0.00013788710930384696\n",
      "Steps : 106800, \t Total Gen Loss : 4250.92724609375, \t Total Dis Loss : 6.740849130437709e-06\n",
      "Time for epoch 19 is 76.77109789848328 sec\n",
      "Steps : 106900, \t Total Gen Loss : 3458.8583984375, \t Total Dis Loss : 0.0001877087343018502\n",
      "Steps : 107000, \t Total Gen Loss : 3632.533447265625, \t Total Dis Loss : 4.571949102682993e-05\n",
      "Steps : 107100, \t Total Gen Loss : 3309.72216796875, \t Total Dis Loss : 0.0001228522596647963\n",
      "Steps : 107200, \t Total Gen Loss : 3610.4921875, \t Total Dis Loss : 1.309095932811033e-05\n",
      "Steps : 107300, \t Total Gen Loss : 4030.222900390625, \t Total Dis Loss : 2.106315878336318e-05\n",
      "Steps : 107400, \t Total Gen Loss : 3583.4833984375, \t Total Dis Loss : 6.409415163943777e-06\n",
      "Steps : 107500, \t Total Gen Loss : 3857.34423828125, \t Total Dis Loss : 0.00026667426573112607\n",
      "Steps : 107600, \t Total Gen Loss : 4269.2734375, \t Total Dis Loss : 5.081642484583426e-06\n",
      "Steps : 107700, \t Total Gen Loss : 3316.741943359375, \t Total Dis Loss : 0.00019597775826696306\n",
      "Steps : 107800, \t Total Gen Loss : 2942.35009765625, \t Total Dis Loss : 3.632324660429731e-05\n",
      "Steps : 107900, \t Total Gen Loss : 3727.9677734375, \t Total Dis Loss : 2.203483018092811e-05\n",
      "Steps : 108000, \t Total Gen Loss : 3809.599365234375, \t Total Dis Loss : 3.0346189305419102e-05\n",
      "Steps : 108100, \t Total Gen Loss : 3973.888916015625, \t Total Dis Loss : 0.0004906191607005894\n",
      "Steps : 108200, \t Total Gen Loss : 4135.1083984375, \t Total Dis Loss : 6.213727465365082e-05\n",
      "Steps : 108300, \t Total Gen Loss : 3256.6201171875, \t Total Dis Loss : 8.706707376404665e-06\n",
      "Steps : 108400, \t Total Gen Loss : 3360.519287109375, \t Total Dis Loss : 0.00022433919366449118\n",
      "Steps : 108500, \t Total Gen Loss : 3487.72607421875, \t Total Dis Loss : 5.18024098710157e-05\n",
      "Steps : 108600, \t Total Gen Loss : 3671.46533203125, \t Total Dis Loss : 2.0450657757464796e-05\n",
      "Steps : 108700, \t Total Gen Loss : 3280.739013671875, \t Total Dis Loss : 9.259903890779242e-06\n",
      "Steps : 108800, \t Total Gen Loss : 3386.385498046875, \t Total Dis Loss : 7.04255435266532e-05\n",
      "Steps : 108900, \t Total Gen Loss : 3462.7392578125, \t Total Dis Loss : 0.0006777275702916086\n",
      "Steps : 109000, \t Total Gen Loss : 3820.5849609375, \t Total Dis Loss : 7.939848728710786e-05\n",
      "Steps : 109100, \t Total Gen Loss : 3308.93310546875, \t Total Dis Loss : 6.0615973779931664e-05\n",
      "Steps : 109200, \t Total Gen Loss : 3370.435546875, \t Total Dis Loss : 4.936262030241778e-06\n",
      "Steps : 109300, \t Total Gen Loss : 3214.010498046875, \t Total Dis Loss : 0.0003225628170184791\n",
      "Steps : 109400, \t Total Gen Loss : 3819.6533203125, \t Total Dis Loss : 1.3138504073140211e-05\n",
      "Steps : 109500, \t Total Gen Loss : 3909.195556640625, \t Total Dis Loss : 0.00014950863260310143\n",
      "Steps : 109600, \t Total Gen Loss : 4007.43603515625, \t Total Dis Loss : 2.621970634208992e-05\n",
      "Steps : 109700, \t Total Gen Loss : 3692.3935546875, \t Total Dis Loss : 6.435443356167525e-05\n",
      "Steps : 109800, \t Total Gen Loss : 3469.3193359375, \t Total Dis Loss : 6.626323738601059e-05\n",
      "Steps : 109900, \t Total Gen Loss : 3490.231689453125, \t Total Dis Loss : 2.2708089090883732e-05\n",
      "Steps : 110000, \t Total Gen Loss : 3882.65185546875, \t Total Dis Loss : 3.1116091122385114e-05\n",
      "Steps : 110100, \t Total Gen Loss : 3932.918701171875, \t Total Dis Loss : 1.3225980183051433e-05\n",
      "Steps : 110200, \t Total Gen Loss : 3530.8447265625, \t Total Dis Loss : 0.00921119935810566\n",
      "Steps : 110300, \t Total Gen Loss : 3826.90576171875, \t Total Dis Loss : 5.209035589359701e-05\n",
      "Steps : 110400, \t Total Gen Loss : 3582.071533203125, \t Total Dis Loss : 0.00033565054764039814\n",
      "Steps : 110500, \t Total Gen Loss : 3753.198486328125, \t Total Dis Loss : 3.528504021232948e-05\n",
      "Steps : 110600, \t Total Gen Loss : 3473.996337890625, \t Total Dis Loss : 4.889048796030693e-05\n",
      "Steps : 110700, \t Total Gen Loss : 3460.224609375, \t Total Dis Loss : 0.0006660089129582047\n",
      "Steps : 110800, \t Total Gen Loss : 3538.843017578125, \t Total Dis Loss : 0.0001424287329427898\n",
      "Steps : 110900, \t Total Gen Loss : 3551.578857421875, \t Total Dis Loss : 7.483681656594854e-06\n",
      "Steps : 111000, \t Total Gen Loss : 3556.904541015625, \t Total Dis Loss : 2.191682142438367e-05\n",
      "Steps : 111100, \t Total Gen Loss : 2938.127685546875, \t Total Dis Loss : 3.351241957716411e-06\n",
      "Steps : 111200, \t Total Gen Loss : 3566.154541015625, \t Total Dis Loss : 0.00020041718380525708\n",
      "Steps : 111300, \t Total Gen Loss : 3954.787841796875, \t Total Dis Loss : 0.0001267312909476459\n",
      "Steps : 111400, \t Total Gen Loss : 3348.3798828125, \t Total Dis Loss : 4.905759124085307e-05\n",
      "Steps : 111500, \t Total Gen Loss : 3173.248046875, \t Total Dis Loss : 0.006451329682022333\n",
      "Steps : 111600, \t Total Gen Loss : 3282.091552734375, \t Total Dis Loss : 0.0013254572404548526\n",
      "Steps : 111700, \t Total Gen Loss : 3450.138427734375, \t Total Dis Loss : 0.0002354695461690426\n",
      "Steps : 111800, \t Total Gen Loss : 3119.81689453125, \t Total Dis Loss : 2.2970003556110896e-05\n",
      "Steps : 111900, \t Total Gen Loss : 3214.035888671875, \t Total Dis Loss : 4.459288902580738e-05\n",
      "Steps : 112000, \t Total Gen Loss : 3435.043701171875, \t Total Dis Loss : 0.00789714977145195\n",
      "Steps : 112100, \t Total Gen Loss : 3134.223876953125, \t Total Dis Loss : 2.1541909518418834e-05\n",
      "Steps : 112200, \t Total Gen Loss : 4074.541015625, \t Total Dis Loss : 5.0504655519034714e-05\n",
      "Steps : 112300, \t Total Gen Loss : 4069.78173828125, \t Total Dis Loss : 1.0727408152888529e-05\n",
      "Steps : 112400, \t Total Gen Loss : 3496.57275390625, \t Total Dis Loss : 2.3630458599654958e-05\n",
      "Steps : 112500, \t Total Gen Loss : 3233.337158203125, \t Total Dis Loss : 1.3597103134088684e-05\n",
      "Time for epoch 20 is 75.45094537734985 sec\n",
      "Steps : 112600, \t Total Gen Loss : 3798.955078125, \t Total Dis Loss : 1.9672914277180098e-05\n",
      "Steps : 112700, \t Total Gen Loss : 3942.688720703125, \t Total Dis Loss : 5.652815889334306e-05\n",
      "Steps : 112800, \t Total Gen Loss : 3840.30859375, \t Total Dis Loss : 9.437688277103007e-05\n",
      "Steps : 112900, \t Total Gen Loss : 3388.5263671875, \t Total Dis Loss : 4.295973485568538e-05\n",
      "Steps : 113000, \t Total Gen Loss : 4142.779296875, \t Total Dis Loss : 2.3784647055435926e-05\n",
      "Steps : 113100, \t Total Gen Loss : 3724.307861328125, \t Total Dis Loss : 6.687940913252532e-05\n",
      "Steps : 113200, \t Total Gen Loss : 3249.30029296875, \t Total Dis Loss : 1.3072015462967101e-05\n",
      "Steps : 113300, \t Total Gen Loss : 3633.135498046875, \t Total Dis Loss : 0.00032357696909457445\n",
      "Steps : 113400, \t Total Gen Loss : 3460.15283203125, \t Total Dis Loss : 0.00025639793602749705\n",
      "Steps : 113500, \t Total Gen Loss : 3454.626220703125, \t Total Dis Loss : 0.00011966880992986262\n",
      "Steps : 113600, \t Total Gen Loss : 3486.994873046875, \t Total Dis Loss : 0.0004961234517395496\n",
      "Steps : 113700, \t Total Gen Loss : 3497.72607421875, \t Total Dis Loss : 0.0002756331523414701\n",
      "Steps : 113800, \t Total Gen Loss : 2794.57958984375, \t Total Dis Loss : 0.00014184060273692012\n",
      "Steps : 113900, \t Total Gen Loss : 3924.26806640625, \t Total Dis Loss : 0.0001115881823352538\n",
      "Steps : 114000, \t Total Gen Loss : 3853.468017578125, \t Total Dis Loss : 0.0005572381196543574\n",
      "Steps : 114100, \t Total Gen Loss : 3748.29150390625, \t Total Dis Loss : 1.320193950959947e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 114200, \t Total Gen Loss : 4316.810546875, \t Total Dis Loss : 2.761669566098135e-05\n",
      "Steps : 114300, \t Total Gen Loss : 3536.97900390625, \t Total Dis Loss : 2.8382355594658293e-05\n",
      "Steps : 114400, \t Total Gen Loss : 3567.2587890625, \t Total Dis Loss : 0.00030075214453972876\n",
      "Steps : 114500, \t Total Gen Loss : 3600.631591796875, \t Total Dis Loss : 6.341944754240103e-06\n",
      "Steps : 114600, \t Total Gen Loss : 3194.336181640625, \t Total Dis Loss : 2.714165202633012e-05\n",
      "Steps : 114700, \t Total Gen Loss : 3623.79541015625, \t Total Dis Loss : 8.535179404134396e-06\n",
      "Steps : 114800, \t Total Gen Loss : 3547.845703125, \t Total Dis Loss : 9.86515951808542e-05\n",
      "Steps : 114900, \t Total Gen Loss : 3689.582763671875, \t Total Dis Loss : 1.3416550245892722e-05\n",
      "Steps : 115000, \t Total Gen Loss : 3269.605712890625, \t Total Dis Loss : 3.787659079534933e-05\n",
      "Steps : 115100, \t Total Gen Loss : 3527.84521484375, \t Total Dis Loss : 0.0006357202655635774\n",
      "Steps : 115200, \t Total Gen Loss : 3113.74951171875, \t Total Dis Loss : 0.0004129309090785682\n",
      "Steps : 115300, \t Total Gen Loss : 3419.832763671875, \t Total Dis Loss : 2.593194949440658e-06\n",
      "Steps : 115400, \t Total Gen Loss : 3662.0654296875, \t Total Dis Loss : 8.542786963516846e-05\n",
      "Steps : 115500, \t Total Gen Loss : 3989.3798828125, \t Total Dis Loss : 1.192108538816683e-05\n",
      "Steps : 115600, \t Total Gen Loss : 4045.885498046875, \t Total Dis Loss : 0.0031287542078644037\n",
      "Steps : 115700, \t Total Gen Loss : 3307.0869140625, \t Total Dis Loss : 1.9459872419247404e-05\n",
      "Steps : 115800, \t Total Gen Loss : 4478.39013671875, \t Total Dis Loss : 4.448183062777389e-06\n",
      "Steps : 115900, \t Total Gen Loss : 3699.663818359375, \t Total Dis Loss : 1.0326566552976146e-05\n",
      "Steps : 116000, \t Total Gen Loss : 3717.888427734375, \t Total Dis Loss : 6.957928235351574e-06\n",
      "Steps : 116100, \t Total Gen Loss : 3078.753173828125, \t Total Dis Loss : 0.00017439799557905644\n",
      "Steps : 116200, \t Total Gen Loss : 3502.537353515625, \t Total Dis Loss : 2.1588150048046373e-05\n",
      "Steps : 116300, \t Total Gen Loss : 3739.154052734375, \t Total Dis Loss : 0.00012912241800222546\n",
      "Steps : 116400, \t Total Gen Loss : 3983.2314453125, \t Total Dis Loss : 1.257906751561677e-05\n",
      "Steps : 116500, \t Total Gen Loss : 3184.73681640625, \t Total Dis Loss : 5.155547114554793e-06\n",
      "Steps : 116600, \t Total Gen Loss : 3895.826416015625, \t Total Dis Loss : 6.219811893970473e-06\n",
      "Steps : 116700, \t Total Gen Loss : 3771.676025390625, \t Total Dis Loss : 3.901470699929632e-05\n",
      "Steps : 116800, \t Total Gen Loss : 3804.2626953125, \t Total Dis Loss : 3.904326513293199e-05\n",
      "Steps : 116900, \t Total Gen Loss : 3502.94140625, \t Total Dis Loss : 1.0503175644771545e-06\n",
      "Steps : 117000, \t Total Gen Loss : 4050.85693359375, \t Total Dis Loss : 1.07527348518488e-05\n",
      "Steps : 117100, \t Total Gen Loss : 3947.311767578125, \t Total Dis Loss : 1.4412734344659839e-05\n",
      "Steps : 117200, \t Total Gen Loss : 3469.968505859375, \t Total Dis Loss : 3.7980057641107123e-06\n",
      "Steps : 117300, \t Total Gen Loss : 3298.930908203125, \t Total Dis Loss : 5.180517291591968e-06\n",
      "Steps : 117400, \t Total Gen Loss : 3376.897705078125, \t Total Dis Loss : 9.859694728220347e-06\n",
      "Steps : 117500, \t Total Gen Loss : 3813.055419921875, \t Total Dis Loss : 2.1246243704808876e-05\n",
      "Steps : 117600, \t Total Gen Loss : 3650.490966796875, \t Total Dis Loss : 6.830848633399e-06\n",
      "Steps : 117700, \t Total Gen Loss : 3261.333984375, \t Total Dis Loss : 4.678126060753129e-06\n",
      "Steps : 117800, \t Total Gen Loss : 3424.21337890625, \t Total Dis Loss : 5.31747127752169e-06\n",
      "Steps : 117900, \t Total Gen Loss : 3797.604248046875, \t Total Dis Loss : 2.192953797930386e-05\n",
      "Steps : 118000, \t Total Gen Loss : 4155.388671875, \t Total Dis Loss : 1.7705528080114163e-05\n",
      "Steps : 118100, \t Total Gen Loss : 3688.322998046875, \t Total Dis Loss : 0.00017508305609226227\n",
      "Time for epoch 21 is 76.46475434303284 sec\n",
      "Steps : 118200, \t Total Gen Loss : 3428.045654296875, \t Total Dis Loss : 0.0003276952775195241\n",
      "Steps : 118300, \t Total Gen Loss : 3866.81640625, \t Total Dis Loss : 3.3696269383654e-05\n",
      "Steps : 118400, \t Total Gen Loss : 3600.19091796875, \t Total Dis Loss : 1.4267976439441554e-05\n",
      "Steps : 118500, \t Total Gen Loss : 3597.51171875, \t Total Dis Loss : 2.3765587684465572e-05\n",
      "Steps : 118600, \t Total Gen Loss : 3357.47314453125, \t Total Dis Loss : 3.297420698800124e-05\n",
      "Steps : 118700, \t Total Gen Loss : 3545.981689453125, \t Total Dis Loss : 2.8378781280480325e-05\n",
      "Steps : 118800, \t Total Gen Loss : 3035.54541015625, \t Total Dis Loss : 9.05975502973888e-06\n",
      "Steps : 118900, \t Total Gen Loss : 3033.052978515625, \t Total Dis Loss : 0.0005042192642576993\n",
      "Steps : 119000, \t Total Gen Loss : 3749.815673828125, \t Total Dis Loss : 4.151528628426604e-06\n",
      "Steps : 119100, \t Total Gen Loss : 3976.980712890625, \t Total Dis Loss : 6.920215673744678e-05\n",
      "Steps : 119200, \t Total Gen Loss : 3294.847900390625, \t Total Dis Loss : 5.696162406820804e-05\n",
      "Steps : 119300, \t Total Gen Loss : 3378.255126953125, \t Total Dis Loss : 0.00017959052638616413\n",
      "Steps : 119400, \t Total Gen Loss : 3996.10400390625, \t Total Dis Loss : 0.00013731214858125895\n",
      "Steps : 119500, \t Total Gen Loss : 3788.5556640625, \t Total Dis Loss : 2.9433576855808496e-05\n",
      "Steps : 119600, \t Total Gen Loss : 3177.9541015625, \t Total Dis Loss : 0.00021822941198479384\n",
      "Steps : 119700, \t Total Gen Loss : 3282.535400390625, \t Total Dis Loss : 8.326846909767482e-06\n",
      "Steps : 119800, \t Total Gen Loss : 2984.61669921875, \t Total Dis Loss : 2.0709998352685943e-05\n",
      "Steps : 119900, \t Total Gen Loss : 3825.740966796875, \t Total Dis Loss : 7.551986072940053e-06\n",
      "Steps : 120000, \t Total Gen Loss : 3159.209716796875, \t Total Dis Loss : 4.803563570021652e-05\n",
      "Steps : 120100, \t Total Gen Loss : 3122.88427734375, \t Total Dis Loss : 0.00014741261838935316\n",
      "Steps : 120200, \t Total Gen Loss : 3262.3984375, \t Total Dis Loss : 1.2275259905436542e-05\n",
      "Steps : 120300, \t Total Gen Loss : 3749.93701171875, \t Total Dis Loss : 1.5029746464279015e-05\n",
      "Steps : 120400, \t Total Gen Loss : 4154.93603515625, \t Total Dis Loss : 2.3657075871597044e-05\n",
      "Steps : 120500, \t Total Gen Loss : 3220.265380859375, \t Total Dis Loss : 1.054268068401143e-05\n",
      "Steps : 120600, \t Total Gen Loss : 4090.923828125, \t Total Dis Loss : 1.0601483154459856e-05\n",
      "Steps : 120700, \t Total Gen Loss : 3220.61669921875, \t Total Dis Loss : 0.0005389493308030069\n",
      "Steps : 120800, \t Total Gen Loss : 4124.0458984375, \t Total Dis Loss : 9.993322237278335e-06\n",
      "Steps : 120900, \t Total Gen Loss : 3928.128173828125, \t Total Dis Loss : 6.324250534817111e-06\n",
      "Steps : 121000, \t Total Gen Loss : 4337.4775390625, \t Total Dis Loss : 1.0804047633428127e-05\n",
      "Steps : 121100, \t Total Gen Loss : 3238.343017578125, \t Total Dis Loss : 9.831480383581948e-06\n",
      "Steps : 121200, \t Total Gen Loss : 3949.462158203125, \t Total Dis Loss : 2.8313565053394996e-05\n",
      "Steps : 121300, \t Total Gen Loss : 3444.6728515625, \t Total Dis Loss : 2.5699464458739385e-05\n",
      "Steps : 121400, \t Total Gen Loss : 4269.07373046875, \t Total Dis Loss : 0.0002033240016316995\n",
      "Steps : 121500, \t Total Gen Loss : 3598.8076171875, \t Total Dis Loss : 6.793050488340668e-06\n",
      "Steps : 121600, \t Total Gen Loss : 3520.4013671875, \t Total Dis Loss : 0.0035430348943918943\n",
      "Steps : 121700, \t Total Gen Loss : 3581.5400390625, \t Total Dis Loss : 3.272125468356535e-05\n",
      "Steps : 121800, \t Total Gen Loss : 3166.099609375, \t Total Dis Loss : 0.00027399210375733674\n",
      "Steps : 121900, \t Total Gen Loss : 4292.7490234375, \t Total Dis Loss : 2.06509303097846e-05\n",
      "Steps : 122000, \t Total Gen Loss : 3869.575439453125, \t Total Dis Loss : 1.7053620467777364e-05\n",
      "Steps : 122100, \t Total Gen Loss : 3520.521240234375, \t Total Dis Loss : 3.274876144132577e-05\n",
      "Steps : 122200, \t Total Gen Loss : 3948.562255859375, \t Total Dis Loss : 4.601652472047135e-05\n",
      "Steps : 122300, \t Total Gen Loss : 3825.100830078125, \t Total Dis Loss : 3.2664731406839564e-05\n",
      "Steps : 122400, \t Total Gen Loss : 3693.037841796875, \t Total Dis Loss : 0.00016546140250284225\n",
      "Steps : 122500, \t Total Gen Loss : 3334.09521484375, \t Total Dis Loss : 3.643701711553149e-05\n",
      "Steps : 122600, \t Total Gen Loss : 3644.446533203125, \t Total Dis Loss : 1.0836980436579324e-05\n",
      "Steps : 122700, \t Total Gen Loss : 3439.197265625, \t Total Dis Loss : 0.00018974876729771495\n",
      "Steps : 122800, \t Total Gen Loss : 3265.812744140625, \t Total Dis Loss : 4.8813650209922343e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 122900, \t Total Gen Loss : 3197.035888671875, \t Total Dis Loss : 7.968566205818206e-05\n",
      "Steps : 123000, \t Total Gen Loss : 3178.363037109375, \t Total Dis Loss : 6.806211604271084e-05\n",
      "Steps : 123100, \t Total Gen Loss : 3903.62548828125, \t Total Dis Loss : 0.0002562308800406754\n",
      "Steps : 123200, \t Total Gen Loss : 3643.682373046875, \t Total Dis Loss : 1.9726427126443014e-05\n",
      "Steps : 123300, \t Total Gen Loss : 3412.441650390625, \t Total Dis Loss : 0.00016688744653947651\n",
      "Steps : 123400, \t Total Gen Loss : 3732.26025390625, \t Total Dis Loss : 3.428301715757698e-05\n",
      "Steps : 123500, \t Total Gen Loss : 3632.4677734375, \t Total Dis Loss : 8.674747368786484e-05\n",
      "Steps : 123600, \t Total Gen Loss : 3470.0, \t Total Dis Loss : 0.00028814608231186867\n",
      "Steps : 123700, \t Total Gen Loss : 3387.6455078125, \t Total Dis Loss : 5.904304271098226e-05\n",
      "Time for epoch 22 is 76.54594349861145 sec\n",
      "Steps : 123800, \t Total Gen Loss : 4287.658203125, \t Total Dis Loss : 5.817291093990207e-05\n",
      "Steps : 123900, \t Total Gen Loss : 3744.3515625, \t Total Dis Loss : 3.0078374038566835e-05\n",
      "Steps : 124000, \t Total Gen Loss : 3047.86962890625, \t Total Dis Loss : 1.852119021350518e-05\n",
      "Steps : 124100, \t Total Gen Loss : 3576.2197265625, \t Total Dis Loss : 6.124406354501843e-05\n",
      "Steps : 124200, \t Total Gen Loss : 3197.34521484375, \t Total Dis Loss : 7.265801832545549e-05\n",
      "Steps : 124300, \t Total Gen Loss : 3511.7119140625, \t Total Dis Loss : 2.7402207706472836e-05\n",
      "Steps : 124400, \t Total Gen Loss : 4062.4765625, \t Total Dis Loss : 0.0007016272866167128\n",
      "Steps : 124500, \t Total Gen Loss : 4288.3671875, \t Total Dis Loss : 0.0003335883666295558\n",
      "Steps : 124600, \t Total Gen Loss : 3906.10595703125, \t Total Dis Loss : 3.5665540053742006e-05\n",
      "Steps : 124700, \t Total Gen Loss : 3249.6484375, \t Total Dis Loss : 1.4143994121695869e-05\n",
      "Steps : 124800, \t Total Gen Loss : 3718.45166015625, \t Total Dis Loss : 5.8002089645015076e-05\n",
      "Steps : 124900, \t Total Gen Loss : 3344.164306640625, \t Total Dis Loss : 5.531343413167633e-05\n",
      "Steps : 125000, \t Total Gen Loss : 3314.222900390625, \t Total Dis Loss : 4.10820975957904e-05\n",
      "Steps : 125100, \t Total Gen Loss : 3397.417236328125, \t Total Dis Loss : 0.00010504593228688464\n",
      "Steps : 125200, \t Total Gen Loss : 3653.9296875, \t Total Dis Loss : 6.129005214461358e-06\n",
      "Steps : 125300, \t Total Gen Loss : 4173.0556640625, \t Total Dis Loss : 3.212507726857439e-05\n",
      "Steps : 125400, \t Total Gen Loss : 4040.55029296875, \t Total Dis Loss : 6.314490019576624e-05\n",
      "Steps : 125500, \t Total Gen Loss : 3863.1884765625, \t Total Dis Loss : 4.743368845083751e-05\n",
      "Steps : 125600, \t Total Gen Loss : 4042.52392578125, \t Total Dis Loss : 0.00023131044872570783\n",
      "Steps : 125700, \t Total Gen Loss : 3367.393310546875, \t Total Dis Loss : 9.793385106604546e-05\n",
      "Steps : 125800, \t Total Gen Loss : 3442.172119140625, \t Total Dis Loss : 0.00010542356903897598\n",
      "Steps : 125900, \t Total Gen Loss : 3717.8291015625, \t Total Dis Loss : 6.097412551753223e-05\n",
      "Steps : 126000, \t Total Gen Loss : 3463.39453125, \t Total Dis Loss : 2.6470572265679948e-05\n",
      "Steps : 126100, \t Total Gen Loss : 3819.065673828125, \t Total Dis Loss : 9.432638762518764e-05\n",
      "Steps : 126200, \t Total Gen Loss : 3773.717041015625, \t Total Dis Loss : 4.933732998324558e-05\n",
      "Steps : 126300, \t Total Gen Loss : 3861.997314453125, \t Total Dis Loss : 6.278946966631338e-05\n",
      "Steps : 126400, \t Total Gen Loss : 3292.063232421875, \t Total Dis Loss : 9.072521788766608e-05\n",
      "Steps : 126500, \t Total Gen Loss : 3845.395263671875, \t Total Dis Loss : 6.51314330752939e-05\n",
      "Steps : 126600, \t Total Gen Loss : 3440.3955078125, \t Total Dis Loss : 6.856109393993393e-05\n",
      "Steps : 126700, \t Total Gen Loss : 3632.602783203125, \t Total Dis Loss : 7.407975499518216e-05\n",
      "Steps : 126800, \t Total Gen Loss : 3451.511962890625, \t Total Dis Loss : 0.002536005573347211\n",
      "Steps : 126900, \t Total Gen Loss : 3973.4921875, \t Total Dis Loss : 7.318485586438328e-05\n",
      "Steps : 127000, \t Total Gen Loss : 3981.935546875, \t Total Dis Loss : 3.077449218835682e-05\n",
      "Steps : 127100, \t Total Gen Loss : 3594.457763671875, \t Total Dis Loss : 2.0785173546755686e-05\n",
      "Steps : 127200, \t Total Gen Loss : 3548.19140625, \t Total Dis Loss : 0.0001529297005617991\n",
      "Steps : 127300, \t Total Gen Loss : 3706.06494140625, \t Total Dis Loss : 0.0001442725770175457\n",
      "Steps : 127400, \t Total Gen Loss : 3531.771728515625, \t Total Dis Loss : 1.244947543455055e-05\n",
      "Steps : 127500, \t Total Gen Loss : 3938.5556640625, \t Total Dis Loss : 8.955474186223e-05\n",
      "Steps : 127600, \t Total Gen Loss : 3671.0546875, \t Total Dis Loss : 4.417742457007989e-05\n",
      "Steps : 127700, \t Total Gen Loss : 3436.789306640625, \t Total Dis Loss : 0.0001011823769658804\n",
      "Steps : 127800, \t Total Gen Loss : 3909.518798828125, \t Total Dis Loss : 0.0016514611197635531\n",
      "Steps : 127900, \t Total Gen Loss : 3004.028564453125, \t Total Dis Loss : 1.783119296305813e-05\n",
      "Steps : 128000, \t Total Gen Loss : 4063.9169921875, \t Total Dis Loss : 9.011885595100466e-06\n",
      "Steps : 128100, \t Total Gen Loss : 3235.961669921875, \t Total Dis Loss : 1.705769682303071e-05\n",
      "Steps : 128200, \t Total Gen Loss : 3780.974365234375, \t Total Dis Loss : 7.178221858339384e-05\n",
      "Steps : 128300, \t Total Gen Loss : 3435.967041015625, \t Total Dis Loss : 1.3251737982500345e-05\n",
      "Steps : 128400, \t Total Gen Loss : 3328.06884765625, \t Total Dis Loss : 9.975517968996428e-06\n",
      "Steps : 128500, \t Total Gen Loss : 3272.614501953125, \t Total Dis Loss : 3.091269900323823e-05\n",
      "Steps : 128600, \t Total Gen Loss : 4023.343017578125, \t Total Dis Loss : 7.853570423321798e-05\n",
      "Steps : 128700, \t Total Gen Loss : 3764.376708984375, \t Total Dis Loss : 3.046243728022091e-05\n",
      "Steps : 128800, \t Total Gen Loss : 4112.13330078125, \t Total Dis Loss : 4.005986556876451e-05\n",
      "Steps : 128900, \t Total Gen Loss : 4023.7822265625, \t Total Dis Loss : 7.620314136147499e-05\n",
      "Steps : 129000, \t Total Gen Loss : 3188.4736328125, \t Total Dis Loss : 0.00013946056424174458\n",
      "Steps : 129100, \t Total Gen Loss : 3163.149658203125, \t Total Dis Loss : 0.00022712472127750516\n",
      "Steps : 129200, \t Total Gen Loss : 4118.52099609375, \t Total Dis Loss : 0.00014875219494570047\n",
      "Steps : 129300, \t Total Gen Loss : 3637.41015625, \t Total Dis Loss : 3.924943302990869e-05\n",
      "Time for epoch 23 is 76.92881035804749 sec\n",
      "Steps : 129400, \t Total Gen Loss : 3468.115234375, \t Total Dis Loss : 8.361891377717257e-05\n",
      "Steps : 129500, \t Total Gen Loss : 4197.109375, \t Total Dis Loss : 7.006387022556737e-05\n",
      "Steps : 129600, \t Total Gen Loss : 3961.65234375, \t Total Dis Loss : 6.079992090235464e-06\n",
      "Steps : 129700, \t Total Gen Loss : 3627.280029296875, \t Total Dis Loss : 0.0007374627166427672\n",
      "Steps : 129800, \t Total Gen Loss : 3142.417724609375, \t Total Dis Loss : 0.0002594629186205566\n",
      "Steps : 129900, \t Total Gen Loss : 3606.0947265625, \t Total Dis Loss : 0.00043072813423350453\n",
      "Steps : 130000, \t Total Gen Loss : 3492.74951171875, \t Total Dis Loss : 0.0009602857171557844\n",
      "Steps : 130100, \t Total Gen Loss : 3245.632568359375, \t Total Dis Loss : 0.0003664031683001667\n",
      "Steps : 130200, \t Total Gen Loss : 3985.634521484375, \t Total Dis Loss : 0.00014179199934005737\n",
      "Steps : 130300, \t Total Gen Loss : 3774.679443359375, \t Total Dis Loss : 7.122773240553215e-05\n",
      "Steps : 130400, \t Total Gen Loss : 3019.91259765625, \t Total Dis Loss : 0.0001854796428233385\n",
      "Steps : 130500, \t Total Gen Loss : 3476.58740234375, \t Total Dis Loss : 3.786231900448911e-05\n",
      "Steps : 130600, \t Total Gen Loss : 3707.348388671875, \t Total Dis Loss : 0.00014277607260737568\n",
      "Steps : 130700, \t Total Gen Loss : 3542.68115234375, \t Total Dis Loss : 0.000111882844066713\n",
      "Steps : 130800, \t Total Gen Loss : 3206.320068359375, \t Total Dis Loss : 4.1279672586824745e-05\n",
      "Steps : 130900, \t Total Gen Loss : 3709.009765625, \t Total Dis Loss : 3.619442577473819e-05\n",
      "Steps : 131000, \t Total Gen Loss : 2927.96630859375, \t Total Dis Loss : 5.344236342352815e-06\n",
      "Steps : 131100, \t Total Gen Loss : 3590.9736328125, \t Total Dis Loss : 1.854016045399476e-05\n",
      "Steps : 131200, \t Total Gen Loss : 3568.648193359375, \t Total Dis Loss : 0.00031032037804834545\n",
      "Steps : 131300, \t Total Gen Loss : 3484.166015625, \t Total Dis Loss : 3.125206058030017e-05\n",
      "Steps : 131400, \t Total Gen Loss : 3921.88525390625, \t Total Dis Loss : 2.5670146897027735e-06\n",
      "Steps : 131500, \t Total Gen Loss : 3557.689453125, \t Total Dis Loss : 3.821297468675766e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 131600, \t Total Gen Loss : 3608.662109375, \t Total Dis Loss : 3.663147799670696e-05\n",
      "Steps : 131700, \t Total Gen Loss : 3466.40869140625, \t Total Dis Loss : 4.465332312975079e-05\n",
      "Steps : 131800, \t Total Gen Loss : 3699.938720703125, \t Total Dis Loss : 2.7505226171342656e-05\n",
      "Steps : 131900, \t Total Gen Loss : 3445.52734375, \t Total Dis Loss : 0.0002451609761919826\n",
      "Steps : 132000, \t Total Gen Loss : 4234.609375, \t Total Dis Loss : 6.176645547384396e-05\n",
      "Steps : 132100, \t Total Gen Loss : 3958.85546875, \t Total Dis Loss : 1.7100795957958326e-05\n",
      "Steps : 132200, \t Total Gen Loss : 3496.72314453125, \t Total Dis Loss : 8.695501492184121e-06\n",
      "Steps : 132300, \t Total Gen Loss : 3842.20458984375, \t Total Dis Loss : 0.00021131413814146072\n",
      "Steps : 132400, \t Total Gen Loss : 3573.073486328125, \t Total Dis Loss : 4.946824265061878e-05\n",
      "Steps : 132500, \t Total Gen Loss : 3643.41455078125, \t Total Dis Loss : 0.00016664236318320036\n",
      "Steps : 132600, \t Total Gen Loss : 3415.244873046875, \t Total Dis Loss : 5.784698714705883e-06\n",
      "Steps : 132700, \t Total Gen Loss : 3720.05419921875, \t Total Dis Loss : 3.430575816310011e-05\n",
      "Steps : 132800, \t Total Gen Loss : 3502.568359375, \t Total Dis Loss : 8.734193397685885e-05\n",
      "Steps : 132900, \t Total Gen Loss : 3509.96337890625, \t Total Dis Loss : 0.0012928593205288053\n",
      "Steps : 133000, \t Total Gen Loss : 4058.118896484375, \t Total Dis Loss : 2.5226134312106296e-05\n",
      "Steps : 133100, \t Total Gen Loss : 4010.886962890625, \t Total Dis Loss : 9.986621989810374e-06\n",
      "Steps : 133200, \t Total Gen Loss : 3505.0390625, \t Total Dis Loss : 3.450621443334967e-05\n",
      "Steps : 133300, \t Total Gen Loss : 3402.8896484375, \t Total Dis Loss : 4.186223668511957e-05\n",
      "Steps : 133400, \t Total Gen Loss : 3285.708740234375, \t Total Dis Loss : 1.742236054269597e-05\n",
      "Steps : 133500, \t Total Gen Loss : 3056.957275390625, \t Total Dis Loss : 0.006837949156761169\n",
      "Steps : 133600, \t Total Gen Loss : 3468.60400390625, \t Total Dis Loss : 0.0001943594397744164\n",
      "Steps : 133700, \t Total Gen Loss : 3537.774658203125, \t Total Dis Loss : 4.8765788960736245e-05\n",
      "Steps : 133800, \t Total Gen Loss : 3602.489013671875, \t Total Dis Loss : 7.327163621084765e-05\n",
      "Steps : 133900, \t Total Gen Loss : 3544.664306640625, \t Total Dis Loss : 1.1973991604463663e-05\n",
      "Steps : 134000, \t Total Gen Loss : 3451.4169921875, \t Total Dis Loss : 9.014723764266819e-05\n",
      "Steps : 134100, \t Total Gen Loss : 3215.2861328125, \t Total Dis Loss : 4.0768230974208564e-05\n",
      "Steps : 134200, \t Total Gen Loss : 3841.650634765625, \t Total Dis Loss : 5.338728897186229e-06\n",
      "Steps : 134300, \t Total Gen Loss : 4015.5625, \t Total Dis Loss : 5.364499429560965e-06\n",
      "Steps : 134400, \t Total Gen Loss : 3725.283447265625, \t Total Dis Loss : 4.0817085391609e-05\n",
      "Steps : 134500, \t Total Gen Loss : 3377.245849609375, \t Total Dis Loss : 1.5310244634747505e-05\n",
      "Steps : 134600, \t Total Gen Loss : 3197.30126953125, \t Total Dis Loss : 1.4874307453283109e-05\n",
      "Steps : 134700, \t Total Gen Loss : 3216.913330078125, \t Total Dis Loss : 4.3342948629288e-05\n",
      "Steps : 134800, \t Total Gen Loss : 3522.953125, \t Total Dis Loss : 4.271774741937406e-06\n",
      "Steps : 134900, \t Total Gen Loss : 3379.218994140625, \t Total Dis Loss : 1.4594412277801894e-05\n",
      "Steps : 135000, \t Total Gen Loss : 3466.186767578125, \t Total Dis Loss : 2.7520847652340308e-05\n",
      "Time for epoch 24 is 76.56688833236694 sec\n",
      "Steps : 135100, \t Total Gen Loss : 3689.549072265625, \t Total Dis Loss : 2.5623079636716284e-05\n",
      "Steps : 135200, \t Total Gen Loss : 4024.421875, \t Total Dis Loss : 7.501756499550538e-06\n",
      "Steps : 135300, \t Total Gen Loss : 3285.947509765625, \t Total Dis Loss : 2.6811892894329503e-05\n",
      "Steps : 135400, \t Total Gen Loss : 3997.758544921875, \t Total Dis Loss : 2.7351354219717905e-05\n",
      "Steps : 135500, \t Total Gen Loss : 3570.49560546875, \t Total Dis Loss : 1.9554154278011993e-05\n",
      "Steps : 135600, \t Total Gen Loss : 3672.891845703125, \t Total Dis Loss : 2.4990789825096726e-05\n",
      "Steps : 135700, \t Total Gen Loss : 3320.34716796875, \t Total Dis Loss : 1.3649334505316801e-05\n",
      "Steps : 135800, \t Total Gen Loss : 3961.052978515625, \t Total Dis Loss : 1.8568287487141788e-05\n",
      "Steps : 135900, \t Total Gen Loss : 3572.8759765625, \t Total Dis Loss : 2.0958159439032897e-05\n",
      "Steps : 136000, \t Total Gen Loss : 3522.036376953125, \t Total Dis Loss : 7.096218723745551e-06\n",
      "Steps : 136100, \t Total Gen Loss : 3162.514404296875, \t Total Dis Loss : 1.301854535995517e-05\n",
      "Steps : 136200, \t Total Gen Loss : 3381.1904296875, \t Total Dis Loss : 3.245844709454104e-05\n",
      "Steps : 136300, \t Total Gen Loss : 3314.0302734375, \t Total Dis Loss : 1.2217677067383192e-05\n",
      "Steps : 136400, \t Total Gen Loss : 3740.49853515625, \t Total Dis Loss : 6.015784492774401e-06\n",
      "Steps : 136500, \t Total Gen Loss : 3074.08056640625, \t Total Dis Loss : 2.3468066501663998e-05\n",
      "Steps : 136600, \t Total Gen Loss : 3394.78759765625, \t Total Dis Loss : 0.0023006366100162268\n",
      "Steps : 136700, \t Total Gen Loss : 4116.36572265625, \t Total Dis Loss : 3.4963322832481936e-05\n",
      "Steps : 136800, \t Total Gen Loss : 3835.1396484375, \t Total Dis Loss : 3.991016637883149e-05\n",
      "Steps : 136900, \t Total Gen Loss : 3643.44775390625, \t Total Dis Loss : 4.035151141579263e-05\n",
      "Steps : 137000, \t Total Gen Loss : 3284.6337890625, \t Total Dis Loss : 4.508372148848139e-05\n",
      "Steps : 137100, \t Total Gen Loss : 3214.72412109375, \t Total Dis Loss : 3.220050348318182e-05\n",
      "Steps : 137200, \t Total Gen Loss : 3465.673828125, \t Total Dis Loss : 2.3356100427918136e-05\n",
      "Steps : 137300, \t Total Gen Loss : 3826.483642578125, \t Total Dis Loss : 3.7925292417639866e-05\n",
      "Steps : 137400, \t Total Gen Loss : 3456.9853515625, \t Total Dis Loss : 0.00011075584188802168\n",
      "Steps : 137500, \t Total Gen Loss : 3558.689453125, \t Total Dis Loss : 0.00010540094808675349\n",
      "Steps : 137600, \t Total Gen Loss : 4109.974609375, \t Total Dis Loss : 1.5987565348041244e-05\n",
      "Steps : 137700, \t Total Gen Loss : 3925.43701171875, \t Total Dis Loss : 6.456977644120343e-06\n",
      "Steps : 137800, \t Total Gen Loss : 3412.261474609375, \t Total Dis Loss : 2.1233970983303152e-05\n",
      "Steps : 137900, \t Total Gen Loss : 3360.575927734375, \t Total Dis Loss : 3.302786353742704e-05\n",
      "Steps : 138000, \t Total Gen Loss : 3678.3642578125, \t Total Dis Loss : 0.00013388307706918567\n",
      "Steps : 138100, \t Total Gen Loss : 3517.180908203125, \t Total Dis Loss : 2.1393825591076165e-05\n",
      "Steps : 138200, \t Total Gen Loss : 3930.5048828125, \t Total Dis Loss : 0.0001271412766072899\n",
      "Steps : 138300, \t Total Gen Loss : 3464.014892578125, \t Total Dis Loss : 0.0008701264741830528\n",
      "Steps : 138400, \t Total Gen Loss : 3909.02099609375, \t Total Dis Loss : 3.48559879057575e-05\n",
      "Steps : 138500, \t Total Gen Loss : 3414.5263671875, \t Total Dis Loss : 4.16627008235082e-05\n",
      "Steps : 138600, \t Total Gen Loss : 3397.3232421875, \t Total Dis Loss : 1.3412406588031445e-05\n",
      "Steps : 138700, \t Total Gen Loss : 3679.462646484375, \t Total Dis Loss : 8.58296116348356e-05\n",
      "Steps : 138800, \t Total Gen Loss : 4226.8154296875, \t Total Dis Loss : 4.870065822615288e-06\n",
      "Steps : 138900, \t Total Gen Loss : 3912.31103515625, \t Total Dis Loss : 2.8789891075575724e-05\n",
      "Steps : 139000, \t Total Gen Loss : 3213.761474609375, \t Total Dis Loss : 1.0528206985327415e-05\n",
      "Steps : 139100, \t Total Gen Loss : 3118.548828125, \t Total Dis Loss : 9.245143701264169e-06\n",
      "Steps : 139200, \t Total Gen Loss : 3190.12109375, \t Total Dis Loss : 2.8912556899740594e-06\n",
      "Steps : 139300, \t Total Gen Loss : 4425.14892578125, \t Total Dis Loss : 0.00015543044719379395\n",
      "Steps : 139400, \t Total Gen Loss : 3435.367919921875, \t Total Dis Loss : 7.096112676663324e-05\n",
      "Steps : 139500, \t Total Gen Loss : 4623.93505859375, \t Total Dis Loss : 7.150183955673128e-05\n",
      "Steps : 139600, \t Total Gen Loss : 3574.186279296875, \t Total Dis Loss : 2.853339901776053e-05\n",
      "Steps : 139700, \t Total Gen Loss : 3550.666748046875, \t Total Dis Loss : 2.256275365652982e-05\n",
      "Steps : 139800, \t Total Gen Loss : 3255.6669921875, \t Total Dis Loss : 7.30059327906929e-05\n",
      "Steps : 139900, \t Total Gen Loss : 3471.91796875, \t Total Dis Loss : 0.0005391190061345696\n",
      "Steps : 140000, \t Total Gen Loss : 3419.963623046875, \t Total Dis Loss : 4.784981138072908e-05\n",
      "Steps : 140100, \t Total Gen Loss : 3248.219482421875, \t Total Dis Loss : 6.323640991467983e-05\n",
      "Steps : 140200, \t Total Gen Loss : 3251.067138671875, \t Total Dis Loss : 8.897844236344099e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 140300, \t Total Gen Loss : 3523.71484375, \t Total Dis Loss : 2.9559660106315278e-05\n",
      "Steps : 140400, \t Total Gen Loss : 3832.613525390625, \t Total Dis Loss : 1.4879914488119539e-05\n",
      "Steps : 140500, \t Total Gen Loss : 3303.556640625, \t Total Dis Loss : 3.8344194763340056e-05\n",
      "Steps : 140600, \t Total Gen Loss : 4249.6455078125, \t Total Dis Loss : 7.011787238297984e-05\n",
      "Time for epoch 25 is 76.0022485256195 sec\n",
      "Steps : 140700, \t Total Gen Loss : 3274.25634765625, \t Total Dis Loss : 3.25600994983688e-05\n",
      "Steps : 140800, \t Total Gen Loss : 4397.22265625, \t Total Dis Loss : 9.942924225470051e-05\n",
      "Steps : 140900, \t Total Gen Loss : 4106.29931640625, \t Total Dis Loss : 2.4468368792440742e-05\n",
      "Steps : 141000, \t Total Gen Loss : 3634.451171875, \t Total Dis Loss : 5.699549728888087e-05\n",
      "Steps : 141100, \t Total Gen Loss : 3554.783447265625, \t Total Dis Loss : 2.1094640032970347e-05\n",
      "Steps : 141200, \t Total Gen Loss : 3194.1220703125, \t Total Dis Loss : 8.827949204714969e-05\n",
      "Steps : 141300, \t Total Gen Loss : 3722.422607421875, \t Total Dis Loss : 0.00012403441360220313\n",
      "Steps : 141400, \t Total Gen Loss : 3621.909912109375, \t Total Dis Loss : 3.1397430575452745e-05\n",
      "Steps : 141500, \t Total Gen Loss : 3731.962646484375, \t Total Dis Loss : 8.195580448955297e-05\n",
      "Steps : 141600, \t Total Gen Loss : 3277.59765625, \t Total Dis Loss : 1.222863829752896e-05\n",
      "Steps : 141700, \t Total Gen Loss : 4063.66259765625, \t Total Dis Loss : 2.720017801038921e-05\n",
      "Steps : 141800, \t Total Gen Loss : 3196.031494140625, \t Total Dis Loss : 5.611682718154043e-05\n",
      "Steps : 141900, \t Total Gen Loss : 3625.306884765625, \t Total Dis Loss : 4.602166518452577e-05\n",
      "Steps : 142000, \t Total Gen Loss : 3468.36572265625, \t Total Dis Loss : 4.196802547085099e-05\n",
      "Steps : 142100, \t Total Gen Loss : 3453.319580078125, \t Total Dis Loss : 6.076785211917013e-05\n",
      "Steps : 142200, \t Total Gen Loss : 3832.642333984375, \t Total Dis Loss : 3.2078260119305924e-05\n",
      "Steps : 142300, \t Total Gen Loss : 3623.309326171875, \t Total Dis Loss : 2.4982524337247014e-05\n",
      "Steps : 142400, \t Total Gen Loss : 3443.45556640625, \t Total Dis Loss : 0.0001964960538316518\n",
      "Steps : 142500, \t Total Gen Loss : 4168.84765625, \t Total Dis Loss : 0.0001475195022067055\n",
      "Steps : 142600, \t Total Gen Loss : 3364.243896484375, \t Total Dis Loss : 4.5969500206410885e-05\n",
      "Steps : 142700, \t Total Gen Loss : 3350.81640625, \t Total Dis Loss : 5.7737135648494586e-05\n",
      "Steps : 142800, \t Total Gen Loss : 3688.709228515625, \t Total Dis Loss : 1.4227995052351616e-05\n",
      "Steps : 142900, \t Total Gen Loss : 3201.6396484375, \t Total Dis Loss : 0.0028514997102320194\n",
      "Steps : 143000, \t Total Gen Loss : 3118.937744140625, \t Total Dis Loss : 2.2867096049594693e-05\n",
      "Steps : 143100, \t Total Gen Loss : 3553.623046875, \t Total Dis Loss : 0.00018678898049984127\n",
      "Steps : 143200, \t Total Gen Loss : 3619.275146484375, \t Total Dis Loss : 7.468795956810936e-05\n",
      "Steps : 143300, \t Total Gen Loss : 3579.385009765625, \t Total Dis Loss : 2.393844442849513e-05\n",
      "Steps : 143400, \t Total Gen Loss : 3946.80712890625, \t Total Dis Loss : 0.0002426103746984154\n",
      "Steps : 143500, \t Total Gen Loss : 4124.515625, \t Total Dis Loss : 9.615511226002127e-05\n",
      "Steps : 143600, \t Total Gen Loss : 3663.121337890625, \t Total Dis Loss : 1.400631299475208e-05\n",
      "Steps : 143700, \t Total Gen Loss : 3982.0185546875, \t Total Dis Loss : 1.2436013093974907e-05\n",
      "Steps : 143800, \t Total Gen Loss : 3097.3486328125, \t Total Dis Loss : 6.72471069265157e-05\n",
      "Steps : 143900, \t Total Gen Loss : 3574.210693359375, \t Total Dis Loss : 6.719055818393826e-05\n",
      "Steps : 144000, \t Total Gen Loss : 3577.263427734375, \t Total Dis Loss : 2.7287542252452113e-05\n",
      "Steps : 144100, \t Total Gen Loss : 3313.552001953125, \t Total Dis Loss : 4.253409133525565e-05\n",
      "Steps : 144200, \t Total Gen Loss : 3697.588134765625, \t Total Dis Loss : 0.0006183875375427306\n",
      "Steps : 144300, \t Total Gen Loss : 3576.914306640625, \t Total Dis Loss : 0.00019646839064080268\n",
      "Steps : 144400, \t Total Gen Loss : 4025.183837890625, \t Total Dis Loss : 0.0008521447889506817\n",
      "Steps : 144500, \t Total Gen Loss : 3502.3779296875, \t Total Dis Loss : 7.044094672892243e-06\n",
      "Steps : 144600, \t Total Gen Loss : 3143.281494140625, \t Total Dis Loss : 8.907546543923672e-06\n",
      "Steps : 144700, \t Total Gen Loss : 3641.274169921875, \t Total Dis Loss : 5.968844106973847e-06\n",
      "Steps : 144800, \t Total Gen Loss : 3940.826171875, \t Total Dis Loss : 1.5402927601826377e-05\n",
      "Steps : 144900, \t Total Gen Loss : 3837.79736328125, \t Total Dis Loss : 1.7704738638713025e-05\n",
      "Steps : 145000, \t Total Gen Loss : 4072.0361328125, \t Total Dis Loss : 1.421666456735693e-05\n",
      "Steps : 145100, \t Total Gen Loss : 3666.6865234375, \t Total Dis Loss : 1.0513524102861993e-05\n",
      "Steps : 145200, \t Total Gen Loss : 3123.3447265625, \t Total Dis Loss : 4.825655196327716e-06\n",
      "Steps : 145300, \t Total Gen Loss : 3742.593017578125, \t Total Dis Loss : 1.4192675735102966e-05\n",
      "Steps : 145400, \t Total Gen Loss : 3937.78515625, \t Total Dis Loss : 1.1777960025938228e-05\n",
      "Steps : 145500, \t Total Gen Loss : 3240.54296875, \t Total Dis Loss : 1.5109152627701405e-05\n",
      "Steps : 145600, \t Total Gen Loss : 3240.353759765625, \t Total Dis Loss : 6.413552000594791e-06\n",
      "Steps : 145700, \t Total Gen Loss : 3374.75390625, \t Total Dis Loss : 3.291766915936023e-05\n",
      "Steps : 145800, \t Total Gen Loss : 3448.564208984375, \t Total Dis Loss : 6.148482498247176e-06\n",
      "Steps : 145900, \t Total Gen Loss : 3290.266357421875, \t Total Dis Loss : 0.0011845426633954048\n",
      "Steps : 146000, \t Total Gen Loss : 3417.82666015625, \t Total Dis Loss : 1.7244616174139082e-05\n",
      "Steps : 146100, \t Total Gen Loss : 3624.556640625, \t Total Dis Loss : 3.397915861569345e-05\n",
      "Steps : 146200, \t Total Gen Loss : 3472.0068359375, \t Total Dis Loss : 0.001116520375944674\n",
      "Time for epoch 26 is 75.97979998588562 sec\n",
      "Steps : 146300, \t Total Gen Loss : 3648.45849609375, \t Total Dis Loss : 3.910827945219353e-05\n",
      "Steps : 146400, \t Total Gen Loss : 3955.613037109375, \t Total Dis Loss : 1.2369147043500561e-05\n",
      "Steps : 146500, \t Total Gen Loss : 4198.55615234375, \t Total Dis Loss : 0.0006455868715420365\n",
      "Steps : 146600, \t Total Gen Loss : 3405.186767578125, \t Total Dis Loss : 1.2159296602476388e-05\n",
      "Steps : 146700, \t Total Gen Loss : 3793.695068359375, \t Total Dis Loss : 0.0010200184769928455\n",
      "Steps : 146800, \t Total Gen Loss : 3702.568359375, \t Total Dis Loss : 2.788046003843192e-05\n",
      "Steps : 146900, \t Total Gen Loss : 4185.7451171875, \t Total Dis Loss : 4.486952821025625e-05\n",
      "Steps : 147000, \t Total Gen Loss : 3857.294677734375, \t Total Dis Loss : 5.647674242936773e-06\n",
      "Steps : 147100, \t Total Gen Loss : 2645.779296875, \t Total Dis Loss : 5.262641207082197e-05\n",
      "Steps : 147200, \t Total Gen Loss : 3423.25244140625, \t Total Dis Loss : 2.374741052335594e-05\n",
      "Steps : 147300, \t Total Gen Loss : 3862.537109375, \t Total Dis Loss : 6.5311134676449e-05\n",
      "Steps : 147400, \t Total Gen Loss : 3816.0634765625, \t Total Dis Loss : 0.0021488075144588947\n",
      "Steps : 147500, \t Total Gen Loss : 3789.2373046875, \t Total Dis Loss : 8.09920129540842e-06\n",
      "Steps : 147600, \t Total Gen Loss : 4381.7119140625, \t Total Dis Loss : 4.8973462980939075e-05\n",
      "Steps : 147700, \t Total Gen Loss : 3967.205078125, \t Total Dis Loss : 4.528893987298943e-05\n",
      "Steps : 147800, \t Total Gen Loss : 3710.134521484375, \t Total Dis Loss : 2.8216814826009795e-05\n",
      "Steps : 147900, \t Total Gen Loss : 3565.387939453125, \t Total Dis Loss : 3.5639146517496556e-05\n",
      "Steps : 148000, \t Total Gen Loss : 3535.389892578125, \t Total Dis Loss : 8.810446161078289e-05\n",
      "Steps : 148100, \t Total Gen Loss : 3517.036865234375, \t Total Dis Loss : 9.352778579341248e-05\n",
      "Steps : 148200, \t Total Gen Loss : 3428.84228515625, \t Total Dis Loss : 5.8104451454710215e-05\n",
      "Steps : 148300, \t Total Gen Loss : 4074.7978515625, \t Total Dis Loss : 0.00011047764564864337\n",
      "Steps : 148400, \t Total Gen Loss : 3183.353271484375, \t Total Dis Loss : 3.505239146761596e-05\n",
      "Steps : 148500, \t Total Gen Loss : 4025.00830078125, \t Total Dis Loss : 8.170069122570567e-06\n",
      "Steps : 148600, \t Total Gen Loss : 3655.280029296875, \t Total Dis Loss : 3.316099173389375e-05\n",
      "Steps : 148700, \t Total Gen Loss : 3330.067138671875, \t Total Dis Loss : 1.8255186660098843e-05\n",
      "Steps : 148800, \t Total Gen Loss : 3803.419677734375, \t Total Dis Loss : 7.626711067132419e-06\n",
      "Steps : 148900, \t Total Gen Loss : 4133.890625, \t Total Dis Loss : 2.4304899852722883e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 149000, \t Total Gen Loss : 4599.97412109375, \t Total Dis Loss : 9.64940591074992e-06\n",
      "Steps : 149100, \t Total Gen Loss : 3295.107666015625, \t Total Dis Loss : 1.453036384191364e-05\n",
      "Steps : 149200, \t Total Gen Loss : 3440.41162109375, \t Total Dis Loss : 6.79234872222878e-05\n",
      "Steps : 149300, \t Total Gen Loss : 3825.06396484375, \t Total Dis Loss : 2.9317434382392094e-05\n",
      "Steps : 149400, \t Total Gen Loss : 3537.441162109375, \t Total Dis Loss : 8.999479177873582e-05\n",
      "Steps : 149500, \t Total Gen Loss : 3183.209228515625, \t Total Dis Loss : 0.00015833493671379983\n",
      "Steps : 149600, \t Total Gen Loss : 3887.36474609375, \t Total Dis Loss : 2.3303991838474758e-05\n",
      "Steps : 149700, \t Total Gen Loss : 3702.9287109375, \t Total Dis Loss : 4.316337435739115e-05\n",
      "Steps : 149800, \t Total Gen Loss : 4026.78955078125, \t Total Dis Loss : 0.0001489036512793973\n",
      "Steps : 149900, \t Total Gen Loss : 3545.47509765625, \t Total Dis Loss : 0.00013994450273457915\n",
      "Steps : 150000, \t Total Gen Loss : 3798.82470703125, \t Total Dis Loss : 0.00011745002120733261\n",
      "Steps : 150100, \t Total Gen Loss : 3693.1083984375, \t Total Dis Loss : 5.085291195427999e-05\n",
      "Steps : 150200, \t Total Gen Loss : 3862.85302734375, \t Total Dis Loss : 0.00018991544493474066\n",
      "Steps : 150300, \t Total Gen Loss : 3628.6787109375, \t Total Dis Loss : 1.4631000340159517e-05\n",
      "Steps : 150400, \t Total Gen Loss : 3905.95361328125, \t Total Dis Loss : 1.5129822713788599e-05\n",
      "Steps : 150500, \t Total Gen Loss : 3918.5205078125, \t Total Dis Loss : 1.0133380783372559e-05\n",
      "Steps : 150600, \t Total Gen Loss : 3274.45263671875, \t Total Dis Loss : 7.327472758333897e-06\n",
      "Steps : 150700, \t Total Gen Loss : 3183.80322265625, \t Total Dis Loss : 7.434996405208949e-06\n",
      "Steps : 150800, \t Total Gen Loss : 3612.24365234375, \t Total Dis Loss : 5.889169187867083e-06\n",
      "Steps : 150900, \t Total Gen Loss : 3225.334228515625, \t Total Dis Loss : 8.717924356460571e-05\n",
      "Steps : 151000, \t Total Gen Loss : 3676.4033203125, \t Total Dis Loss : 3.860906872432679e-06\n",
      "Steps : 151100, \t Total Gen Loss : 3717.618408203125, \t Total Dis Loss : 2.6377743779448792e-05\n",
      "Steps : 151200, \t Total Gen Loss : 4032.073486328125, \t Total Dis Loss : 4.966471533407457e-05\n",
      "Steps : 151300, \t Total Gen Loss : 3310.222900390625, \t Total Dis Loss : 2.033050259342417e-05\n",
      "Steps : 151400, \t Total Gen Loss : 3313.107421875, \t Total Dis Loss : 4.474213346838951e-05\n",
      "Steps : 151500, \t Total Gen Loss : 3474.359619140625, \t Total Dis Loss : 2.4137101718224585e-05\n",
      "Steps : 151600, \t Total Gen Loss : 3821.62060546875, \t Total Dis Loss : 0.000937284785322845\n",
      "Steps : 151700, \t Total Gen Loss : 3791.28564453125, \t Total Dis Loss : 5.147210686118342e-06\n",
      "Steps : 151800, \t Total Gen Loss : 3597.16748046875, \t Total Dis Loss : 0.0006601476343348622\n",
      "Time for epoch 27 is 77.90729451179504 sec\n",
      "Steps : 151900, \t Total Gen Loss : 3858.65625, \t Total Dis Loss : 7.964003998495173e-06\n",
      "Steps : 152000, \t Total Gen Loss : 3725.357177734375, \t Total Dis Loss : 1.2929351214552298e-05\n",
      "Steps : 152100, \t Total Gen Loss : 3550.12841796875, \t Total Dis Loss : 0.00011490940232761204\n",
      "Steps : 152200, \t Total Gen Loss : 3953.9833984375, \t Total Dis Loss : 1.0830441169673577e-05\n",
      "Steps : 152300, \t Total Gen Loss : 3255.8046875, \t Total Dis Loss : 3.875837137456983e-06\n",
      "Steps : 152400, \t Total Gen Loss : 3521.33935546875, \t Total Dis Loss : 6.036388185748365e-06\n",
      "Steps : 152500, \t Total Gen Loss : 3647.09765625, \t Total Dis Loss : 0.00012810016050934792\n",
      "Steps : 152600, \t Total Gen Loss : 3750.673583984375, \t Total Dis Loss : 1.8347652712691342e-06\n",
      "Steps : 152700, \t Total Gen Loss : 3693.601318359375, \t Total Dis Loss : 7.3156802500307094e-06\n",
      "Steps : 152800, \t Total Gen Loss : 3534.648193359375, \t Total Dis Loss : 4.529505531536415e-06\n",
      "Steps : 152900, \t Total Gen Loss : 3191.05029296875, \t Total Dis Loss : 2.519311965443194e-05\n",
      "Steps : 153000, \t Total Gen Loss : 3698.203369140625, \t Total Dis Loss : 8.028160664252937e-06\n",
      "Steps : 153100, \t Total Gen Loss : 3752.0693359375, \t Total Dis Loss : 1.0837447916856036e-05\n",
      "Steps : 153200, \t Total Gen Loss : 3405.6630859375, \t Total Dis Loss : 6.653763557551429e-05\n",
      "Steps : 153300, \t Total Gen Loss : 4392.625, \t Total Dis Loss : 0.00010677975660655648\n",
      "Steps : 153400, \t Total Gen Loss : 3426.4931640625, \t Total Dis Loss : 0.00013286652392707765\n",
      "Steps : 153500, \t Total Gen Loss : 4105.65380859375, \t Total Dis Loss : 0.003275732509791851\n",
      "Steps : 153600, \t Total Gen Loss : 3715.4609375, \t Total Dis Loss : 0.0001242156431544572\n",
      "Steps : 153700, \t Total Gen Loss : 3421.6337890625, \t Total Dis Loss : 1.6000914911273867e-05\n",
      "Steps : 153800, \t Total Gen Loss : 4248.8974609375, \t Total Dis Loss : 4.749721847474575e-05\n",
      "Steps : 153900, \t Total Gen Loss : 3571.28466796875, \t Total Dis Loss : 9.941388270817697e-06\n",
      "Steps : 154000, \t Total Gen Loss : 3610.3818359375, \t Total Dis Loss : 2.727008904912509e-05\n",
      "Steps : 154100, \t Total Gen Loss : 3484.203369140625, \t Total Dis Loss : 0.00012055259139742702\n",
      "Steps : 154200, \t Total Gen Loss : 3019.892578125, \t Total Dis Loss : 3.163443034281954e-05\n",
      "Steps : 154300, \t Total Gen Loss : 3865.930908203125, \t Total Dis Loss : 1.317937312705908e-05\n",
      "Steps : 154400, \t Total Gen Loss : 4005.544189453125, \t Total Dis Loss : 7.701800313952845e-06\n",
      "Steps : 154500, \t Total Gen Loss : 4068.84130859375, \t Total Dis Loss : 3.158932304359041e-05\n",
      "Steps : 154600, \t Total Gen Loss : 3233.482666015625, \t Total Dis Loss : 7.75838052504696e-05\n",
      "Steps : 154700, \t Total Gen Loss : 3584.29638671875, \t Total Dis Loss : 3.686351919895969e-05\n",
      "Steps : 154800, \t Total Gen Loss : 3912.47802734375, \t Total Dis Loss : 0.00013814785052090883\n",
      "Steps : 154900, \t Total Gen Loss : 3547.177001953125, \t Total Dis Loss : 1.6555231923121028e-05\n",
      "Steps : 155000, \t Total Gen Loss : 3141.71484375, \t Total Dis Loss : 7.0352398324757814e-06\n",
      "Steps : 155100, \t Total Gen Loss : 3878.49951171875, \t Total Dis Loss : 1.2231179425725713e-05\n",
      "Steps : 155200, \t Total Gen Loss : 3896.612548828125, \t Total Dis Loss : 3.00517313007731e-05\n",
      "Steps : 155300, \t Total Gen Loss : 3501.29833984375, \t Total Dis Loss : 1.2060318113071844e-05\n",
      "Steps : 155400, \t Total Gen Loss : 3859.96240234375, \t Total Dis Loss : 5.244187377684284e-06\n",
      "Steps : 155500, \t Total Gen Loss : 4190.634765625, \t Total Dis Loss : 0.0018243095837533474\n",
      "Steps : 155600, \t Total Gen Loss : 3705.4248046875, \t Total Dis Loss : 3.1817560284252977e-06\n",
      "Steps : 155700, \t Total Gen Loss : 3734.531982421875, \t Total Dis Loss : 4.12284543926944e-06\n",
      "Steps : 155800, \t Total Gen Loss : 3877.86669921875, \t Total Dis Loss : 3.7512149901886005e-06\n",
      "Steps : 155900, \t Total Gen Loss : 3873.777587890625, \t Total Dis Loss : 7.715006177022588e-06\n",
      "Steps : 156000, \t Total Gen Loss : 3601.180908203125, \t Total Dis Loss : 0.001744774985127151\n",
      "Steps : 156100, \t Total Gen Loss : 3441.130615234375, \t Total Dis Loss : 4.972697752236854e-06\n",
      "Steps : 156200, \t Total Gen Loss : 3190.654541015625, \t Total Dis Loss : 1.5099214579095133e-05\n",
      "Steps : 156300, \t Total Gen Loss : 3755.170654296875, \t Total Dis Loss : 3.117034793831408e-05\n",
      "Steps : 156400, \t Total Gen Loss : 4059.79736328125, \t Total Dis Loss : 3.213379386579618e-05\n",
      "Steps : 156500, \t Total Gen Loss : 3653.235595703125, \t Total Dis Loss : 1.667729884502478e-05\n",
      "Steps : 156600, \t Total Gen Loss : 4135.74169921875, \t Total Dis Loss : 1.4035745152796153e-05\n",
      "Steps : 156700, \t Total Gen Loss : 3322.13818359375, \t Total Dis Loss : 1.805817191780079e-05\n",
      "Steps : 156800, \t Total Gen Loss : 3308.2216796875, \t Total Dis Loss : 1.3537010090658441e-05\n",
      "Steps : 156900, \t Total Gen Loss : 4049.886474609375, \t Total Dis Loss : 3.4716096706688404e-05\n",
      "Steps : 157000, \t Total Gen Loss : 3505.5263671875, \t Total Dis Loss : 1.1703989002853632e-05\n",
      "Steps : 157100, \t Total Gen Loss : 3272.336181640625, \t Total Dis Loss : 1.546172279631719e-05\n",
      "Steps : 157200, \t Total Gen Loss : 3622.82763671875, \t Total Dis Loss : 9.181505447486416e-06\n",
      "Steps : 157300, \t Total Gen Loss : 3472.101318359375, \t Total Dis Loss : 5.740771666751243e-05\n",
      "Steps : 157400, \t Total Gen Loss : 4053.640869140625, \t Total Dis Loss : 1.640641494304873e-05\n",
      "Steps : 157500, \t Total Gen Loss : 2955.49755859375, \t Total Dis Loss : 5.905244506720919e-06\n",
      "Time for epoch 28 is 78.01458549499512 sec\n",
      "Steps : 157600, \t Total Gen Loss : 3112.2265625, \t Total Dis Loss : 9.669544851931278e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 157700, \t Total Gen Loss : 3440.85205078125, \t Total Dis Loss : 0.00043424247996881604\n",
      "Steps : 157800, \t Total Gen Loss : 3451.072509765625, \t Total Dis Loss : 7.1104436756286304e-06\n",
      "Steps : 157900, \t Total Gen Loss : 3849.56103515625, \t Total Dis Loss : 2.4680703063495457e-05\n",
      "Steps : 158000, \t Total Gen Loss : 3867.8369140625, \t Total Dis Loss : 1.1568895388336387e-05\n",
      "Steps : 158100, \t Total Gen Loss : 3354.132568359375, \t Total Dis Loss : 1.0114900760527235e-05\n",
      "Steps : 158200, \t Total Gen Loss : 3576.197021484375, \t Total Dis Loss : 0.000147445680340752\n",
      "Steps : 158300, \t Total Gen Loss : 3682.740478515625, \t Total Dis Loss : 6.297274467215175e-06\n",
      "Steps : 158400, \t Total Gen Loss : 3151.888916015625, \t Total Dis Loss : 4.449611878953874e-06\n",
      "Steps : 158500, \t Total Gen Loss : 3582.49658203125, \t Total Dis Loss : 9.36342439672444e-06\n",
      "Steps : 158600, \t Total Gen Loss : 3993.907958984375, \t Total Dis Loss : 6.450959335779771e-05\n",
      "Steps : 158700, \t Total Gen Loss : 4105.03369140625, \t Total Dis Loss : 1.4817334886174649e-05\n",
      "Steps : 158800, \t Total Gen Loss : 3770.956298828125, \t Total Dis Loss : 5.709560355171561e-06\n",
      "Steps : 158900, \t Total Gen Loss : 3247.452392578125, \t Total Dis Loss : 4.45362129539717e-05\n",
      "Steps : 159000, \t Total Gen Loss : 3070.03955078125, \t Total Dis Loss : 6.161013880046085e-05\n",
      "Steps : 159100, \t Total Gen Loss : 3524.2529296875, \t Total Dis Loss : 8.0563049777993e-06\n",
      "Steps : 159200, \t Total Gen Loss : 3619.712158203125, \t Total Dis Loss : 2.4873079382814467e-06\n",
      "Steps : 159300, \t Total Gen Loss : 2969.0849609375, \t Total Dis Loss : 8.767783583607525e-05\n",
      "Steps : 159400, \t Total Gen Loss : 3991.008056640625, \t Total Dis Loss : 6.546553777297959e-06\n",
      "Steps : 159500, \t Total Gen Loss : 3664.673095703125, \t Total Dis Loss : 0.00014091894263401628\n",
      "Steps : 159600, \t Total Gen Loss : 3601.199951171875, \t Total Dis Loss : 1.4195058611221611e-05\n",
      "Steps : 159700, \t Total Gen Loss : 3602.84375, \t Total Dis Loss : 2.608104477985762e-05\n",
      "Steps : 159800, \t Total Gen Loss : 3689.755615234375, \t Total Dis Loss : 8.625332156952936e-06\n",
      "Steps : 159900, \t Total Gen Loss : 3614.47216796875, \t Total Dis Loss : 0.00011987436300842091\n",
      "Steps : 160000, \t Total Gen Loss : 3821.005126953125, \t Total Dis Loss : 0.00040925387293100357\n",
      "Steps : 160100, \t Total Gen Loss : 4165.27490234375, \t Total Dis Loss : 0.00016235018847510219\n",
      "Steps : 160200, \t Total Gen Loss : 3646.790771484375, \t Total Dis Loss : 0.0002813391329254955\n",
      "Steps : 160300, \t Total Gen Loss : 3892.609375, \t Total Dis Loss : 7.294610259123147e-05\n",
      "Steps : 160400, \t Total Gen Loss : 3201.906005859375, \t Total Dis Loss : 4.7973562686820515e-06\n",
      "Steps : 160500, \t Total Gen Loss : 3598.880126953125, \t Total Dis Loss : 7.620212727488251e-06\n",
      "Steps : 160600, \t Total Gen Loss : 3395.0244140625, \t Total Dis Loss : 3.307343831693288e-06\n",
      "Steps : 160700, \t Total Gen Loss : 3750.7587890625, \t Total Dis Loss : 2.679673343664035e-06\n",
      "Steps : 160800, \t Total Gen Loss : 3540.520263671875, \t Total Dis Loss : 0.0015564261702820659\n",
      "Steps : 160900, \t Total Gen Loss : 2941.146484375, \t Total Dis Loss : 1.8440730855218135e-05\n",
      "Steps : 161000, \t Total Gen Loss : 3223.076416015625, \t Total Dis Loss : 3.223660314688459e-06\n",
      "Steps : 161100, \t Total Gen Loss : 3531.40283203125, \t Total Dis Loss : 3.3554124456713907e-06\n",
      "Steps : 161200, \t Total Gen Loss : 3813.368408203125, \t Total Dis Loss : 1.3769247743766755e-05\n",
      "Steps : 161300, \t Total Gen Loss : 3827.9375, \t Total Dis Loss : 0.0009350814507342875\n",
      "Steps : 161400, \t Total Gen Loss : 3155.8974609375, \t Total Dis Loss : 1.461586407458526e-06\n",
      "Steps : 161500, \t Total Gen Loss : 3411.175537109375, \t Total Dis Loss : 1.6293144653900526e-05\n",
      "Steps : 161600, \t Total Gen Loss : 4146.162109375, \t Total Dis Loss : 9.275658521801233e-05\n",
      "Steps : 161700, \t Total Gen Loss : 3810.9892578125, \t Total Dis Loss : 0.00012127371155656874\n",
      "Steps : 161800, \t Total Gen Loss : 3381.14208984375, \t Total Dis Loss : 2.2555526811629534e-05\n",
      "Steps : 161900, \t Total Gen Loss : 3758.1103515625, \t Total Dis Loss : 2.0197259800625034e-05\n",
      "Steps : 162000, \t Total Gen Loss : 3822.23779296875, \t Total Dis Loss : 2.39130858972203e-05\n",
      "Steps : 162100, \t Total Gen Loss : 3445.694580078125, \t Total Dis Loss : 0.0006452553789131343\n",
      "Steps : 162200, \t Total Gen Loss : 3716.53466796875, \t Total Dis Loss : 1.5922538295853883e-05\n",
      "Steps : 162300, \t Total Gen Loss : 3255.32080078125, \t Total Dis Loss : 2.0154619051027112e-05\n",
      "Steps : 162400, \t Total Gen Loss : 3795.1708984375, \t Total Dis Loss : 2.192704960179981e-05\n",
      "Steps : 162500, \t Total Gen Loss : 3131.52099609375, \t Total Dis Loss : 0.00017387319530826062\n",
      "Steps : 162600, \t Total Gen Loss : 4196.8251953125, \t Total Dis Loss : 6.470562948379666e-05\n",
      "Steps : 162700, \t Total Gen Loss : 2998.956298828125, \t Total Dis Loss : 0.000898481288459152\n",
      "Steps : 162800, \t Total Gen Loss : 3654.114990234375, \t Total Dis Loss : 0.001579191768541932\n",
      "Steps : 162900, \t Total Gen Loss : 3325.70166015625, \t Total Dis Loss : 2.9256118523335317e-06\n",
      "Steps : 163000, \t Total Gen Loss : 2812.85595703125, \t Total Dis Loss : 0.00016406262875534594\n",
      "Steps : 163100, \t Total Gen Loss : 3376.129150390625, \t Total Dis Loss : 0.002922116545960307\n",
      "Time for epoch 29 is 76.74427819252014 sec\n",
      "Steps : 163200, \t Total Gen Loss : 3136.31640625, \t Total Dis Loss : 1.6594993212493137e-05\n",
      "Steps : 163300, \t Total Gen Loss : 3552.6484375, \t Total Dis Loss : 9.315620263805613e-05\n",
      "Steps : 163400, \t Total Gen Loss : 3408.169921875, \t Total Dis Loss : 7.355240813922137e-05\n",
      "Steps : 163500, \t Total Gen Loss : 3668.297607421875, \t Total Dis Loss : 1.7159256458398886e-05\n",
      "Steps : 163600, \t Total Gen Loss : 3856.843017578125, \t Total Dis Loss : 1.0915124221355654e-05\n",
      "Steps : 163700, \t Total Gen Loss : 3912.2734375, \t Total Dis Loss : 9.039064934768248e-06\n",
      "Steps : 163800, \t Total Gen Loss : 3759.736328125, \t Total Dis Loss : 1.2835067536798306e-05\n",
      "Steps : 163900, \t Total Gen Loss : 3614.845458984375, \t Total Dis Loss : 2.4792272597551346e-05\n",
      "Steps : 164000, \t Total Gen Loss : 3474.171875, \t Total Dis Loss : 0.0004993631737306714\n",
      "Steps : 164100, \t Total Gen Loss : 3767.509033203125, \t Total Dis Loss : 6.5044105213019066e-06\n",
      "Steps : 164200, \t Total Gen Loss : 3161.03857421875, \t Total Dis Loss : 2.1439338524942286e-05\n",
      "Steps : 164300, \t Total Gen Loss : 3202.7265625, \t Total Dis Loss : 2.255331310152542e-05\n",
      "Steps : 164400, \t Total Gen Loss : 4132.0830078125, \t Total Dis Loss : 7.580425881315023e-05\n",
      "Steps : 164500, \t Total Gen Loss : 3536.550537109375, \t Total Dis Loss : 1.1636010640359018e-05\n",
      "Steps : 164600, \t Total Gen Loss : 3967.68212890625, \t Total Dis Loss : 2.747297185123898e-05\n",
      "Steps : 164700, \t Total Gen Loss : 3407.323974609375, \t Total Dis Loss : 3.896804992109537e-05\n",
      "Steps : 164800, \t Total Gen Loss : 3379.6591796875, \t Total Dis Loss : 1.1091422493336722e-05\n",
      "Steps : 164900, \t Total Gen Loss : 3296.53125, \t Total Dis Loss : 6.603916517633479e-06\n",
      "Steps : 165000, \t Total Gen Loss : 3336.1259765625, \t Total Dis Loss : 2.8239715902600437e-05\n",
      "Steps : 165100, \t Total Gen Loss : 3729.843017578125, \t Total Dis Loss : 4.2514593587839045e-06\n",
      "Steps : 165200, \t Total Gen Loss : 3498.2724609375, \t Total Dis Loss : 1.0891497367992997e-05\n",
      "Steps : 165300, \t Total Gen Loss : 3506.87646484375, \t Total Dis Loss : 7.471969001926482e-05\n",
      "Steps : 165400, \t Total Gen Loss : 3962.708984375, \t Total Dis Loss : 6.8308563641039655e-06\n",
      "Steps : 165500, \t Total Gen Loss : 2961.4619140625, \t Total Dis Loss : 2.5975800781452563e-06\n",
      "Steps : 165600, \t Total Gen Loss : 3829.96533203125, \t Total Dis Loss : 1.3899636542191729e-05\n",
      "Steps : 165700, \t Total Gen Loss : 3391.494384765625, \t Total Dis Loss : 9.285417036153376e-06\n",
      "Steps : 165800, \t Total Gen Loss : 3662.86767578125, \t Total Dis Loss : 6.96502092978335e-06\n",
      "Steps : 165900, \t Total Gen Loss : 3526.7822265625, \t Total Dis Loss : 4.570277451421134e-05\n",
      "Steps : 166000, \t Total Gen Loss : 4235.43212890625, \t Total Dis Loss : 1.4642456335423049e-05\n",
      "Steps : 166100, \t Total Gen Loss : 3520.156982421875, \t Total Dis Loss : 1.5504856492043473e-05\n",
      "Steps : 166200, \t Total Gen Loss : 3578.631103515625, \t Total Dis Loss : 4.731503850052832e-06\n",
      "Steps : 166300, \t Total Gen Loss : 3431.94873046875, \t Total Dis Loss : 2.7588504963205196e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 166400, \t Total Gen Loss : 3382.58544921875, \t Total Dis Loss : 4.21212098444812e-05\n",
      "Steps : 166500, \t Total Gen Loss : 3170.184814453125, \t Total Dis Loss : 2.5447026928304695e-05\n",
      "Steps : 166600, \t Total Gen Loss : 3310.037109375, \t Total Dis Loss : 2.652731564012356e-05\n",
      "Steps : 166700, \t Total Gen Loss : 3931.9951171875, \t Total Dis Loss : 3.764221764868125e-05\n",
      "Steps : 166800, \t Total Gen Loss : 3769.434814453125, \t Total Dis Loss : 3.3006239391397685e-05\n",
      "Steps : 166900, \t Total Gen Loss : 3637.44482421875, \t Total Dis Loss : 0.00010795588605105877\n",
      "Steps : 167000, \t Total Gen Loss : 3317.22705078125, \t Total Dis Loss : 1.702649751678109e-05\n",
      "Steps : 167100, \t Total Gen Loss : 3647.20263671875, \t Total Dis Loss : 2.6778590836329386e-05\n",
      "Steps : 167200, \t Total Gen Loss : 4175.095703125, \t Total Dis Loss : 3.1914089049678296e-05\n",
      "Steps : 167300, \t Total Gen Loss : 3390.6630859375, \t Total Dis Loss : 3.1408788345288485e-05\n",
      "Steps : 167400, \t Total Gen Loss : 3218.014404296875, \t Total Dis Loss : 1.913246524054557e-05\n",
      "Steps : 167500, \t Total Gen Loss : 3632.194091796875, \t Total Dis Loss : 1.00816923804814e-05\n",
      "Steps : 167600, \t Total Gen Loss : 3664.85888671875, \t Total Dis Loss : 0.0011801361106336117\n",
      "Steps : 167700, \t Total Gen Loss : 3760.412109375, \t Total Dis Loss : 4.185778379905969e-05\n",
      "Steps : 167800, \t Total Gen Loss : 3481.57177734375, \t Total Dis Loss : 3.742405169759877e-05\n",
      "Steps : 167900, \t Total Gen Loss : 3182.100341796875, \t Total Dis Loss : 4.954598261974752e-05\n",
      "Steps : 168000, \t Total Gen Loss : 3449.474365234375, \t Total Dis Loss : 2.5172186724375933e-05\n",
      "Steps : 168100, \t Total Gen Loss : 3044.564697265625, \t Total Dis Loss : 4.481217547436245e-05\n",
      "Steps : 168200, \t Total Gen Loss : 3677.266845703125, \t Total Dis Loss : 6.778123861295171e-06\n",
      "Steps : 168300, \t Total Gen Loss : 3779.6298828125, \t Total Dis Loss : 1.6881749616004527e-05\n",
      "Steps : 168400, \t Total Gen Loss : 3802.202392578125, \t Total Dis Loss : 5.546679312828928e-05\n",
      "Steps : 168500, \t Total Gen Loss : 3689.6181640625, \t Total Dis Loss : 3.6515342799248174e-05\n",
      "Steps : 168600, \t Total Gen Loss : 4060.051025390625, \t Total Dis Loss : 8.816970876068808e-06\n",
      "Steps : 168700, \t Total Gen Loss : 3570.027587890625, \t Total Dis Loss : 1.232018621522002e-05\n",
      "Time for epoch 30 is 75.45677208900452 sec\n",
      "Steps : 168800, \t Total Gen Loss : 3018.85302734375, \t Total Dis Loss : 1.8747190551948734e-05\n",
      "Steps : 168900, \t Total Gen Loss : 3737.658447265625, \t Total Dis Loss : 0.002500430680811405\n",
      "Steps : 169000, \t Total Gen Loss : 3484.11669921875, \t Total Dis Loss : 3.3744196116458625e-05\n",
      "Steps : 169100, \t Total Gen Loss : 2931.435546875, \t Total Dis Loss : 4.431023262441158e-05\n",
      "Steps : 169200, \t Total Gen Loss : 3863.2265625, \t Total Dis Loss : 9.221350410371087e-06\n",
      "Steps : 169300, \t Total Gen Loss : 3887.537353515625, \t Total Dis Loss : 1.732570854073856e-05\n",
      "Steps : 169400, \t Total Gen Loss : 3538.91796875, \t Total Dis Loss : 6.30084741715109e-06\n",
      "Steps : 169500, \t Total Gen Loss : 3323.06689453125, \t Total Dis Loss : 1.3224351278040558e-05\n",
      "Steps : 169600, \t Total Gen Loss : 3687.5224609375, \t Total Dis Loss : 7.964401447679847e-06\n",
      "Steps : 169700, \t Total Gen Loss : 4346.84130859375, \t Total Dis Loss : 4.646420165954623e-06\n",
      "Steps : 169800, \t Total Gen Loss : 3839.648681640625, \t Total Dis Loss : 3.5836990718962625e-05\n",
      "Steps : 169900, \t Total Gen Loss : 3752.591064453125, \t Total Dis Loss : 0.00010327866766601801\n",
      "Steps : 170000, \t Total Gen Loss : 3470.73291015625, \t Total Dis Loss : 7.822085535735823e-06\n",
      "Steps : 170100, \t Total Gen Loss : 3742.773193359375, \t Total Dis Loss : 0.3408343195915222\n",
      "Steps : 170200, \t Total Gen Loss : 3573.494384765625, \t Total Dis Loss : 8.220579275075579e-07\n",
      "Steps : 170300, \t Total Gen Loss : 4378.70068359375, \t Total Dis Loss : 5.745159796788357e-06\n",
      "Steps : 170400, \t Total Gen Loss : 3706.841552734375, \t Total Dis Loss : 0.00010398486483609304\n",
      "Steps : 170500, \t Total Gen Loss : 4563.4892578125, \t Total Dis Loss : 5.778591912530828e-06\n",
      "Steps : 170600, \t Total Gen Loss : 4544.32666015625, \t Total Dis Loss : 2.4668306650710292e-06\n",
      "Steps : 170700, \t Total Gen Loss : 3718.665283203125, \t Total Dis Loss : 4.887552222498925e-06\n",
      "Steps : 170800, \t Total Gen Loss : 3488.7275390625, \t Total Dis Loss : 6.127663255028892e-06\n",
      "Steps : 170900, \t Total Gen Loss : 3571.392333984375, \t Total Dis Loss : 0.00013149605365470052\n",
      "Steps : 171000, \t Total Gen Loss : 3661.670166015625, \t Total Dis Loss : 0.0016815979033708572\n",
      "Steps : 171100, \t Total Gen Loss : 3471.4453125, \t Total Dis Loss : 0.0001866828533820808\n",
      "Steps : 171200, \t Total Gen Loss : 3268.433837890625, \t Total Dis Loss : 1.1951012311328668e-05\n",
      "Steps : 171300, \t Total Gen Loss : 3663.19091796875, \t Total Dis Loss : 2.0764980945386924e-05\n",
      "Steps : 171400, \t Total Gen Loss : 3007.499267578125, \t Total Dis Loss : 9.21374885365367e-06\n",
      "Steps : 171500, \t Total Gen Loss : 3347.782958984375, \t Total Dis Loss : 4.033027289551683e-05\n",
      "Steps : 171600, \t Total Gen Loss : 3095.84423828125, \t Total Dis Loss : 0.00014533096691593528\n",
      "Steps : 171700, \t Total Gen Loss : 3360.02685546875, \t Total Dis Loss : 0.016588250175118446\n",
      "Steps : 171800, \t Total Gen Loss : 3565.020751953125, \t Total Dis Loss : 8.860296475177165e-06\n",
      "Steps : 171900, \t Total Gen Loss : 3950.953125, \t Total Dis Loss : 2.142204721167218e-06\n",
      "Steps : 172000, \t Total Gen Loss : 3764.377685546875, \t Total Dis Loss : 1.5660585631849244e-05\n",
      "Steps : 172100, \t Total Gen Loss : 3833.277587890625, \t Total Dis Loss : 5.658983354805969e-06\n",
      "Steps : 172200, \t Total Gen Loss : 4046.098876953125, \t Total Dis Loss : 2.4049742933129892e-05\n",
      "Steps : 172300, \t Total Gen Loss : 3972.05517578125, \t Total Dis Loss : 6.545869382534875e-06\n",
      "Steps : 172400, \t Total Gen Loss : 3650.0537109375, \t Total Dis Loss : 2.454764035064727e-05\n",
      "Steps : 172500, \t Total Gen Loss : 3532.90478515625, \t Total Dis Loss : 1.277736373594962e-05\n",
      "Steps : 172600, \t Total Gen Loss : 3721.5478515625, \t Total Dis Loss : 1.8250191715196706e-05\n",
      "Steps : 172700, \t Total Gen Loss : 3659.55322265625, \t Total Dis Loss : 6.7889204728999175e-06\n",
      "Steps : 172800, \t Total Gen Loss : 3907.223388671875, \t Total Dis Loss : 1.472126768931048e-05\n",
      "Steps : 172900, \t Total Gen Loss : 3646.43408203125, \t Total Dis Loss : 4.7172379709081724e-06\n",
      "Steps : 173000, \t Total Gen Loss : 3759.4521484375, \t Total Dis Loss : 2.6761932531371713e-05\n",
      "Steps : 173100, \t Total Gen Loss : 3135.890625, \t Total Dis Loss : 7.925199315650389e-05\n",
      "Steps : 173200, \t Total Gen Loss : 3828.18310546875, \t Total Dis Loss : 1.0385785571997985e-05\n",
      "Steps : 173300, \t Total Gen Loss : 3612.720703125, \t Total Dis Loss : 1.3596978533314541e-05\n",
      "Steps : 173400, \t Total Gen Loss : 3243.810302734375, \t Total Dis Loss : 1.7878099242807366e-05\n",
      "Steps : 173500, \t Total Gen Loss : 3886.3466796875, \t Total Dis Loss : 7.342983280977933e-06\n",
      "Steps : 173600, \t Total Gen Loss : 3811.59375, \t Total Dis Loss : 4.448568506632e-05\n",
      "Steps : 173700, \t Total Gen Loss : 3850.354736328125, \t Total Dis Loss : 1.1196580089745112e-05\n",
      "Steps : 173800, \t Total Gen Loss : 3506.875244140625, \t Total Dis Loss : 2.381932245043572e-05\n",
      "Steps : 173900, \t Total Gen Loss : 3519.29736328125, \t Total Dis Loss : 0.00046586201642639935\n",
      "Steps : 174000, \t Total Gen Loss : 3644.364013671875, \t Total Dis Loss : 9.801073247217573e-06\n",
      "Steps : 174100, \t Total Gen Loss : 4097.14697265625, \t Total Dis Loss : 0.00030761066591367126\n",
      "Steps : 174200, \t Total Gen Loss : 3561.10693359375, \t Total Dis Loss : 7.470518903573975e-05\n",
      "Steps : 174300, \t Total Gen Loss : 3706.428955078125, \t Total Dis Loss : 4.472311047720723e-05\n",
      "Time for epoch 31 is 74.83833646774292 sec\n",
      "Steps : 174400, \t Total Gen Loss : 3524.551025390625, \t Total Dis Loss : 7.642007403774187e-06\n",
      "Steps : 174500, \t Total Gen Loss : 3507.62646484375, \t Total Dis Loss : 1.157613041868899e-05\n",
      "Steps : 174600, \t Total Gen Loss : 3109.913330078125, \t Total Dis Loss : 6.894707621540874e-05\n",
      "Steps : 174700, \t Total Gen Loss : 2945.112548828125, \t Total Dis Loss : 1.5690447980887257e-06\n",
      "Steps : 174800, \t Total Gen Loss : 3510.495361328125, \t Total Dis Loss : 1.6577793076066882e-06\n",
      "Steps : 174900, \t Total Gen Loss : 3561.734130859375, \t Total Dis Loss : 1.4696001926495228e-06\n",
      "Steps : 175000, \t Total Gen Loss : 3702.302734375, \t Total Dis Loss : 1.553542460897006e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 175100, \t Total Gen Loss : 3857.5205078125, \t Total Dis Loss : 1.1166901458636858e-05\n",
      "Steps : 175200, \t Total Gen Loss : 3494.02587890625, \t Total Dis Loss : 0.0017738738097250462\n",
      "Steps : 175300, \t Total Gen Loss : 3860.169921875, \t Total Dis Loss : 3.118989116046578e-05\n",
      "Steps : 175400, \t Total Gen Loss : 3271.790283203125, \t Total Dis Loss : 1.1807875125668943e-05\n",
      "Steps : 175500, \t Total Gen Loss : 3758.3037109375, \t Total Dis Loss : 4.86731696582865e-05\n",
      "Steps : 175600, \t Total Gen Loss : 2773.163330078125, \t Total Dis Loss : 7.5493144322535954e-06\n",
      "Steps : 175700, \t Total Gen Loss : 3899.3515625, \t Total Dis Loss : 5.77198552491609e-05\n",
      "Steps : 175800, \t Total Gen Loss : 3247.49267578125, \t Total Dis Loss : 0.0001226684107678011\n",
      "Steps : 175900, \t Total Gen Loss : 3415.180908203125, \t Total Dis Loss : 0.00010267991456203163\n",
      "Steps : 176000, \t Total Gen Loss : 3850.05029296875, \t Total Dis Loss : 0.001642929739318788\n",
      "Steps : 176100, \t Total Gen Loss : 3472.27197265625, \t Total Dis Loss : 0.00014679375453852117\n",
      "Steps : 176200, \t Total Gen Loss : 3314.0126953125, \t Total Dis Loss : 1.0296223990735598e-05\n",
      "Steps : 176300, \t Total Gen Loss : 3692.48046875, \t Total Dis Loss : 1.8263506717630662e-05\n",
      "Steps : 176400, \t Total Gen Loss : 4252.69287109375, \t Total Dis Loss : 2.808002318488434e-05\n",
      "Steps : 176500, \t Total Gen Loss : 3196.26953125, \t Total Dis Loss : 5.455513473862084e-06\n",
      "Steps : 176600, \t Total Gen Loss : 3352.837890625, \t Total Dis Loss : 9.777233572094701e-06\n",
      "Steps : 176700, \t Total Gen Loss : 3644.095947265625, \t Total Dis Loss : 1.8663784430827945e-05\n",
      "Steps : 176800, \t Total Gen Loss : 3432.667724609375, \t Total Dis Loss : 1.695263563306071e-05\n",
      "Steps : 176900, \t Total Gen Loss : 4011.2080078125, \t Total Dis Loss : 2.441395054120221e-06\n",
      "Steps : 177000, \t Total Gen Loss : 3595.0126953125, \t Total Dis Loss : 1.048501599143492e-05\n",
      "Steps : 177100, \t Total Gen Loss : 3284.157958984375, \t Total Dis Loss : 8.226655154430773e-06\n",
      "Steps : 177200, \t Total Gen Loss : 3522.21923828125, \t Total Dis Loss : 9.08403490029741e-06\n",
      "Steps : 177300, \t Total Gen Loss : 3595.276611328125, \t Total Dis Loss : 1.8943908798974007e-05\n",
      "Steps : 177400, \t Total Gen Loss : 3046.93994140625, \t Total Dis Loss : 4.457193790585734e-05\n",
      "Steps : 177500, \t Total Gen Loss : 3395.221923828125, \t Total Dis Loss : 0.0001614666252862662\n",
      "Steps : 177600, \t Total Gen Loss : 3139.222900390625, \t Total Dis Loss : 5.6861081247916445e-05\n",
      "Steps : 177700, \t Total Gen Loss : 3375.3310546875, \t Total Dis Loss : 0.00011363987869117409\n",
      "Steps : 177800, \t Total Gen Loss : 3574.523681640625, \t Total Dis Loss : 1.619195973034948e-05\n",
      "Steps : 177900, \t Total Gen Loss : 3887.83544921875, \t Total Dis Loss : 4.9177222535945475e-05\n",
      "Steps : 178000, \t Total Gen Loss : 3884.63818359375, \t Total Dis Loss : 1.1501299923111219e-05\n",
      "Steps : 178100, \t Total Gen Loss : 3458.9501953125, \t Total Dis Loss : 5.654527558363043e-05\n",
      "Steps : 178200, \t Total Gen Loss : 3860.227294921875, \t Total Dis Loss : 7.354430636041798e-06\n",
      "Steps : 178300, \t Total Gen Loss : 3773.826416015625, \t Total Dis Loss : 3.657899651443586e-05\n",
      "Steps : 178400, \t Total Gen Loss : 3083.8251953125, \t Total Dis Loss : 1.6185860658879392e-05\n",
      "Steps : 178500, \t Total Gen Loss : 2653.164306640625, \t Total Dis Loss : 3.485409979475662e-05\n",
      "Steps : 178600, \t Total Gen Loss : 3226.65087890625, \t Total Dis Loss : 9.504033187113237e-06\n",
      "Steps : 178700, \t Total Gen Loss : 3421.946533203125, \t Total Dis Loss : 1.9816023268504068e-05\n",
      "Steps : 178800, \t Total Gen Loss : 3150.6376953125, \t Total Dis Loss : 2.308996226929594e-05\n",
      "Steps : 178900, \t Total Gen Loss : 3617.63232421875, \t Total Dis Loss : 0.00044416586752049625\n",
      "Steps : 179000, \t Total Gen Loss : 3455.233154296875, \t Total Dis Loss : 0.00031799872522242367\n",
      "Steps : 179100, \t Total Gen Loss : 4229.369140625, \t Total Dis Loss : 0.0013036461314186454\n",
      "Steps : 179200, \t Total Gen Loss : 3858.93505859375, \t Total Dis Loss : 2.6412668375996873e-05\n",
      "Steps : 179300, \t Total Gen Loss : 3701.394775390625, \t Total Dis Loss : 7.051386637613177e-05\n",
      "Steps : 179400, \t Total Gen Loss : 3791.979248046875, \t Total Dis Loss : 7.426099182339385e-05\n",
      "Steps : 179500, \t Total Gen Loss : 3643.27734375, \t Total Dis Loss : 2.3076601792126894e-05\n",
      "Steps : 179600, \t Total Gen Loss : 3475.016845703125, \t Total Dis Loss : 3.81743157049641e-05\n",
      "Steps : 179700, \t Total Gen Loss : 4075.056396484375, \t Total Dis Loss : 4.499938222579658e-05\n",
      "Steps : 179800, \t Total Gen Loss : 3596.39404296875, \t Total Dis Loss : 2.0899025912513025e-05\n",
      "Steps : 179900, \t Total Gen Loss : 3722.667724609375, \t Total Dis Loss : 1.4163897503749467e-05\n",
      "Steps : 180000, \t Total Gen Loss : 3865.512451171875, \t Total Dis Loss : 5.0630907935556024e-05\n",
      "Time for epoch 32 is 74.8039002418518 sec\n",
      "Steps : 180100, \t Total Gen Loss : 3591.460693359375, \t Total Dis Loss : 0.0003483066684566438\n",
      "Steps : 180200, \t Total Gen Loss : 2984.4658203125, \t Total Dis Loss : 0.00011070913751609623\n",
      "Steps : 180300, \t Total Gen Loss : 4102.1435546875, \t Total Dis Loss : 5.190624506212771e-05\n",
      "Steps : 180400, \t Total Gen Loss : 3096.203857421875, \t Total Dis Loss : 1.6296371541102417e-05\n",
      "Steps : 180500, \t Total Gen Loss : 4021.09521484375, \t Total Dis Loss : 3.211520379409194e-05\n",
      "Steps : 180600, \t Total Gen Loss : 2684.056884765625, \t Total Dis Loss : 1.4945168004487641e-05\n",
      "Steps : 180700, \t Total Gen Loss : 3778.41552734375, \t Total Dis Loss : 1.4481838661595248e-05\n",
      "Steps : 180800, \t Total Gen Loss : 3654.95263671875, \t Total Dis Loss : 5.548190529225394e-06\n",
      "Steps : 180900, \t Total Gen Loss : 2678.771728515625, \t Total Dis Loss : 1.2473953574954066e-05\n",
      "Steps : 181000, \t Total Gen Loss : 3303.594970703125, \t Total Dis Loss : 8.18822009023279e-05\n",
      "Steps : 181100, \t Total Gen Loss : 3606.708251953125, \t Total Dis Loss : 0.0005803773528896272\n",
      "Steps : 181200, \t Total Gen Loss : 3904.5322265625, \t Total Dis Loss : 3.237972123315558e-05\n",
      "Steps : 181300, \t Total Gen Loss : 3715.9306640625, \t Total Dis Loss : 0.0003752049233298749\n",
      "Steps : 181400, \t Total Gen Loss : 3506.03466796875, \t Total Dis Loss : 9.600781777407974e-05\n",
      "Steps : 181500, \t Total Gen Loss : 3696.46435546875, \t Total Dis Loss : 7.839091267669573e-06\n",
      "Steps : 181600, \t Total Gen Loss : 3291.165283203125, \t Total Dis Loss : 2.3090957256499678e-05\n",
      "Steps : 181700, \t Total Gen Loss : 3931.4267578125, \t Total Dis Loss : 9.476851118961349e-05\n",
      "Steps : 181800, \t Total Gen Loss : 3789.39697265625, \t Total Dis Loss : 1.0785178346850444e-05\n",
      "Steps : 181900, \t Total Gen Loss : 3790.132568359375, \t Total Dis Loss : 0.0011990211205556989\n",
      "Steps : 182000, \t Total Gen Loss : 3490.58935546875, \t Total Dis Loss : 7.744834147160873e-05\n",
      "Steps : 182100, \t Total Gen Loss : 3855.59228515625, \t Total Dis Loss : 6.688918801955879e-06\n",
      "Steps : 182200, \t Total Gen Loss : 3607.578857421875, \t Total Dis Loss : 4.001775596407242e-05\n",
      "Steps : 182300, \t Total Gen Loss : 3859.5439453125, \t Total Dis Loss : 4.915325189358555e-05\n",
      "Steps : 182400, \t Total Gen Loss : 3655.62158203125, \t Total Dis Loss : 0.0029537274967879057\n",
      "Steps : 182500, \t Total Gen Loss : 3472.2568359375, \t Total Dis Loss : 1.4199544239090756e-05\n",
      "Steps : 182600, \t Total Gen Loss : 3571.313720703125, \t Total Dis Loss : 4.510311555350199e-06\n",
      "Steps : 182700, \t Total Gen Loss : 3559.42138671875, \t Total Dis Loss : 8.618734136689454e-05\n",
      "Steps : 182800, \t Total Gen Loss : 3853.2294921875, \t Total Dis Loss : 1.3963474884803873e-05\n",
      "Steps : 182900, \t Total Gen Loss : 3757.9990234375, \t Total Dis Loss : 4.808854646398686e-05\n",
      "Steps : 183000, \t Total Gen Loss : 3750.5078125, \t Total Dis Loss : 3.543307684594765e-05\n",
      "Steps : 183100, \t Total Gen Loss : 3110.61083984375, \t Total Dis Loss : 2.1346164430724457e-05\n",
      "Steps : 183200, \t Total Gen Loss : 3353.889404296875, \t Total Dis Loss : 1.5051142327138223e-05\n",
      "Steps : 183300, \t Total Gen Loss : 4077.90966796875, \t Total Dis Loss : 4.171607724856585e-05\n",
      "Steps : 183400, \t Total Gen Loss : 3803.46435546875, \t Total Dis Loss : 4.0611339500173926e-05\n",
      "Steps : 183500, \t Total Gen Loss : 3404.0361328125, \t Total Dis Loss : 2.6858677301788703e-05\n",
      "Steps : 183600, \t Total Gen Loss : 4056.740478515625, \t Total Dis Loss : 1.7497382941655815e-05\n",
      "Steps : 183700, \t Total Gen Loss : 3616.019287109375, \t Total Dis Loss : 2.7989899535896257e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 183800, \t Total Gen Loss : 4097.48779296875, \t Total Dis Loss : 6.184495578054339e-05\n",
      "Steps : 183900, \t Total Gen Loss : 3532.143798828125, \t Total Dis Loss : 0.00013952396693639457\n",
      "Steps : 184000, \t Total Gen Loss : 3759.368896484375, \t Total Dis Loss : 6.755958020221442e-05\n",
      "Steps : 184100, \t Total Gen Loss : 3603.908447265625, \t Total Dis Loss : 7.694106898270547e-05\n",
      "Steps : 184200, \t Total Gen Loss : 3071.6728515625, \t Total Dis Loss : 7.081751391524449e-05\n",
      "Steps : 184300, \t Total Gen Loss : 3745.920166015625, \t Total Dis Loss : 3.0128507205517963e-05\n",
      "Steps : 184400, \t Total Gen Loss : 3742.32470703125, \t Total Dis Loss : 5.359040369512513e-05\n",
      "Steps : 184500, \t Total Gen Loss : 3919.538330078125, \t Total Dis Loss : 1.4027867109689396e-05\n",
      "Steps : 184600, \t Total Gen Loss : 4016.83837890625, \t Total Dis Loss : 2.8473350539570674e-05\n",
      "Steps : 184700, \t Total Gen Loss : 3151.43505859375, \t Total Dis Loss : 7.872507922002114e-06\n",
      "Steps : 184800, \t Total Gen Loss : 3239.443115234375, \t Total Dis Loss : 4.0835915569914505e-05\n",
      "Steps : 184900, \t Total Gen Loss : 3886.733642578125, \t Total Dis Loss : 2.1017576727899723e-05\n",
      "Steps : 185000, \t Total Gen Loss : 4175.51171875, \t Total Dis Loss : 4.3559815821936354e-05\n",
      "Steps : 185100, \t Total Gen Loss : 3331.209716796875, \t Total Dis Loss : 2.5155706680379808e-05\n",
      "Steps : 185200, \t Total Gen Loss : 3387.364501953125, \t Total Dis Loss : 8.439767952950206e-06\n",
      "Steps : 185300, \t Total Gen Loss : 3433.033203125, \t Total Dis Loss : 1.3485250747180544e-05\n",
      "Steps : 185400, \t Total Gen Loss : 3804.654052734375, \t Total Dis Loss : 1.1331940186209977e-05\n",
      "Steps : 185500, \t Total Gen Loss : 3653.35888671875, \t Total Dis Loss : 4.951634764438495e-05\n",
      "Steps : 185600, \t Total Gen Loss : 3805.415283203125, \t Total Dis Loss : 2.6460686058271676e-05\n",
      "Time for epoch 33 is 74.78643274307251 sec\n",
      "Steps : 185700, \t Total Gen Loss : 3152.356201171875, \t Total Dis Loss : 3.1799845601199195e-05\n",
      "Steps : 185800, \t Total Gen Loss : 3184.4296875, \t Total Dis Loss : 2.531938298488967e-05\n",
      "Steps : 185900, \t Total Gen Loss : 3630.221923828125, \t Total Dis Loss : 9.30787791730836e-06\n",
      "Steps : 186000, \t Total Gen Loss : 2888.673583984375, \t Total Dis Loss : 0.0007089056307449937\n",
      "Steps : 186100, \t Total Gen Loss : 3716.2861328125, \t Total Dis Loss : 0.0001423378853360191\n",
      "Steps : 186200, \t Total Gen Loss : 3644.387451171875, \t Total Dis Loss : 2.023919478233438e-05\n",
      "Steps : 186300, \t Total Gen Loss : 3274.2119140625, \t Total Dis Loss : 3.917479625670239e-05\n",
      "Steps : 186400, \t Total Gen Loss : 4201.8828125, \t Total Dis Loss : 1.2348006748652551e-05\n",
      "Steps : 186500, \t Total Gen Loss : 3223.0078125, \t Total Dis Loss : 2.5734465452842414e-05\n",
      "Steps : 186600, \t Total Gen Loss : 3822.870361328125, \t Total Dis Loss : 0.00011162373994011432\n",
      "Steps : 186700, \t Total Gen Loss : 3387.29833984375, \t Total Dis Loss : 1.0734926945588086e-05\n",
      "Steps : 186800, \t Total Gen Loss : 3725.734130859375, \t Total Dis Loss : 2.7897667678189464e-06\n",
      "Steps : 186900, \t Total Gen Loss : 3609.06396484375, \t Total Dis Loss : 1.0491214197827503e-05\n",
      "Steps : 187000, \t Total Gen Loss : 3613.371826171875, \t Total Dis Loss : 2.572667654021643e-05\n",
      "Steps : 187100, \t Total Gen Loss : 3584.521240234375, \t Total Dis Loss : 4.380184691399336e-05\n",
      "Steps : 187200, \t Total Gen Loss : 4019.450439453125, \t Total Dis Loss : 9.479409527557436e-06\n",
      "Steps : 187300, \t Total Gen Loss : 3450.87744140625, \t Total Dis Loss : 3.669875877676532e-05\n",
      "Steps : 187400, \t Total Gen Loss : 3700.740234375, \t Total Dis Loss : 2.730555206653662e-05\n",
      "Steps : 187500, \t Total Gen Loss : 3430.41357421875, \t Total Dis Loss : 1.0731228030635975e-05\n",
      "Steps : 187600, \t Total Gen Loss : 3608.24560546875, \t Total Dis Loss : 2.0199347545712953e-06\n",
      "Steps : 187700, \t Total Gen Loss : 3107.116943359375, \t Total Dis Loss : 2.7358055376680568e-06\n",
      "Steps : 187800, \t Total Gen Loss : 3440.029296875, \t Total Dis Loss : 1.623487514734734e-05\n",
      "Steps : 187900, \t Total Gen Loss : 3710.530517578125, \t Total Dis Loss : 7.18137971489341e-06\n",
      "Steps : 188000, \t Total Gen Loss : 3196.76708984375, \t Total Dis Loss : 2.21323443838628e-06\n",
      "Steps : 188100, \t Total Gen Loss : 3547.998779296875, \t Total Dis Loss : 2.8851559363829438e-06\n",
      "Steps : 188200, \t Total Gen Loss : 3521.4169921875, \t Total Dis Loss : 0.0001554891641717404\n",
      "Steps : 188300, \t Total Gen Loss : 3038.260498046875, \t Total Dis Loss : 1.816862823034171e-05\n",
      "Steps : 188400, \t Total Gen Loss : 3896.0908203125, \t Total Dis Loss : 0.011808219365775585\n",
      "Steps : 188500, \t Total Gen Loss : 4075.151611328125, \t Total Dis Loss : 1.1666184946079738e-05\n",
      "Steps : 188600, \t Total Gen Loss : 4018.03515625, \t Total Dis Loss : 1.1926047591259703e-05\n",
      "Steps : 188700, \t Total Gen Loss : 3803.107177734375, \t Total Dis Loss : 7.13273811925319e-06\n",
      "Steps : 188800, \t Total Gen Loss : 3086.191650390625, \t Total Dis Loss : 6.40636972093489e-06\n",
      "Steps : 188900, \t Total Gen Loss : 3547.374267578125, \t Total Dis Loss : 3.905274752469268e-06\n",
      "Steps : 189000, \t Total Gen Loss : 3672.541259765625, \t Total Dis Loss : 7.938764611026272e-06\n",
      "Steps : 189100, \t Total Gen Loss : 3775.336181640625, \t Total Dis Loss : 1.2177505595900584e-05\n",
      "Steps : 189200, \t Total Gen Loss : 3471.313720703125, \t Total Dis Loss : 5.960903763480019e-06\n",
      "Steps : 189300, \t Total Gen Loss : 3306.167724609375, \t Total Dis Loss : 6.03508779022377e-05\n",
      "Steps : 189400, \t Total Gen Loss : 3652.751220703125, \t Total Dis Loss : 5.302431600284763e-05\n",
      "Steps : 189500, \t Total Gen Loss : 3519.363525390625, \t Total Dis Loss : 1.9430910469964147e-05\n",
      "Steps : 189600, \t Total Gen Loss : 4593.61865234375, \t Total Dis Loss : 9.418292393092997e-06\n",
      "Steps : 189700, \t Total Gen Loss : 3768.69580078125, \t Total Dis Loss : 3.115708295808872e-06\n",
      "Steps : 189800, \t Total Gen Loss : 3430.525634765625, \t Total Dis Loss : 1.349862759525422e-05\n",
      "Steps : 189900, \t Total Gen Loss : 3681.978515625, \t Total Dis Loss : 8.20613422547467e-06\n",
      "Steps : 190000, \t Total Gen Loss : 3987.94482421875, \t Total Dis Loss : 4.668199107982218e-05\n",
      "Steps : 190100, \t Total Gen Loss : 3686.753173828125, \t Total Dis Loss : 3.774623473873362e-05\n",
      "Steps : 190200, \t Total Gen Loss : 3832.377197265625, \t Total Dis Loss : 8.444285776931792e-05\n",
      "Steps : 190300, \t Total Gen Loss : 3409.544677734375, \t Total Dis Loss : 9.769124517333694e-06\n",
      "Steps : 190400, \t Total Gen Loss : 3915.16015625, \t Total Dis Loss : 0.00011775997700169683\n",
      "Steps : 190500, \t Total Gen Loss : 3736.316650390625, \t Total Dis Loss : 3.4115669222956058e-06\n",
      "Steps : 190600, \t Total Gen Loss : 3477.78125, \t Total Dis Loss : 0.016191400587558746\n",
      "Steps : 190700, \t Total Gen Loss : 3046.13427734375, \t Total Dis Loss : 9.498460713075474e-06\n",
      "Steps : 190800, \t Total Gen Loss : 4441.15234375, \t Total Dis Loss : 4.3596669456746895e-06\n",
      "Steps : 190900, \t Total Gen Loss : 3443.200439453125, \t Total Dis Loss : 5.600588337983936e-06\n",
      "Steps : 191000, \t Total Gen Loss : 3035.556396484375, \t Total Dis Loss : 1.0652981472958345e-05\n",
      "Steps : 191100, \t Total Gen Loss : 3837.64111328125, \t Total Dis Loss : 1.8580376490717754e-05\n",
      "Steps : 191200, \t Total Gen Loss : 4080.97900390625, \t Total Dis Loss : 1.378433717036387e-05\n",
      "Time for epoch 34 is 75.93998885154724 sec\n",
      "Steps : 191300, \t Total Gen Loss : 3195.147216796875, \t Total Dis Loss : 3.829963134194259e-06\n",
      "Steps : 191400, \t Total Gen Loss : 3514.8349609375, \t Total Dis Loss : 1.3400716852629557e-05\n",
      "Steps : 191500, \t Total Gen Loss : 3149.0712890625, \t Total Dis Loss : 0.00045373724424280226\n",
      "Steps : 191600, \t Total Gen Loss : 4154.37890625, \t Total Dis Loss : 0.0005308425752446055\n",
      "Steps : 191700, \t Total Gen Loss : 3618.7509765625, \t Total Dis Loss : 0.0008140131249092519\n",
      "Steps : 191800, \t Total Gen Loss : 2884.272216796875, \t Total Dis Loss : 1.555972448841203e-05\n",
      "Steps : 191900, \t Total Gen Loss : 3392.93896484375, \t Total Dis Loss : 1.9686774976435117e-05\n",
      "Steps : 192000, \t Total Gen Loss : 3267.416748046875, \t Total Dis Loss : 2.7674564989865758e-05\n",
      "Steps : 192100, \t Total Gen Loss : 3410.313232421875, \t Total Dis Loss : 3.231573282391764e-05\n",
      "Steps : 192200, \t Total Gen Loss : 3408.37939453125, \t Total Dis Loss : 4.8649355449015275e-05\n",
      "Steps : 192300, \t Total Gen Loss : 3240.097412109375, \t Total Dis Loss : 2.650942769832909e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 192400, \t Total Gen Loss : 4022.754638671875, \t Total Dis Loss : 0.003401324851438403\n",
      "Steps : 192500, \t Total Gen Loss : 4169.61474609375, \t Total Dis Loss : 1.1880869351443835e-05\n",
      "Steps : 192600, \t Total Gen Loss : 3779.236083984375, \t Total Dis Loss : 4.541135240287986e-06\n",
      "Steps : 192700, \t Total Gen Loss : 4178.68212890625, \t Total Dis Loss : 5.737986066378653e-05\n",
      "Steps : 192800, \t Total Gen Loss : 3609.13330078125, \t Total Dis Loss : 2.647373366926331e-05\n",
      "Steps : 192900, \t Total Gen Loss : 3313.80712890625, \t Total Dis Loss : 0.00026847311528399587\n",
      "Steps : 193000, \t Total Gen Loss : 3752.498291015625, \t Total Dis Loss : 3.770207331399433e-05\n",
      "Steps : 193100, \t Total Gen Loss : 3652.851806640625, \t Total Dis Loss : 4.348339280113578e-05\n",
      "Steps : 193200, \t Total Gen Loss : 3510.99169921875, \t Total Dis Loss : 2.385628795309458e-05\n",
      "Steps : 193300, \t Total Gen Loss : 3555.876953125, \t Total Dis Loss : 5.215414603298996e-06\n",
      "Steps : 193400, \t Total Gen Loss : 3246.5361328125, \t Total Dis Loss : 1.8923332390841097e-05\n",
      "Steps : 193500, \t Total Gen Loss : 3646.536376953125, \t Total Dis Loss : 3.354769432917237e-05\n",
      "Steps : 193600, \t Total Gen Loss : 4088.48974609375, \t Total Dis Loss : 5.354507266019937e-06\n",
      "Steps : 193700, \t Total Gen Loss : 4099.73046875, \t Total Dis Loss : 1.6900461559998803e-05\n",
      "Steps : 193800, \t Total Gen Loss : 3270.833984375, \t Total Dis Loss : 4.149002052145079e-06\n",
      "Steps : 193900, \t Total Gen Loss : 3602.634765625, \t Total Dis Loss : 1.3951956816526945e-06\n",
      "Steps : 194000, \t Total Gen Loss : 3711.63330078125, \t Total Dis Loss : 1.9786889424722176e-06\n",
      "Steps : 194100, \t Total Gen Loss : 3645.042724609375, \t Total Dis Loss : 0.015005860477685928\n",
      "Steps : 194200, \t Total Gen Loss : 3593.86865234375, \t Total Dis Loss : 1.4344688679557294e-05\n",
      "Steps : 194300, \t Total Gen Loss : 2988.63623046875, \t Total Dis Loss : 7.60384227760369e-06\n",
      "Steps : 194400, \t Total Gen Loss : 3345.1494140625, \t Total Dis Loss : 1.6488280380144715e-05\n",
      "Steps : 194500, \t Total Gen Loss : 3400.60400390625, \t Total Dis Loss : 3.2175979868043214e-05\n",
      "Steps : 194600, \t Total Gen Loss : 3777.529052734375, \t Total Dis Loss : 6.199767085490748e-06\n",
      "Steps : 194700, \t Total Gen Loss : 2957.614013671875, \t Total Dis Loss : 4.920160790788941e-06\n",
      "Steps : 194800, \t Total Gen Loss : 3815.595458984375, \t Total Dis Loss : 9.442969712836202e-06\n",
      "Steps : 194900, \t Total Gen Loss : 3948.70751953125, \t Total Dis Loss : 1.2346738003543578e-05\n",
      "Steps : 195000, \t Total Gen Loss : 4072.633056640625, \t Total Dis Loss : 3.091185135417618e-05\n",
      "Steps : 195100, \t Total Gen Loss : 3150.621337890625, \t Total Dis Loss : 1.3049168046563864e-05\n",
      "Steps : 195200, \t Total Gen Loss : 3548.114501953125, \t Total Dis Loss : 0.0002140079450327903\n",
      "Steps : 195300, \t Total Gen Loss : 3466.375732421875, \t Total Dis Loss : 1.0940901120193303e-05\n",
      "Steps : 195400, \t Total Gen Loss : 3631.695068359375, \t Total Dis Loss : 2.345571920159273e-05\n",
      "Steps : 195500, \t Total Gen Loss : 3266.9716796875, \t Total Dis Loss : 1.0248171747662127e-05\n",
      "Steps : 195600, \t Total Gen Loss : 3809.663330078125, \t Total Dis Loss : 2.05319174710894e-05\n",
      "Steps : 195700, \t Total Gen Loss : 3707.627685546875, \t Total Dis Loss : 0.00012300230446271598\n",
      "Steps : 195800, \t Total Gen Loss : 3148.962646484375, \t Total Dis Loss : 9.677927300799638e-05\n",
      "Steps : 195900, \t Total Gen Loss : 4122.37939453125, \t Total Dis Loss : 0.0018257321789860725\n",
      "Steps : 196000, \t Total Gen Loss : 3480.5693359375, \t Total Dis Loss : 1.2761597645294387e-05\n",
      "Steps : 196100, \t Total Gen Loss : 3250.283935546875, \t Total Dis Loss : 1.952775528479833e-05\n",
      "Steps : 196200, \t Total Gen Loss : 3120.750732421875, \t Total Dis Loss : 1.4303500392998103e-05\n",
      "Steps : 196300, \t Total Gen Loss : 3779.366943359375, \t Total Dis Loss : 0.020919106900691986\n",
      "Steps : 196400, \t Total Gen Loss : 3518.577880859375, \t Total Dis Loss : 2.211717946920544e-05\n",
      "Steps : 196500, \t Total Gen Loss : 3471.0712890625, \t Total Dis Loss : 3.271833747930941e-06\n",
      "Steps : 196600, \t Total Gen Loss : 3831.98681640625, \t Total Dis Loss : 3.6977933177695377e-06\n",
      "Steps : 196700, \t Total Gen Loss : 4104.2861328125, \t Total Dis Loss : 2.482380750734592e-06\n",
      "Steps : 196800, \t Total Gen Loss : 3202.214111328125, \t Total Dis Loss : 7.444745733664604e-06\n",
      "Time for epoch 35 is 77.17725610733032 sec\n",
      "Steps : 196900, \t Total Gen Loss : 3111.330078125, \t Total Dis Loss : 1.1999345588264987e-05\n",
      "Steps : 197000, \t Total Gen Loss : 4214.02880859375, \t Total Dis Loss : 1.8266821371071273e-06\n",
      "Steps : 197100, \t Total Gen Loss : 3837.40087890625, \t Total Dis Loss : 2.346107976336498e-06\n",
      "Steps : 197200, \t Total Gen Loss : 3352.35693359375, \t Total Dis Loss : 2.6747600259113824e-06\n",
      "Steps : 197300, \t Total Gen Loss : 3772.51171875, \t Total Dis Loss : 1.1671896572806872e-05\n",
      "Steps : 197400, \t Total Gen Loss : 3560.751708984375, \t Total Dis Loss : 2.011130163737107e-05\n",
      "Steps : 197500, \t Total Gen Loss : 3241.163818359375, \t Total Dis Loss : 0.0002221730537712574\n",
      "Steps : 197600, \t Total Gen Loss : 3792.076416015625, \t Total Dis Loss : 0.00010753626702353358\n",
      "Steps : 197700, \t Total Gen Loss : 3623.998779296875, \t Total Dis Loss : 0.00010062849469250068\n",
      "Steps : 197800, \t Total Gen Loss : 4196.9580078125, \t Total Dis Loss : 2.379192847001832e-05\n",
      "Steps : 197900, \t Total Gen Loss : 3351.2587890625, \t Total Dis Loss : 0.008383406326174736\n",
      "Steps : 198000, \t Total Gen Loss : 3734.07568359375, \t Total Dis Loss : 2.5143834136542864e-05\n",
      "Steps : 198100, \t Total Gen Loss : 3537.092041015625, \t Total Dis Loss : 8.031998004298657e-05\n",
      "Steps : 198200, \t Total Gen Loss : 3346.682861328125, \t Total Dis Loss : 0.00013413459237199277\n",
      "Steps : 198300, \t Total Gen Loss : 3483.513916015625, \t Total Dis Loss : 4.090699439984746e-05\n",
      "Steps : 198400, \t Total Gen Loss : 3539.239501953125, \t Total Dis Loss : 0.0009057731949724257\n",
      "Steps : 198500, \t Total Gen Loss : 4241.14892578125, \t Total Dis Loss : 2.9053815524093807e-05\n",
      "Steps : 198600, \t Total Gen Loss : 3784.31884765625, \t Total Dis Loss : 9.771309123607352e-06\n",
      "Steps : 198700, \t Total Gen Loss : 3729.067626953125, \t Total Dis Loss : 1.1513689969433472e-05\n",
      "Steps : 198800, \t Total Gen Loss : 3634.81396484375, \t Total Dis Loss : 8.481207623844966e-05\n",
      "Steps : 198900, \t Total Gen Loss : 3881.02099609375, \t Total Dis Loss : 6.307156581897289e-05\n",
      "Steps : 199000, \t Total Gen Loss : 4140.28076171875, \t Total Dis Loss : 0.00045924284495413303\n",
      "Steps : 199100, \t Total Gen Loss : 4371.2548828125, \t Total Dis Loss : 8.892194273357745e-06\n",
      "Steps : 199200, \t Total Gen Loss : 3831.428466796875, \t Total Dis Loss : 1.5968000298016705e-05\n",
      "Steps : 199300, \t Total Gen Loss : 3877.959716796875, \t Total Dis Loss : 1.738385617500171e-05\n",
      "Steps : 199400, \t Total Gen Loss : 3557.55615234375, \t Total Dis Loss : 7.733001439191867e-06\n",
      "Steps : 199500, \t Total Gen Loss : 3895.386474609375, \t Total Dis Loss : 1.8413922589388676e-05\n",
      "Steps : 199600, \t Total Gen Loss : 3040.484619140625, \t Total Dis Loss : 1.0870280675590038e-05\n",
      "Steps : 199700, \t Total Gen Loss : 3376.616943359375, \t Total Dis Loss : 8.131989488902036e-06\n",
      "Steps : 199800, \t Total Gen Loss : 3049.43115234375, \t Total Dis Loss : 2.7570415568334283e-06\n",
      "Steps : 199900, \t Total Gen Loss : 3350.130859375, \t Total Dis Loss : 1.5443227312061936e-05\n",
      "Steps : 200000, \t Total Gen Loss : 3033.693603515625, \t Total Dis Loss : 0.0002491238701622933\n",
      "Steps : 200100, \t Total Gen Loss : 2989.541259765625, \t Total Dis Loss : 8.41742439661175e-06\n",
      "Steps : 200200, \t Total Gen Loss : 3963.310302734375, \t Total Dis Loss : 3.710980990945245e-06\n",
      "Steps : 200300, \t Total Gen Loss : 3474.099609375, \t Total Dis Loss : 1.2747464097628836e-05\n",
      "Steps : 200400, \t Total Gen Loss : 3535.4306640625, \t Total Dis Loss : 8.749897460802458e-06\n",
      "Steps : 200500, \t Total Gen Loss : 3826.098876953125, \t Total Dis Loss : 0.0001376573636662215\n",
      "Steps : 200600, \t Total Gen Loss : 2781.13134765625, \t Total Dis Loss : 1.4479226592811756e-05\n",
      "Steps : 200700, \t Total Gen Loss : 3594.994873046875, \t Total Dis Loss : 3.0954310204833746e-05\n",
      "Steps : 200800, \t Total Gen Loss : 3633.53173828125, \t Total Dis Loss : 0.00017031666357070208\n",
      "Steps : 200900, \t Total Gen Loss : 4005.030029296875, \t Total Dis Loss : 6.1175287555670366e-06\n",
      "Steps : 201000, \t Total Gen Loss : 3734.671630859375, \t Total Dis Loss : 5.815439180878457e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 201100, \t Total Gen Loss : 3467.287353515625, \t Total Dis Loss : 2.9022487524343887e-06\n",
      "Steps : 201200, \t Total Gen Loss : 3787.685546875, \t Total Dis Loss : 4.32375782111194e-06\n",
      "Steps : 201300, \t Total Gen Loss : 3432.0966796875, \t Total Dis Loss : 9.291828064306173e-06\n",
      "Steps : 201400, \t Total Gen Loss : 3666.40087890625, \t Total Dis Loss : 2.062108978861943e-05\n",
      "Steps : 201500, \t Total Gen Loss : 3487.703857421875, \t Total Dis Loss : 2.7013779799744952e-06\n",
      "Steps : 201600, \t Total Gen Loss : 3410.532470703125, \t Total Dis Loss : 9.548240996082313e-06\n",
      "Steps : 201700, \t Total Gen Loss : 3648.126953125, \t Total Dis Loss : 8.409690781263635e-05\n",
      "Steps : 201800, \t Total Gen Loss : 3278.281494140625, \t Total Dis Loss : 5.1228438678663224e-05\n",
      "Steps : 201900, \t Total Gen Loss : 3390.950927734375, \t Total Dis Loss : 1.0805812053149566e-05\n",
      "Steps : 202000, \t Total Gen Loss : 3710.83837890625, \t Total Dis Loss : 4.692951279139379e-06\n",
      "Steps : 202100, \t Total Gen Loss : 3693.19677734375, \t Total Dis Loss : 4.32195229222998e-05\n",
      "Steps : 202200, \t Total Gen Loss : 3470.963623046875, \t Total Dis Loss : 2.3984106519492343e-05\n",
      "Steps : 202300, \t Total Gen Loss : 3557.64306640625, \t Total Dis Loss : 5.3543676585832145e-06\n",
      "Steps : 202400, \t Total Gen Loss : 3564.367919921875, \t Total Dis Loss : 9.300256351707503e-06\n",
      "Steps : 202500, \t Total Gen Loss : 3257.537353515625, \t Total Dis Loss : 1.6356498235836625e-06\n",
      "Time for epoch 36 is 75.18584203720093 sec\n",
      "Steps : 202600, \t Total Gen Loss : 3371.997314453125, \t Total Dis Loss : 0.00012117598089389503\n",
      "Steps : 202700, \t Total Gen Loss : 2895.1650390625, \t Total Dis Loss : 2.3946602595970035e-05\n",
      "Steps : 202800, \t Total Gen Loss : 4015.398681640625, \t Total Dis Loss : 8.214153785957024e-05\n",
      "Steps : 202900, \t Total Gen Loss : 3585.63623046875, \t Total Dis Loss : 2.4645878511364572e-05\n",
      "Steps : 203000, \t Total Gen Loss : 3792.2509765625, \t Total Dis Loss : 0.0001892598083941266\n",
      "Steps : 203100, \t Total Gen Loss : 3613.828857421875, \t Total Dis Loss : 1.951769445440732e-05\n",
      "Steps : 203200, \t Total Gen Loss : 3717.64501953125, \t Total Dis Loss : 7.368980004685e-05\n",
      "Steps : 203300, \t Total Gen Loss : 3579.438232421875, \t Total Dis Loss : 1.5524921764153987e-05\n",
      "Steps : 203400, \t Total Gen Loss : 3308.486572265625, \t Total Dis Loss : 4.375089338282123e-05\n",
      "Steps : 203500, \t Total Gen Loss : 4160.34423828125, \t Total Dis Loss : 2.0309773390181363e-05\n",
      "Steps : 203600, \t Total Gen Loss : 3349.22119140625, \t Total Dis Loss : 0.0006629094132222235\n",
      "Steps : 203700, \t Total Gen Loss : 3887.510986328125, \t Total Dis Loss : 4.703901595348725e-06\n",
      "Steps : 203800, \t Total Gen Loss : 3823.9150390625, \t Total Dis Loss : 9.278200013795868e-05\n",
      "Steps : 203900, \t Total Gen Loss : 3362.099609375, \t Total Dis Loss : 1.566789433127269e-05\n",
      "Steps : 204000, \t Total Gen Loss : 3196.4443359375, \t Total Dis Loss : 7.429258403135464e-06\n",
      "Steps : 204100, \t Total Gen Loss : 3485.333984375, \t Total Dis Loss : 2.795818363665603e-05\n",
      "Steps : 204200, \t Total Gen Loss : 3272.318359375, \t Total Dis Loss : 2.3496362700825557e-05\n",
      "Steps : 204300, \t Total Gen Loss : 3401.63623046875, \t Total Dis Loss : 4.498838825384155e-05\n",
      "Steps : 204400, \t Total Gen Loss : 3555.380126953125, \t Total Dis Loss : 7.512187585234642e-05\n",
      "Steps : 204500, \t Total Gen Loss : 3195.61767578125, \t Total Dis Loss : 4.859379259869456e-05\n",
      "Steps : 204600, \t Total Gen Loss : 3713.264892578125, \t Total Dis Loss : 2.0198354832245968e-05\n",
      "Steps : 204700, \t Total Gen Loss : 4268.40771484375, \t Total Dis Loss : 7.293195812962949e-05\n",
      "Steps : 204800, \t Total Gen Loss : 3467.2392578125, \t Total Dis Loss : 2.0997433239244856e-05\n",
      "Steps : 204900, \t Total Gen Loss : 3241.2314453125, \t Total Dis Loss : 1.9392631656955928e-05\n",
      "Steps : 205000, \t Total Gen Loss : 3570.487060546875, \t Total Dis Loss : 3.973453203798272e-05\n",
      "Steps : 205100, \t Total Gen Loss : 3465.396484375, \t Total Dis Loss : 1.4001359886606224e-05\n",
      "Steps : 205200, \t Total Gen Loss : 3606.800048828125, \t Total Dis Loss : 1.262401565327309e-05\n",
      "Steps : 205300, \t Total Gen Loss : 3196.0751953125, \t Total Dis Loss : 0.00028280107653699815\n",
      "Steps : 205400, \t Total Gen Loss : 3487.76513671875, \t Total Dis Loss : 2.5938086764654145e-05\n",
      "Steps : 205500, \t Total Gen Loss : 4212.67822265625, \t Total Dis Loss : 3.2829513656906784e-05\n",
      "Steps : 205600, \t Total Gen Loss : 3893.47705078125, \t Total Dis Loss : 2.8406779165379703e-05\n",
      "Steps : 205700, \t Total Gen Loss : 3921.172119140625, \t Total Dis Loss : 2.2501468265545554e-05\n",
      "Steps : 205800, \t Total Gen Loss : 4040.236083984375, \t Total Dis Loss : 3.505555014271522e-06\n",
      "Steps : 205900, \t Total Gen Loss : 3576.763671875, \t Total Dis Loss : 2.4633023713249713e-05\n",
      "Steps : 206000, \t Total Gen Loss : 3518.202392578125, \t Total Dis Loss : 7.058717164909467e-05\n",
      "Steps : 206100, \t Total Gen Loss : 3549.833251953125, \t Total Dis Loss : 2.8558977646753192e-05\n",
      "Steps : 206200, \t Total Gen Loss : 3375.116455078125, \t Total Dis Loss : 0.00015238591004163027\n",
      "Steps : 206300, \t Total Gen Loss : 3588.809814453125, \t Total Dis Loss : 2.0334648070274852e-05\n",
      "Steps : 206400, \t Total Gen Loss : 3254.149658203125, \t Total Dis Loss : 5.652344771078788e-05\n",
      "Steps : 206500, \t Total Gen Loss : 3128.540771484375, \t Total Dis Loss : 2.85284586425405e-05\n",
      "Steps : 206600, \t Total Gen Loss : 3496.403076171875, \t Total Dis Loss : 2.178360227844678e-05\n",
      "Steps : 206700, \t Total Gen Loss : 3899.64208984375, \t Total Dis Loss : 1.0365884918428492e-05\n",
      "Steps : 206800, \t Total Gen Loss : 3196.819091796875, \t Total Dis Loss : 7.245618235174334e-06\n",
      "Steps : 206900, \t Total Gen Loss : 4362.50927734375, \t Total Dis Loss : 3.930892944481457e-06\n",
      "Steps : 207000, \t Total Gen Loss : 3937.263671875, \t Total Dis Loss : 2.5742410798557103e-06\n",
      "Steps : 207100, \t Total Gen Loss : 3085.3046875, \t Total Dis Loss : 2.8886759537272155e-05\n",
      "Steps : 207200, \t Total Gen Loss : 3598.375244140625, \t Total Dis Loss : 1.7647038475843146e-05\n",
      "Steps : 207300, \t Total Gen Loss : 3537.602783203125, \t Total Dis Loss : 1.3339915312826633e-05\n",
      "Steps : 207400, \t Total Gen Loss : 2986.0244140625, \t Total Dis Loss : 0.0001318986323894933\n",
      "Steps : 207500, \t Total Gen Loss : 3687.30517578125, \t Total Dis Loss : 6.080514140194282e-05\n",
      "Steps : 207600, \t Total Gen Loss : 3758.041748046875, \t Total Dis Loss : 1.985838207474444e-05\n",
      "Steps : 207700, \t Total Gen Loss : 3923.4169921875, \t Total Dis Loss : 0.0004493059532251209\n",
      "Steps : 207800, \t Total Gen Loss : 3918.111572265625, \t Total Dis Loss : 2.5329922209493816e-05\n",
      "Steps : 207900, \t Total Gen Loss : 3651.62841796875, \t Total Dis Loss : 3.265283339715097e-06\n",
      "Steps : 208000, \t Total Gen Loss : 3843.15625, \t Total Dis Loss : 1.2287843674130272e-05\n",
      "Steps : 208100, \t Total Gen Loss : 3883.095703125, \t Total Dis Loss : 4.981875918019796e-06\n",
      "Time for epoch 37 is 76.12619304656982 sec\n",
      "Steps : 208200, \t Total Gen Loss : 3324.713134765625, \t Total Dis Loss : 0.0003188072005286813\n",
      "Steps : 208300, \t Total Gen Loss : 3383.51025390625, \t Total Dis Loss : 0.00012881237489636987\n",
      "Steps : 208400, \t Total Gen Loss : 3626.614013671875, \t Total Dis Loss : 0.00010229357576463372\n",
      "Steps : 208500, \t Total Gen Loss : 3234.3525390625, \t Total Dis Loss : 6.4337768890254665e-06\n",
      "Steps : 208600, \t Total Gen Loss : 3715.193603515625, \t Total Dis Loss : 1.7858430510386825e-05\n",
      "Steps : 208700, \t Total Gen Loss : 3773.3671875, \t Total Dis Loss : 3.0219785912777297e-05\n",
      "Steps : 208800, \t Total Gen Loss : 3340.5498046875, \t Total Dis Loss : 2.8052872949047014e-05\n",
      "Steps : 208900, \t Total Gen Loss : 3566.9287109375, \t Total Dis Loss : 1.5913014976831619e-06\n",
      "Steps : 209000, \t Total Gen Loss : 3831.860107421875, \t Total Dis Loss : 2.318736960660317e-06\n",
      "Steps : 209100, \t Total Gen Loss : 3640.56005859375, \t Total Dis Loss : 1.2492144378484227e-05\n",
      "Steps : 209200, \t Total Gen Loss : 3839.419677734375, \t Total Dis Loss : 4.6460909288725816e-06\n",
      "Steps : 209300, \t Total Gen Loss : 2772.963623046875, \t Total Dis Loss : 1.1604831342992838e-05\n",
      "Steps : 209400, \t Total Gen Loss : 4114.6591796875, \t Total Dis Loss : 8.99315818969626e-06\n",
      "Steps : 209500, \t Total Gen Loss : 3421.796630859375, \t Total Dis Loss : 1.80110837391112e-05\n",
      "Steps : 209600, \t Total Gen Loss : 3824.458984375, \t Total Dis Loss : 1.574882844579406e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 209700, \t Total Gen Loss : 3519.62255859375, \t Total Dis Loss : 1.5054811228765175e-05\n",
      "Steps : 209800, \t Total Gen Loss : 3701.947265625, \t Total Dis Loss : 0.0004370852839201689\n",
      "Steps : 209900, \t Total Gen Loss : 3434.41259765625, \t Total Dis Loss : 6.957957521080971e-05\n",
      "Steps : 210000, \t Total Gen Loss : 3758.254638671875, \t Total Dis Loss : 0.00011240125604672357\n",
      "Steps : 210100, \t Total Gen Loss : 3458.1142578125, \t Total Dis Loss : 1.307804268435575e-05\n",
      "Steps : 210200, \t Total Gen Loss : 3712.73779296875, \t Total Dis Loss : 7.638645001861732e-06\n",
      "Steps : 210300, \t Total Gen Loss : 3414.489990234375, \t Total Dis Loss : 1.4448178262682632e-05\n",
      "Steps : 210400, \t Total Gen Loss : 3429.283935546875, \t Total Dis Loss : 0.0016629198798909783\n",
      "Steps : 210500, \t Total Gen Loss : 3295.732666015625, \t Total Dis Loss : 6.42333397991024e-05\n",
      "Steps : 210600, \t Total Gen Loss : 3602.57080078125, \t Total Dis Loss : 7.881665078457445e-05\n",
      "Steps : 210700, \t Total Gen Loss : 3911.456298828125, \t Total Dis Loss : 7.556706987088546e-05\n",
      "Steps : 210800, \t Total Gen Loss : 3652.09716796875, \t Total Dis Loss : 1.5334884665207937e-05\n",
      "Steps : 210900, \t Total Gen Loss : 3153.789794921875, \t Total Dis Loss : 0.00022701016860082746\n",
      "Steps : 211000, \t Total Gen Loss : 3344.37939453125, \t Total Dis Loss : 2.6062158212880604e-05\n",
      "Steps : 211100, \t Total Gen Loss : 3544.455810546875, \t Total Dis Loss : 0.0005218666628934443\n",
      "Steps : 211200, \t Total Gen Loss : 3156.048828125, \t Total Dis Loss : 2.2828118744655512e-05\n",
      "Steps : 211300, \t Total Gen Loss : 3611.970703125, \t Total Dis Loss : 2.927174500655383e-05\n",
      "Steps : 211400, \t Total Gen Loss : 3523.67822265625, \t Total Dis Loss : 6.277466127357911e-06\n",
      "Steps : 211500, \t Total Gen Loss : 3237.02490234375, \t Total Dis Loss : 7.062985787342768e-06\n",
      "Steps : 211600, \t Total Gen Loss : 3526.19189453125, \t Total Dis Loss : 1.0816826943482738e-05\n",
      "Steps : 211700, \t Total Gen Loss : 3466.24169921875, \t Total Dis Loss : 1.4356626707012765e-05\n",
      "Steps : 211800, \t Total Gen Loss : 3574.260009765625, \t Total Dis Loss : 1.0105058208864648e-05\n",
      "Steps : 211900, \t Total Gen Loss : 4350.611328125, \t Total Dis Loss : 5.891010914638173e-06\n",
      "Steps : 212000, \t Total Gen Loss : 3707.19384765625, \t Total Dis Loss : 4.129025910515338e-05\n",
      "Steps : 212100, \t Total Gen Loss : 4093.56640625, \t Total Dis Loss : 8.249002348748036e-06\n",
      "Steps : 212200, \t Total Gen Loss : 3738.786376953125, \t Total Dis Loss : 0.00027752466849051416\n",
      "Steps : 212300, \t Total Gen Loss : 3283.77490234375, \t Total Dis Loss : 3.825176463578828e-05\n",
      "Steps : 212400, \t Total Gen Loss : 3013.48779296875, \t Total Dis Loss : 6.170786946313456e-05\n",
      "Steps : 212500, \t Total Gen Loss : 3284.9345703125, \t Total Dis Loss : 1.44883288157871e-05\n",
      "Steps : 212600, \t Total Gen Loss : 3441.048095703125, \t Total Dis Loss : 9.743036571308039e-06\n",
      "Steps : 212700, \t Total Gen Loss : 4017.667724609375, \t Total Dis Loss : 1.1450710189819802e-05\n",
      "Steps : 212800, \t Total Gen Loss : 3697.32177734375, \t Total Dis Loss : 2.9632740279339487e-06\n",
      "Steps : 212900, \t Total Gen Loss : 3220.441162109375, \t Total Dis Loss : 7.446608833561186e-06\n",
      "Steps : 213000, \t Total Gen Loss : 3953.76123046875, \t Total Dis Loss : 0.0001602783886482939\n",
      "Steps : 213100, \t Total Gen Loss : 4237.22998046875, \t Total Dis Loss : 3.95394854422193e-05\n",
      "Steps : 213200, \t Total Gen Loss : 3336.820556640625, \t Total Dis Loss : 2.3151733330450952e-05\n",
      "Steps : 213300, \t Total Gen Loss : 2906.72802734375, \t Total Dis Loss : 3.1559033232042566e-05\n",
      "Steps : 213400, \t Total Gen Loss : 3217.674560546875, \t Total Dis Loss : 4.1817733290372416e-05\n",
      "Steps : 213500, \t Total Gen Loss : 3779.2265625, \t Total Dis Loss : 1.7180365830427036e-05\n",
      "Steps : 213600, \t Total Gen Loss : 3576.341552734375, \t Total Dis Loss : 5.861888894287404e-06\n",
      "Steps : 213700, \t Total Gen Loss : 3903.845703125, \t Total Dis Loss : 1.4371235010912642e-05\n",
      "Time for epoch 38 is 76.95050430297852 sec\n",
      "Steps : 213800, \t Total Gen Loss : 3793.050537109375, \t Total Dis Loss : 9.094690540223382e-06\n",
      "Steps : 213900, \t Total Gen Loss : 3435.972900390625, \t Total Dis Loss : 2.0811216018046252e-05\n",
      "Steps : 214000, \t Total Gen Loss : 3465.24560546875, \t Total Dis Loss : 1.5272216842276976e-05\n",
      "Steps : 214100, \t Total Gen Loss : 3699.7861328125, \t Total Dis Loss : 6.219575152499601e-05\n",
      "Steps : 214200, \t Total Gen Loss : 3543.89697265625, \t Total Dis Loss : 2.7100533770862967e-05\n",
      "Steps : 214300, \t Total Gen Loss : 3998.797119140625, \t Total Dis Loss : 5.020409662392922e-05\n",
      "Steps : 214400, \t Total Gen Loss : 3252.810546875, \t Total Dis Loss : 1.7681768440525047e-05\n",
      "Steps : 214500, \t Total Gen Loss : 3519.95166015625, \t Total Dis Loss : 2.7785446945927106e-05\n",
      "Steps : 214600, \t Total Gen Loss : 3850.256591796875, \t Total Dis Loss : 8.82571839611046e-06\n",
      "Steps : 214700, \t Total Gen Loss : 4343.24462890625, \t Total Dis Loss : 1.4528384781442583e-05\n",
      "Steps : 214800, \t Total Gen Loss : 4040.7646484375, \t Total Dis Loss : 0.15409712493419647\n",
      "Steps : 214900, \t Total Gen Loss : 3496.734619140625, \t Total Dis Loss : 5.7290402764920145e-05\n",
      "Steps : 215000, \t Total Gen Loss : 3716.808349609375, \t Total Dis Loss : 8.89636703504948e-06\n",
      "Steps : 215100, \t Total Gen Loss : 3578.79541015625, \t Total Dis Loss : 2.5077473765122704e-05\n",
      "Steps : 215200, \t Total Gen Loss : 3803.09326171875, \t Total Dis Loss : 2.9364712190727005e-06\n",
      "Steps : 215300, \t Total Gen Loss : 3144.1513671875, \t Total Dis Loss : 1.559122210892383e-05\n",
      "Steps : 215400, \t Total Gen Loss : 3749.677001953125, \t Total Dis Loss : 4.470296062208945e-06\n",
      "Steps : 215500, \t Total Gen Loss : 3027.5087890625, \t Total Dis Loss : 5.508692993316799e-06\n",
      "Steps : 215600, \t Total Gen Loss : 3239.425537109375, \t Total Dis Loss : 2.2147316940390738e-06\n",
      "Steps : 215700, \t Total Gen Loss : 3125.31884765625, \t Total Dis Loss : 1.2677852282649837e-05\n",
      "Steps : 215800, \t Total Gen Loss : 3311.468017578125, \t Total Dis Loss : 2.752971340669319e-06\n",
      "Steps : 215900, \t Total Gen Loss : 3682.24462890625, \t Total Dis Loss : 3.496886620268924e-06\n",
      "Steps : 216000, \t Total Gen Loss : 3814.0322265625, \t Total Dis Loss : 9.630615750211291e-06\n",
      "Steps : 216100, \t Total Gen Loss : 3147.00537109375, \t Total Dis Loss : 2.8926946470164694e-05\n",
      "Steps : 216200, \t Total Gen Loss : 3193.274169921875, \t Total Dis Loss : 8.101434104901273e-06\n",
      "Steps : 216300, \t Total Gen Loss : 3874.197509765625, \t Total Dis Loss : 2.442726690787822e-05\n",
      "Steps : 216400, \t Total Gen Loss : 2915.24169921875, \t Total Dis Loss : 1.6970507203950547e-06\n",
      "Steps : 216500, \t Total Gen Loss : 2725.3828125, \t Total Dis Loss : 2.0093302737222984e-05\n",
      "Steps : 216600, \t Total Gen Loss : 3930.845947265625, \t Total Dis Loss : 4.03858666686574e-06\n",
      "Steps : 216700, \t Total Gen Loss : 3508.827880859375, \t Total Dis Loss : 2.2170590455061756e-05\n",
      "Steps : 216800, \t Total Gen Loss : 3244.64306640625, \t Total Dis Loss : 0.0036507241893559694\n",
      "Steps : 216900, \t Total Gen Loss : 3364.53857421875, \t Total Dis Loss : 1.1524905858095735e-05\n",
      "Steps : 217000, \t Total Gen Loss : 3459.956298828125, \t Total Dis Loss : 1.4794036360399332e-05\n",
      "Steps : 217100, \t Total Gen Loss : 3431.477294921875, \t Total Dis Loss : 1.0784541700559203e-05\n",
      "Steps : 217200, \t Total Gen Loss : 3493.542724609375, \t Total Dis Loss : 8.33306785352761e-06\n",
      "Steps : 217300, \t Total Gen Loss : 3594.69140625, \t Total Dis Loss : 2.1329751689336263e-05\n",
      "Steps : 217400, \t Total Gen Loss : 4019.250732421875, \t Total Dis Loss : 8.253487067122478e-06\n",
      "Steps : 217500, \t Total Gen Loss : 3246.742431640625, \t Total Dis Loss : 1.31787683130824e-05\n",
      "Steps : 217600, \t Total Gen Loss : 3025.481201171875, \t Total Dis Loss : 4.5405549826682545e-06\n",
      "Steps : 217700, \t Total Gen Loss : 4179.13134765625, \t Total Dis Loss : 0.0001697361294645816\n",
      "Steps : 217800, \t Total Gen Loss : 3999.955322265625, \t Total Dis Loss : 4.358829028205946e-05\n",
      "Steps : 217900, \t Total Gen Loss : 3638.685546875, \t Total Dis Loss : 1.2334779057709966e-05\n",
      "Steps : 218000, \t Total Gen Loss : 3683.753173828125, \t Total Dis Loss : 1.9094181880063843e-06\n",
      "Steps : 218100, \t Total Gen Loss : 3932.105712890625, \t Total Dis Loss : 2.665679858182557e-06\n",
      "Steps : 218200, \t Total Gen Loss : 3572.877685546875, \t Total Dis Loss : 4.806694050785154e-06\n",
      "Steps : 218300, \t Total Gen Loss : 3461.781005859375, \t Total Dis Loss : 3.383184230187908e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 218400, \t Total Gen Loss : 3252.49462890625, \t Total Dis Loss : 0.00020789069822058082\n",
      "Steps : 218500, \t Total Gen Loss : 3650.498046875, \t Total Dis Loss : 1.8527927750255913e-05\n",
      "Steps : 218600, \t Total Gen Loss : 3374.2412109375, \t Total Dis Loss : 1.0441282256579143e-06\n",
      "Steps : 218700, \t Total Gen Loss : 3466.219970703125, \t Total Dis Loss : 7.856065167288762e-07\n",
      "Steps : 218800, \t Total Gen Loss : 3898.0908203125, \t Total Dis Loss : 1.8139451640308835e-05\n",
      "Steps : 218900, \t Total Gen Loss : 4229.90087890625, \t Total Dis Loss : 2.9053891921648756e-05\n",
      "Steps : 219000, \t Total Gen Loss : 3337.153564453125, \t Total Dis Loss : 0.00015809957403689623\n",
      "Steps : 219100, \t Total Gen Loss : 3153.838623046875, \t Total Dis Loss : 6.341912012430839e-06\n",
      "Steps : 219200, \t Total Gen Loss : 3478.11962890625, \t Total Dis Loss : 1.2379172403598204e-05\n",
      "Steps : 219300, \t Total Gen Loss : 3796.191650390625, \t Total Dis Loss : 8.77958518685773e-06\n",
      "Time for epoch 39 is 76.7157928943634 sec\n",
      "Steps : 219400, \t Total Gen Loss : 3862.81689453125, \t Total Dis Loss : 2.8355907488730736e-06\n",
      "Steps : 219500, \t Total Gen Loss : 3635.78271484375, \t Total Dis Loss : 1.297484413953498e-05\n",
      "Steps : 219600, \t Total Gen Loss : 3161.953125, \t Total Dis Loss : 2.9105867724865675e-06\n",
      "Steps : 219700, \t Total Gen Loss : 3745.967529296875, \t Total Dis Loss : 1.0220221611234592e-06\n",
      "Steps : 219800, \t Total Gen Loss : 4272.40185546875, \t Total Dis Loss : 6.9394081947393715e-06\n",
      "Steps : 219900, \t Total Gen Loss : 3557.096435546875, \t Total Dis Loss : 8.859191439114511e-06\n",
      "Steps : 220000, \t Total Gen Loss : 3879.625, \t Total Dis Loss : 1.5213772712741047e-05\n",
      "Steps : 220100, \t Total Gen Loss : 4038.36181640625, \t Total Dis Loss : 2.2427188014262356e-05\n",
      "Steps : 220200, \t Total Gen Loss : 3645.8359375, \t Total Dis Loss : 7.745327820884995e-06\n",
      "Steps : 220300, \t Total Gen Loss : 4215.94140625, \t Total Dis Loss : 4.354749762569554e-06\n",
      "Steps : 220400, \t Total Gen Loss : 3322.1318359375, \t Total Dis Loss : 3.4688405321503524e-06\n",
      "Steps : 220500, \t Total Gen Loss : 3398.324951171875, \t Total Dis Loss : 1.5137739865167532e-05\n",
      "Steps : 220600, \t Total Gen Loss : 3288.201904296875, \t Total Dis Loss : 7.919388735899702e-06\n",
      "Steps : 220700, \t Total Gen Loss : 3647.262451171875, \t Total Dis Loss : 2.5965571694541723e-05\n",
      "Steps : 220800, \t Total Gen Loss : 3332.2705078125, \t Total Dis Loss : 1.4713545169797726e-05\n",
      "Steps : 220900, \t Total Gen Loss : 3511.3251953125, \t Total Dis Loss : 2.2031559637980536e-05\n",
      "Steps : 221000, \t Total Gen Loss : 4020.63330078125, \t Total Dis Loss : 2.2338231246976648e-06\n",
      "Steps : 221100, \t Total Gen Loss : 3444.17236328125, \t Total Dis Loss : 2.922588919318514e-06\n",
      "Steps : 221200, \t Total Gen Loss : 2852.482177734375, \t Total Dis Loss : 1.9534886632754933e-06\n",
      "Steps : 221300, \t Total Gen Loss : 3226.116455078125, \t Total Dis Loss : 0.00010650791955413297\n",
      "Steps : 221400, \t Total Gen Loss : 4030.047119140625, \t Total Dis Loss : 1.5078973774507176e-05\n",
      "Steps : 221500, \t Total Gen Loss : 3939.716796875, \t Total Dis Loss : 4.740585063700564e-05\n",
      "Steps : 221600, \t Total Gen Loss : 3798.94580078125, \t Total Dis Loss : 1.3873424222765607e-06\n",
      "Steps : 221700, \t Total Gen Loss : 3603.6123046875, \t Total Dis Loss : 6.0590073189814575e-06\n",
      "Steps : 221800, \t Total Gen Loss : 3517.848876953125, \t Total Dis Loss : 1.6697678802302107e-05\n",
      "Steps : 221900, \t Total Gen Loss : 3283.412841796875, \t Total Dis Loss : 0.00017792682047002017\n",
      "Steps : 222000, \t Total Gen Loss : 3814.587646484375, \t Total Dis Loss : 3.952303814003244e-06\n",
      "Steps : 222100, \t Total Gen Loss : 3754.1201171875, \t Total Dis Loss : 5.935517947364133e-06\n",
      "Steps : 222200, \t Total Gen Loss : 2832.55126953125, \t Total Dis Loss : 4.907313268631697e-06\n",
      "Steps : 222300, \t Total Gen Loss : 3916.30126953125, \t Total Dis Loss : 2.1681273210560903e-05\n",
      "Steps : 222400, \t Total Gen Loss : 3891.41796875, \t Total Dis Loss : 6.254437903407961e-05\n",
      "Steps : 222500, \t Total Gen Loss : 3754.226806640625, \t Total Dis Loss : 1.2883883755421266e-06\n",
      "Steps : 222600, \t Total Gen Loss : 3144.32763671875, \t Total Dis Loss : 5.0240610107721295e-06\n",
      "Steps : 222700, \t Total Gen Loss : 3883.230224609375, \t Total Dis Loss : 9.515699275652878e-06\n",
      "Steps : 222800, \t Total Gen Loss : 3239.877685546875, \t Total Dis Loss : 2.2094271116657183e-05\n",
      "Steps : 222900, \t Total Gen Loss : 3709.02392578125, \t Total Dis Loss : 1.3728962585446425e-05\n",
      "Steps : 223000, \t Total Gen Loss : 3877.118896484375, \t Total Dis Loss : 2.599149775051046e-05\n",
      "Steps : 223100, \t Total Gen Loss : 3760.180908203125, \t Total Dis Loss : 7.994503903319128e-06\n",
      "Steps : 223200, \t Total Gen Loss : 3507.52978515625, \t Total Dis Loss : 1.9997449271613732e-05\n",
      "Steps : 223300, \t Total Gen Loss : 3538.16455078125, \t Total Dis Loss : 4.64790082332911e-06\n",
      "Steps : 223400, \t Total Gen Loss : 3497.68115234375, \t Total Dis Loss : 5.080834398540901e-06\n",
      "Steps : 223500, \t Total Gen Loss : 3271.690673828125, \t Total Dis Loss : 1.7115569789893925e-05\n",
      "Steps : 223600, \t Total Gen Loss : 3508.99560546875, \t Total Dis Loss : 1.9275610611657612e-05\n",
      "Steps : 223700, \t Total Gen Loss : 3755.544189453125, \t Total Dis Loss : 4.027699105790816e-06\n",
      "Steps : 223800, \t Total Gen Loss : 2952.6064453125, \t Total Dis Loss : 1.4640301742474549e-05\n",
      "Steps : 223900, \t Total Gen Loss : 3998.936279296875, \t Total Dis Loss : 1.341174265689915e-05\n",
      "Steps : 224000, \t Total Gen Loss : 3994.362060546875, \t Total Dis Loss : 0.0009071836830116808\n",
      "Steps : 224100, \t Total Gen Loss : 3807.0654296875, \t Total Dis Loss : 1.4914960047462955e-05\n",
      "Steps : 224200, \t Total Gen Loss : 3783.8974609375, \t Total Dis Loss : 3.224411921110004e-05\n",
      "Steps : 224300, \t Total Gen Loss : 3677.19384765625, \t Total Dis Loss : 8.581856673117727e-05\n",
      "Steps : 224400, \t Total Gen Loss : 3225.2841796875, \t Total Dis Loss : 7.901933713583276e-06\n",
      "Steps : 224500, \t Total Gen Loss : 3566.52197265625, \t Total Dis Loss : 2.6366407837485895e-05\n",
      "Steps : 224600, \t Total Gen Loss : 3734.65771484375, \t Total Dis Loss : 6.994572231633356e-06\n",
      "Steps : 224700, \t Total Gen Loss : 3300.183349609375, \t Total Dis Loss : 1.5295521734515205e-05\n",
      "Steps : 224800, \t Total Gen Loss : 3737.78515625, \t Total Dis Loss : 1.8454767996445298e-05\n",
      "Steps : 224900, \t Total Gen Loss : 3553.31787109375, \t Total Dis Loss : 0.0001446767128072679\n",
      "Steps : 225000, \t Total Gen Loss : 3739.910888671875, \t Total Dis Loss : 5.1685046855709516e-06\n",
      "Time for epoch 40 is 77.16595602035522 sec\n",
      "Steps : 225100, \t Total Gen Loss : 3861.12890625, \t Total Dis Loss : 3.337034286232665e-05\n",
      "Steps : 225200, \t Total Gen Loss : 3451.846435546875, \t Total Dis Loss : 5.670862447004765e-05\n",
      "Steps : 225300, \t Total Gen Loss : 3636.5576171875, \t Total Dis Loss : 2.367651177337393e-06\n",
      "Steps : 225400, \t Total Gen Loss : 3348.5595703125, \t Total Dis Loss : 5.668459380103741e-06\n",
      "Steps : 225500, \t Total Gen Loss : 4223.82373046875, \t Total Dis Loss : 4.634176002582535e-05\n",
      "Steps : 225600, \t Total Gen Loss : 3771.854248046875, \t Total Dis Loss : 3.072999243158847e-05\n",
      "Steps : 225700, \t Total Gen Loss : 4224.1708984375, \t Total Dis Loss : 4.202255695417989e-06\n",
      "Steps : 225800, \t Total Gen Loss : 3474.526611328125, \t Total Dis Loss : 2.3190223146229982e-05\n",
      "Steps : 225900, \t Total Gen Loss : 3106.104736328125, \t Total Dis Loss : 1.4102528439252637e-05\n",
      "Steps : 226000, \t Total Gen Loss : 3860.400390625, \t Total Dis Loss : 1.7132810171460733e-05\n",
      "Steps : 226100, \t Total Gen Loss : 3804.552490234375, \t Total Dis Loss : 3.6510048175841803e-06\n",
      "Steps : 226200, \t Total Gen Loss : 3174.41162109375, \t Total Dis Loss : 1.980410706892144e-05\n",
      "Steps : 226300, \t Total Gen Loss : 3751.900390625, \t Total Dis Loss : 1.5311892639147118e-05\n",
      "Steps : 226400, \t Total Gen Loss : 3422.902587890625, \t Total Dis Loss : 1.1070051186834462e-05\n",
      "Steps : 226500, \t Total Gen Loss : 3757.473876953125, \t Total Dis Loss : 1.2178013093944173e-05\n",
      "Steps : 226600, \t Total Gen Loss : 3585.57080078125, \t Total Dis Loss : 0.6232786178588867\n",
      "Steps : 226700, \t Total Gen Loss : 3410.326416015625, \t Total Dis Loss : 9.081280950340442e-06\n",
      "Steps : 226800, \t Total Gen Loss : 3601.555419921875, \t Total Dis Loss : 7.696721877437085e-05\n",
      "Steps : 226900, \t Total Gen Loss : 4071.311767578125, \t Total Dis Loss : 3.929856393369846e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 227000, \t Total Gen Loss : 4158.98779296875, \t Total Dis Loss : 7.691053178859875e-05\n",
      "Steps : 227100, \t Total Gen Loss : 3849.24658203125, \t Total Dis Loss : 4.265781171852723e-05\n",
      "Steps : 227200, \t Total Gen Loss : 3603.862060546875, \t Total Dis Loss : 3.3654854632914066e-05\n",
      "Steps : 227300, \t Total Gen Loss : 3910.339111328125, \t Total Dis Loss : 7.951910447445698e-06\n",
      "Steps : 227400, \t Total Gen Loss : 3838.962890625, \t Total Dis Loss : 3.7942922972433735e-06\n",
      "Steps : 227500, \t Total Gen Loss : 3637.28662109375, \t Total Dis Loss : 9.379161383549217e-06\n",
      "Steps : 227600, \t Total Gen Loss : 3148.45947265625, \t Total Dis Loss : 0.003234418574720621\n",
      "Steps : 227700, \t Total Gen Loss : 3650.1748046875, \t Total Dis Loss : 7.269986326718936e-06\n",
      "Steps : 227800, \t Total Gen Loss : 3201.90576171875, \t Total Dis Loss : 1.880599847936537e-05\n",
      "Steps : 227900, \t Total Gen Loss : 4334.58642578125, \t Total Dis Loss : 2.6700610760599375e-05\n",
      "Steps : 228000, \t Total Gen Loss : 3302.79736328125, \t Total Dis Loss : 1.4481611287919804e-05\n",
      "Steps : 228100, \t Total Gen Loss : 3344.01025390625, \t Total Dis Loss : 5.021284596296027e-05\n",
      "Steps : 228200, \t Total Gen Loss : 3628.453857421875, \t Total Dis Loss : 2.5911549528245814e-06\n",
      "Steps : 228300, \t Total Gen Loss : 3752.082763671875, \t Total Dis Loss : 1.0155118616239633e-05\n",
      "Steps : 228400, \t Total Gen Loss : 4341.6005859375, \t Total Dis Loss : 9.459382454224396e-06\n",
      "Steps : 228500, \t Total Gen Loss : 3633.259765625, \t Total Dis Loss : 3.042841626665904e-06\n",
      "Steps : 228600, \t Total Gen Loss : 3152.219970703125, \t Total Dis Loss : 3.6789801924896892e-06\n",
      "Steps : 228700, \t Total Gen Loss : 3527.074951171875, \t Total Dis Loss : 2.120600129273953e-06\n",
      "Steps : 228800, \t Total Gen Loss : 3188.022216796875, \t Total Dis Loss : 2.2262276615947485e-05\n",
      "Steps : 228900, \t Total Gen Loss : 3611.108154296875, \t Total Dis Loss : 5.392601451603696e-06\n",
      "Steps : 229000, \t Total Gen Loss : 3873.18994140625, \t Total Dis Loss : 4.232090577716008e-05\n",
      "Steps : 229100, \t Total Gen Loss : 3129.39013671875, \t Total Dis Loss : 3.861681398120709e-05\n",
      "Steps : 229200, \t Total Gen Loss : 3723.572509765625, \t Total Dis Loss : 2.6882040401687846e-05\n",
      "Steps : 229300, \t Total Gen Loss : 3617.000732421875, \t Total Dis Loss : 3.375830783625133e-05\n",
      "Steps : 229400, \t Total Gen Loss : 3628.31787109375, \t Total Dis Loss : 2.967528416775167e-06\n",
      "Steps : 229500, \t Total Gen Loss : 3585.22265625, \t Total Dis Loss : 1.678356056800112e-05\n",
      "Steps : 229600, \t Total Gen Loss : 3284.8740234375, \t Total Dis Loss : 1.185266773973126e-05\n",
      "Steps : 229700, \t Total Gen Loss : 3560.353271484375, \t Total Dis Loss : 4.127155989408493e-06\n",
      "Steps : 229800, \t Total Gen Loss : 3990.4169921875, \t Total Dis Loss : 4.493635060498491e-05\n",
      "Steps : 229900, \t Total Gen Loss : 3799.263427734375, \t Total Dis Loss : 1.62948745128233e-05\n",
      "Steps : 230000, \t Total Gen Loss : 3773.8984375, \t Total Dis Loss : 3.347971505718306e-05\n",
      "Steps : 230100, \t Total Gen Loss : 3441.764404296875, \t Total Dis Loss : 4.4820295443059877e-05\n",
      "Steps : 230200, \t Total Gen Loss : 4289.50048828125, \t Total Dis Loss : 2.5730496417963877e-05\n",
      "Steps : 230300, \t Total Gen Loss : 3790.5283203125, \t Total Dis Loss : 1.468635855417233e-05\n",
      "Steps : 230400, \t Total Gen Loss : 3225.897216796875, \t Total Dis Loss : 3.341463298056624e-06\n",
      "Steps : 230500, \t Total Gen Loss : 2972.205322265625, \t Total Dis Loss : 2.6962650281348033e-06\n",
      "Steps : 230600, \t Total Gen Loss : 3116.337158203125, \t Total Dis Loss : 4.598431405611336e-05\n",
      "Time for epoch 41 is 78.26984739303589 sec\n",
      "Steps : 230700, \t Total Gen Loss : 3986.014404296875, \t Total Dis Loss : 0.00010056619794340804\n",
      "Steps : 230800, \t Total Gen Loss : 4302.35986328125, \t Total Dis Loss : 5.638216680381447e-05\n",
      "Steps : 230900, \t Total Gen Loss : 3395.338623046875, \t Total Dis Loss : 1.4631352769356454e-06\n",
      "Steps : 231000, \t Total Gen Loss : 3808.87890625, \t Total Dis Loss : 3.4540272508820635e-07\n",
      "Steps : 231100, \t Total Gen Loss : 3555.68408203125, \t Total Dis Loss : 4.427306976140244e-06\n",
      "Steps : 231200, \t Total Gen Loss : 3130.212646484375, \t Total Dis Loss : 3.896392172464402e-06\n",
      "Steps : 231300, \t Total Gen Loss : 3733.716064453125, \t Total Dis Loss : 7.205001679722045e-07\n",
      "Steps : 231400, \t Total Gen Loss : 3541.590087890625, \t Total Dis Loss : 4.750490461447043e-06\n",
      "Steps : 231500, \t Total Gen Loss : 3750.07373046875, \t Total Dis Loss : 6.842507218607352e-07\n",
      "Steps : 231600, \t Total Gen Loss : 4137.943359375, \t Total Dis Loss : 1.8586881651572185e-06\n",
      "Steps : 231700, \t Total Gen Loss : 3816.15185546875, \t Total Dis Loss : 3.240632622691919e-06\n",
      "Steps : 231800, \t Total Gen Loss : 3436.884765625, \t Total Dis Loss : 8.120590791804716e-06\n",
      "Steps : 231900, \t Total Gen Loss : 3685.909912109375, \t Total Dis Loss : 1.0708249646995682e-06\n",
      "Steps : 232000, \t Total Gen Loss : 3899.933349609375, \t Total Dis Loss : 1.8402541854811716e-06\n",
      "Steps : 232100, \t Total Gen Loss : 3422.87890625, \t Total Dis Loss : 1.8216978787677363e-06\n",
      "Steps : 232200, \t Total Gen Loss : 3629.673583984375, \t Total Dis Loss : 2.0063985175511334e-06\n",
      "Steps : 232300, \t Total Gen Loss : 3460.3935546875, \t Total Dis Loss : 1.362743432764546e-06\n",
      "Steps : 232400, \t Total Gen Loss : 4239.0439453125, \t Total Dis Loss : 6.345094334392343e-06\n",
      "Steps : 232500, \t Total Gen Loss : 3449.245361328125, \t Total Dis Loss : 5.317888849276642e-07\n",
      "Steps : 232600, \t Total Gen Loss : 3416.75390625, \t Total Dis Loss : 3.2921730053203646e-07\n",
      "Steps : 232700, \t Total Gen Loss : 3449.662841796875, \t Total Dis Loss : 7.2773250394675415e-06\n",
      "Steps : 232800, \t Total Gen Loss : 3774.541259765625, \t Total Dis Loss : 3.234717269151588e-06\n",
      "Steps : 232900, \t Total Gen Loss : 3698.18408203125, \t Total Dis Loss : 1.2951710232300684e-05\n",
      "Steps : 233000, \t Total Gen Loss : 3941.1181640625, \t Total Dis Loss : 5.789918213849887e-05\n",
      "Steps : 233100, \t Total Gen Loss : 3278.46875, \t Total Dis Loss : 1.4111942618910689e-05\n",
      "Steps : 233200, \t Total Gen Loss : 4330.48291015625, \t Total Dis Loss : 4.9041511374525726e-05\n",
      "Steps : 233300, \t Total Gen Loss : 3791.705322265625, \t Total Dis Loss : 7.49665850889869e-05\n",
      "Steps : 233400, \t Total Gen Loss : 3717.51611328125, \t Total Dis Loss : 2.413583752058912e-05\n",
      "Steps : 233500, \t Total Gen Loss : 3754.01171875, \t Total Dis Loss : 1.8273505702381954e-05\n",
      "Steps : 233600, \t Total Gen Loss : 3256.91259765625, \t Total Dis Loss : 4.7580335376551375e-05\n",
      "Steps : 233700, \t Total Gen Loss : 3710.420654296875, \t Total Dis Loss : 8.865820745995734e-06\n",
      "Steps : 233800, \t Total Gen Loss : 3269.795654296875, \t Total Dis Loss : 1.0411574294266757e-05\n",
      "Steps : 233900, \t Total Gen Loss : 3492.270263671875, \t Total Dis Loss : 6.047221177141182e-06\n",
      "Steps : 234000, \t Total Gen Loss : 3727.91552734375, \t Total Dis Loss : 5.648455953632947e-06\n",
      "Steps : 234100, \t Total Gen Loss : 3922.17041015625, \t Total Dis Loss : 2.2540380086866207e-05\n",
      "Steps : 234200, \t Total Gen Loss : 3071.826416015625, \t Total Dis Loss : 4.729760348709533e-06\n",
      "Steps : 234300, \t Total Gen Loss : 3909.19873046875, \t Total Dis Loss : 1.4500684301310685e-05\n",
      "Steps : 234400, \t Total Gen Loss : 3883.833984375, \t Total Dis Loss : 3.5430548450676724e-06\n",
      "Steps : 234500, \t Total Gen Loss : 3397.940673828125, \t Total Dis Loss : 6.3732204580446705e-06\n",
      "Steps : 234600, \t Total Gen Loss : 3963.392333984375, \t Total Dis Loss : 6.178944204293657e-06\n",
      "Steps : 234700, \t Total Gen Loss : 3433.745361328125, \t Total Dis Loss : 2.8977472084079636e-06\n",
      "Steps : 234800, \t Total Gen Loss : 4285.4384765625, \t Total Dis Loss : 1.7257589206565171e-06\n",
      "Steps : 234900, \t Total Gen Loss : 4124.57275390625, \t Total Dis Loss : 3.2624229788780212e-06\n",
      "Steps : 235000, \t Total Gen Loss : 3600.83349609375, \t Total Dis Loss : 7.508414910262218e-06\n",
      "Steps : 235100, \t Total Gen Loss : 3644.031494140625, \t Total Dis Loss : 0.021764066070318222\n",
      "Steps : 235200, \t Total Gen Loss : 3333.27392578125, \t Total Dis Loss : 6.2441290538117755e-06\n",
      "Steps : 235300, \t Total Gen Loss : 4181.4560546875, \t Total Dis Loss : 3.1422696338268e-05\n",
      "Steps : 235400, \t Total Gen Loss : 3431.070556640625, \t Total Dis Loss : 2.56587918556761e-05\n",
      "Steps : 235500, \t Total Gen Loss : 3301.451171875, \t Total Dis Loss : 3.1339324777945876e-05\n",
      "Steps : 235600, \t Total Gen Loss : 3067.045166015625, \t Total Dis Loss : 1.7078480141208274e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 235700, \t Total Gen Loss : 3844.749755859375, \t Total Dis Loss : 1.1163248245793511e-06\n",
      "Steps : 235800, \t Total Gen Loss : 3718.216796875, \t Total Dis Loss : 5.837723620061297e-06\n",
      "Steps : 235900, \t Total Gen Loss : 3380.461669921875, \t Total Dis Loss : 2.2161598280945327e-06\n",
      "Steps : 236000, \t Total Gen Loss : 3391.18701171875, \t Total Dis Loss : 1.2174706171208527e-05\n",
      "Steps : 236100, \t Total Gen Loss : 3374.987548828125, \t Total Dis Loss : 0.0004853219143114984\n",
      "Steps : 236200, \t Total Gen Loss : 3574.031982421875, \t Total Dis Loss : 6.053520701243542e-05\n",
      "Time for epoch 42 is 79.01811671257019 sec\n",
      "Steps : 236300, \t Total Gen Loss : 3715.653076171875, \t Total Dis Loss : 1.7757638488546945e-05\n",
      "Steps : 236400, \t Total Gen Loss : 3041.98486328125, \t Total Dis Loss : 7.971038576215506e-05\n",
      "Steps : 236500, \t Total Gen Loss : 3898.29541015625, \t Total Dis Loss : 0.0002868977899197489\n",
      "Steps : 236600, \t Total Gen Loss : 3776.68359375, \t Total Dis Loss : 4.202362833893858e-05\n",
      "Steps : 236700, \t Total Gen Loss : 3873.17431640625, \t Total Dis Loss : 5.590039654634893e-05\n",
      "Steps : 236800, \t Total Gen Loss : 3406.68115234375, \t Total Dis Loss : 2.183187461923808e-05\n",
      "Steps : 236900, \t Total Gen Loss : 3276.708740234375, \t Total Dis Loss : 8.963911386672407e-05\n",
      "Steps : 237000, \t Total Gen Loss : 3258.094970703125, \t Total Dis Loss : 9.000890713650733e-06\n",
      "Steps : 237100, \t Total Gen Loss : 3704.40771484375, \t Total Dis Loss : 5.990146746626124e-05\n",
      "Steps : 237200, \t Total Gen Loss : 3534.53564453125, \t Total Dis Loss : 1.9809060177067295e-05\n",
      "Steps : 237300, \t Total Gen Loss : 3430.835205078125, \t Total Dis Loss : 4.032014658150729e-06\n",
      "Steps : 237400, \t Total Gen Loss : 3885.91748046875, \t Total Dis Loss : 2.615958919705008e-06\n",
      "Steps : 237500, \t Total Gen Loss : 4216.72021484375, \t Total Dis Loss : 0.002298622624948621\n",
      "Steps : 237600, \t Total Gen Loss : 3771.07958984375, \t Total Dis Loss : 4.904211891698651e-06\n",
      "Steps : 237700, \t Total Gen Loss : 3409.324462890625, \t Total Dis Loss : 8.508392056683078e-06\n",
      "Steps : 237800, \t Total Gen Loss : 3709.175537109375, \t Total Dis Loss : 6.513193511636928e-05\n",
      "Steps : 237900, \t Total Gen Loss : 3291.763427734375, \t Total Dis Loss : 1.2588860727191786e-06\n",
      "Steps : 238000, \t Total Gen Loss : 3400.906005859375, \t Total Dis Loss : 5.2934706218366046e-06\n",
      "Steps : 238100, \t Total Gen Loss : 3794.094970703125, \t Total Dis Loss : 1.2392142707540188e-05\n",
      "Steps : 238200, \t Total Gen Loss : 3718.7236328125, \t Total Dis Loss : 2.0629802747862414e-05\n",
      "Steps : 238300, \t Total Gen Loss : 3900.197265625, \t Total Dis Loss : 6.4800942709553055e-06\n",
      "Steps : 238400, \t Total Gen Loss : 3936.822509765625, \t Total Dis Loss : 1.1272879419266246e-05\n",
      "Steps : 238500, \t Total Gen Loss : 3688.7587890625, \t Total Dis Loss : 2.7734158720704727e-06\n",
      "Steps : 238600, \t Total Gen Loss : 3876.46875, \t Total Dis Loss : 1.2938454347022343e-05\n",
      "Steps : 238700, \t Total Gen Loss : 3852.972900390625, \t Total Dis Loss : 9.650455012888415e-07\n",
      "Steps : 238800, \t Total Gen Loss : 4611.943359375, \t Total Dis Loss : 1.6257959032373037e-06\n",
      "Steps : 238900, \t Total Gen Loss : 3653.02490234375, \t Total Dis Loss : 3.119434040854685e-06\n",
      "Steps : 239000, \t Total Gen Loss : 4077.547607421875, \t Total Dis Loss : 2.8670174287981354e-06\n",
      "Steps : 239100, \t Total Gen Loss : 3575.45068359375, \t Total Dis Loss : 7.270758942468092e-06\n",
      "Steps : 239200, \t Total Gen Loss : 3953.365234375, \t Total Dis Loss : 5.076540674053831e-06\n",
      "Steps : 239300, \t Total Gen Loss : 3568.398193359375, \t Total Dis Loss : 4.663384970626794e-06\n",
      "Steps : 239400, \t Total Gen Loss : 3527.8740234375, \t Total Dis Loss : 1.0258098882331979e-05\n",
      "Steps : 239500, \t Total Gen Loss : 3513.5712890625, \t Total Dis Loss : 1.9324402273923624e-06\n",
      "Steps : 239600, \t Total Gen Loss : 3606.275146484375, \t Total Dis Loss : 0.00030918631819076836\n",
      "Steps : 239700, \t Total Gen Loss : 4149.87646484375, \t Total Dis Loss : 3.472929893177934e-05\n",
      "Steps : 239800, \t Total Gen Loss : 3945.99560546875, \t Total Dis Loss : 5.824643267260399e-06\n",
      "Steps : 239900, \t Total Gen Loss : 3502.571533203125, \t Total Dis Loss : 1.2981281543034129e-05\n",
      "Steps : 240000, \t Total Gen Loss : 3696.5771484375, \t Total Dis Loss : 4.373060437501408e-05\n",
      "Steps : 240100, \t Total Gen Loss : 3358.451171875, \t Total Dis Loss : 5.075602985016303e-06\n",
      "Steps : 240200, \t Total Gen Loss : 3517.1171875, \t Total Dis Loss : 1.3579740880231839e-05\n",
      "Steps : 240300, \t Total Gen Loss : 3106.559326171875, \t Total Dis Loss : 2.7863632567459717e-05\n",
      "Steps : 240400, \t Total Gen Loss : 3699.571533203125, \t Total Dis Loss : 2.226444485131651e-05\n",
      "Steps : 240500, \t Total Gen Loss : 4331.83203125, \t Total Dis Loss : 1.2500087905209512e-05\n",
      "Steps : 240600, \t Total Gen Loss : 3605.89990234375, \t Total Dis Loss : 0.00028608128195628524\n",
      "Steps : 240700, \t Total Gen Loss : 3656.893798828125, \t Total Dis Loss : 2.67916802840773e-05\n",
      "Steps : 240800, \t Total Gen Loss : 3776.514404296875, \t Total Dis Loss : 5.865981620445382e-06\n",
      "Steps : 240900, \t Total Gen Loss : 4063.783203125, \t Total Dis Loss : 2.568679519754369e-06\n",
      "Steps : 241000, \t Total Gen Loss : 3699.02978515625, \t Total Dis Loss : 5.418912769528106e-05\n",
      "Steps : 241100, \t Total Gen Loss : 4126.22607421875, \t Total Dis Loss : 4.174013611191185e-06\n",
      "Steps : 241200, \t Total Gen Loss : 3752.294677734375, \t Total Dis Loss : 3.3167052606586367e-06\n",
      "Steps : 241300, \t Total Gen Loss : 4304.8779296875, \t Total Dis Loss : 9.073359251488e-06\n",
      "Steps : 241400, \t Total Gen Loss : 3962.77783203125, \t Total Dis Loss : 5.0137073230871465e-06\n",
      "Steps : 241500, \t Total Gen Loss : 2799.72021484375, \t Total Dis Loss : 0.00020133081125095487\n",
      "Steps : 241600, \t Total Gen Loss : 3017.385009765625, \t Total Dis Loss : 3.84562044928316e-05\n",
      "Steps : 241700, \t Total Gen Loss : 3363.57275390625, \t Total Dis Loss : 9.200869499181863e-06\n",
      "Steps : 241800, \t Total Gen Loss : 3410.700927734375, \t Total Dis Loss : 1.297487960982835e-05\n",
      "Time for epoch 43 is 79.62507247924805 sec\n",
      "Steps : 241900, \t Total Gen Loss : 3478.28076171875, \t Total Dis Loss : 1.7048399968189187e-05\n",
      "Steps : 242000, \t Total Gen Loss : 3646.272216796875, \t Total Dis Loss : 2.8231097530806437e-05\n",
      "Steps : 242100, \t Total Gen Loss : 3461.2177734375, \t Total Dis Loss : 2.0822024453082122e-05\n",
      "Steps : 242200, \t Total Gen Loss : 4037.309326171875, \t Total Dis Loss : 4.946207354805665e-06\n",
      "Steps : 242300, \t Total Gen Loss : 3306.187744140625, \t Total Dis Loss : 2.4587670850451104e-06\n",
      "Steps : 242400, \t Total Gen Loss : 3796.600830078125, \t Total Dis Loss : 0.0008332448196597397\n",
      "Steps : 242500, \t Total Gen Loss : 3908.4697265625, \t Total Dis Loss : 1.117059582611546e-05\n",
      "Steps : 242600, \t Total Gen Loss : 3281.89404296875, \t Total Dis Loss : 4.039764462504536e-05\n",
      "Steps : 242700, \t Total Gen Loss : 3299.364990234375, \t Total Dis Loss : 3.12280690195621e-06\n",
      "Steps : 242800, \t Total Gen Loss : 3542.75830078125, \t Total Dis Loss : 3.78678560082335e-05\n",
      "Steps : 242900, \t Total Gen Loss : 3579.430419921875, \t Total Dis Loss : 2.9706072382396087e-05\n",
      "Steps : 243000, \t Total Gen Loss : 3818.49560546875, \t Total Dis Loss : 4.649602033168776e-06\n",
      "Steps : 243100, \t Total Gen Loss : 3232.396240234375, \t Total Dis Loss : 1.442462053091731e-05\n",
      "Steps : 243200, \t Total Gen Loss : 3323.3095703125, \t Total Dis Loss : 1.973265898413956e-05\n",
      "Steps : 243300, \t Total Gen Loss : 3212.02001953125, \t Total Dis Loss : 1.2717122444882989e-05\n",
      "Steps : 243400, \t Total Gen Loss : 4336.1318359375, \t Total Dis Loss : 9.834083357418422e-06\n",
      "Steps : 243500, \t Total Gen Loss : 3222.384521484375, \t Total Dis Loss : 9.629410669731442e-06\n",
      "Steps : 243600, \t Total Gen Loss : 3996.958251953125, \t Total Dis Loss : 4.8491892812307924e-05\n",
      "Steps : 243700, \t Total Gen Loss : 3190.295654296875, \t Total Dis Loss : 1.19534397526877e-05\n",
      "Steps : 243800, \t Total Gen Loss : 3871.7841796875, \t Total Dis Loss : 8.083760621957481e-05\n",
      "Steps : 243900, \t Total Gen Loss : 3752.016357421875, \t Total Dis Loss : 6.938354636076838e-05\n",
      "Steps : 244000, \t Total Gen Loss : 3840.863037109375, \t Total Dis Loss : 2.3131306079449132e-05\n",
      "Steps : 244100, \t Total Gen Loss : 3592.058837890625, \t Total Dis Loss : 7.4025956564582884e-06\n",
      "Steps : 244200, \t Total Gen Loss : 3161.449462890625, \t Total Dis Loss : 7.180900138337165e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 244300, \t Total Gen Loss : 3708.736083984375, \t Total Dis Loss : 2.673776907613501e-05\n",
      "Steps : 244400, \t Total Gen Loss : 3312.28955078125, \t Total Dis Loss : 0.00013313947420101613\n",
      "Steps : 244500, \t Total Gen Loss : 3355.1591796875, \t Total Dis Loss : 1.8443897715769708e-05\n",
      "Steps : 244600, \t Total Gen Loss : 3839.049072265625, \t Total Dis Loss : 4.423402970132884e-06\n",
      "Steps : 244700, \t Total Gen Loss : 3787.290771484375, \t Total Dis Loss : 1.1611406080191955e-05\n",
      "Steps : 244800, \t Total Gen Loss : 3661.41748046875, \t Total Dis Loss : 6.212899279489648e-06\n",
      "Steps : 244900, \t Total Gen Loss : 3867.462646484375, \t Total Dis Loss : 1.0277922228851821e-05\n",
      "Steps : 245000, \t Total Gen Loss : 3185.36962890625, \t Total Dis Loss : 3.3639258617768064e-05\n",
      "Steps : 245100, \t Total Gen Loss : 3808.5703125, \t Total Dis Loss : 9.583279461367056e-05\n",
      "Steps : 245200, \t Total Gen Loss : 3585.9580078125, \t Total Dis Loss : 2.9552180421887897e-05\n",
      "Steps : 245300, \t Total Gen Loss : 3840.604248046875, \t Total Dis Loss : 0.00012732799223158509\n",
      "Steps : 245400, \t Total Gen Loss : 3407.962158203125, \t Total Dis Loss : 1.5474823158001527e-05\n",
      "Steps : 245500, \t Total Gen Loss : 3766.5830078125, \t Total Dis Loss : 1.0307581760571338e-05\n",
      "Steps : 245600, \t Total Gen Loss : 3244.791259765625, \t Total Dis Loss : 4.94427058583824e-06\n",
      "Steps : 245700, \t Total Gen Loss : 3675.552734375, \t Total Dis Loss : 1.02074263850227e-05\n",
      "Steps : 245800, \t Total Gen Loss : 4193.373046875, \t Total Dis Loss : 1.5525758499279618e-05\n",
      "Steps : 245900, \t Total Gen Loss : 4160.59765625, \t Total Dis Loss : 1.6924141164054163e-06\n",
      "Steps : 246000, \t Total Gen Loss : 2931.831787109375, \t Total Dis Loss : 1.027071448334027e-05\n",
      "Steps : 246100, \t Total Gen Loss : 3516.888427734375, \t Total Dis Loss : 3.386421303730458e-05\n",
      "Steps : 246200, \t Total Gen Loss : 3791.26904296875, \t Total Dis Loss : 3.8001110169716412e-06\n",
      "Steps : 246300, \t Total Gen Loss : 3530.6494140625, \t Total Dis Loss : 7.089565770002082e-06\n",
      "Steps : 246400, \t Total Gen Loss : 3348.398193359375, \t Total Dis Loss : 1.808355409593787e-05\n",
      "Steps : 246500, \t Total Gen Loss : 3281.777099609375, \t Total Dis Loss : 4.397270458866842e-05\n",
      "Steps : 246600, \t Total Gen Loss : 3256.822021484375, \t Total Dis Loss : 2.0858396965195425e-05\n",
      "Steps : 246700, \t Total Gen Loss : 3829.70654296875, \t Total Dis Loss : 1.3403090633801185e-05\n",
      "Steps : 246800, \t Total Gen Loss : 3700.426513671875, \t Total Dis Loss : 7.538875706813997e-06\n",
      "Steps : 246900, \t Total Gen Loss : 3803.23193359375, \t Total Dis Loss : 2.2215035642147996e-06\n",
      "Steps : 247000, \t Total Gen Loss : 3177.045654296875, \t Total Dis Loss : 1.7862872482510284e-05\n",
      "Steps : 247100, \t Total Gen Loss : 3173.785400390625, \t Total Dis Loss : 8.156313924700953e-06\n",
      "Steps : 247200, \t Total Gen Loss : 3582.96142578125, \t Total Dis Loss : 2.249327371828258e-05\n",
      "Steps : 247300, \t Total Gen Loss : 3542.6181640625, \t Total Dis Loss : 5.8027508202940226e-05\n",
      "Steps : 247400, \t Total Gen Loss : 3960.153564453125, \t Total Dis Loss : 1.632049861655105e-05\n",
      "Steps : 247500, \t Total Gen Loss : 3844.39404296875, \t Total Dis Loss : 1.778501973603852e-05\n",
      "Time for epoch 44 is 79.5685567855835 sec\n",
      "Steps : 247600, \t Total Gen Loss : 3509.041015625, \t Total Dis Loss : 7.185481081251055e-06\n",
      "Steps : 247700, \t Total Gen Loss : 3279.814697265625, \t Total Dis Loss : 3.151046621496789e-05\n",
      "Steps : 247800, \t Total Gen Loss : 3283.736083984375, \t Total Dis Loss : 7.806178473401815e-06\n",
      "Steps : 247900, \t Total Gen Loss : 3981.935302734375, \t Total Dis Loss : 6.605733688047621e-06\n",
      "Steps : 248000, \t Total Gen Loss : 3653.220703125, \t Total Dis Loss : 3.650999497040175e-05\n",
      "Steps : 248100, \t Total Gen Loss : 3201.22216796875, \t Total Dis Loss : 3.088662924710661e-05\n",
      "Steps : 248200, \t Total Gen Loss : 3378.603759765625, \t Total Dis Loss : 3.0469580451608635e-05\n",
      "Steps : 248300, \t Total Gen Loss : 3642.985595703125, \t Total Dis Loss : 0.0006224650423973799\n",
      "Steps : 248400, \t Total Gen Loss : 3266.277099609375, \t Total Dis Loss : 3.9428963646059856e-05\n",
      "Steps : 248500, \t Total Gen Loss : 3571.237060546875, \t Total Dis Loss : 0.00024630213738419116\n",
      "Steps : 248600, \t Total Gen Loss : 3742.14013671875, \t Total Dis Loss : 0.0026686706114560366\n",
      "Steps : 248700, \t Total Gen Loss : 3474.43701171875, \t Total Dis Loss : 6.223308446351439e-05\n",
      "Steps : 248800, \t Total Gen Loss : 3599.73095703125, \t Total Dis Loss : 0.0003543186467140913\n",
      "Steps : 248900, \t Total Gen Loss : 3619.672119140625, \t Total Dis Loss : 8.847071148920804e-05\n",
      "Steps : 249000, \t Total Gen Loss : 3697.056396484375, \t Total Dis Loss : 9.839199628913775e-05\n",
      "Steps : 249100, \t Total Gen Loss : 3553.547119140625, \t Total Dis Loss : 4.3590374843915924e-05\n",
      "Steps : 249200, \t Total Gen Loss : 3637.64013671875, \t Total Dis Loss : 5.0397000450175256e-05\n",
      "Steps : 249300, \t Total Gen Loss : 3953.25244140625, \t Total Dis Loss : 0.00014249631203711033\n",
      "Steps : 249400, \t Total Gen Loss : 4374.9814453125, \t Total Dis Loss : 1.5318767054850468e-06\n",
      "Steps : 249500, \t Total Gen Loss : 3431.4091796875, \t Total Dis Loss : 3.4882552881754236e-06\n",
      "Steps : 249600, \t Total Gen Loss : 3540.780517578125, \t Total Dis Loss : 2.2028578314348124e-05\n",
      "Steps : 249700, \t Total Gen Loss : 3180.817626953125, \t Total Dis Loss : 2.55272357208014e-06\n",
      "Steps : 249800, \t Total Gen Loss : 3059.45263671875, \t Total Dis Loss : 2.324703018530272e-06\n",
      "Steps : 249900, \t Total Gen Loss : 3271.87744140625, \t Total Dis Loss : 3.821100563072832e-06\n",
      "Steps : 250000, \t Total Gen Loss : 3885.779296875, \t Total Dis Loss : 2.6500297281017993e-06\n",
      "Steps : 250100, \t Total Gen Loss : 3488.52490234375, \t Total Dis Loss : 3.767364660234307e-06\n",
      "Steps : 250200, \t Total Gen Loss : 3504.260498046875, \t Total Dis Loss : 5.680221420334419e-06\n",
      "Steps : 250300, \t Total Gen Loss : 3702.25537109375, \t Total Dis Loss : 6.2987419369164854e-06\n",
      "Steps : 250400, \t Total Gen Loss : 3636.769287109375, \t Total Dis Loss : 8.106970926746726e-05\n",
      "Steps : 250500, \t Total Gen Loss : 3253.4638671875, \t Total Dis Loss : 8.338122825080063e-06\n",
      "Steps : 250600, \t Total Gen Loss : 3438.31103515625, \t Total Dis Loss : 0.00018037352128885686\n",
      "Steps : 250700, \t Total Gen Loss : 3259.345947265625, \t Total Dis Loss : 8.959023034549318e-06\n",
      "Steps : 250800, \t Total Gen Loss : 3543.504638671875, \t Total Dis Loss : 5.789679107692791e-06\n",
      "Steps : 250900, \t Total Gen Loss : 3595.23486328125, \t Total Dis Loss : 2.027816663030535e-05\n",
      "Steps : 251000, \t Total Gen Loss : 3807.26708984375, \t Total Dis Loss : 1.2750109817716293e-05\n",
      "Steps : 251100, \t Total Gen Loss : 3770.73388671875, \t Total Dis Loss : 5.75827834836673e-05\n",
      "Steps : 251200, \t Total Gen Loss : 4303.34228515625, \t Total Dis Loss : 3.099177547483123e-06\n",
      "Steps : 251300, \t Total Gen Loss : 4110.59326171875, \t Total Dis Loss : 1.0881414709729142e-05\n",
      "Steps : 251400, \t Total Gen Loss : 3199.671142578125, \t Total Dis Loss : 4.567324594972888e-06\n",
      "Steps : 251500, \t Total Gen Loss : 3761.857177734375, \t Total Dis Loss : 9.893237802316435e-06\n",
      "Steps : 251600, \t Total Gen Loss : 3746.494140625, \t Total Dis Loss : 1.3531978765968233e-05\n",
      "Steps : 251700, \t Total Gen Loss : 3500.967529296875, \t Total Dis Loss : 7.674674634472467e-06\n",
      "Steps : 251800, \t Total Gen Loss : 3792.91064453125, \t Total Dis Loss : 6.036843387846602e-06\n",
      "Steps : 251900, \t Total Gen Loss : 3979.772216796875, \t Total Dis Loss : 3.914363696821965e-05\n",
      "Steps : 252000, \t Total Gen Loss : 3478.27001953125, \t Total Dis Loss : 4.090570655534975e-05\n",
      "Steps : 252100, \t Total Gen Loss : 3491.443115234375, \t Total Dis Loss : 2.0526565549516818e-06\n",
      "Steps : 252200, \t Total Gen Loss : 3295.53564453125, \t Total Dis Loss : 4.982075097359484e-06\n",
      "Steps : 252300, \t Total Gen Loss : 3726.65185546875, \t Total Dis Loss : 3.3552855711604934e-06\n",
      "Steps : 252400, \t Total Gen Loss : 3038.481689453125, \t Total Dis Loss : 1.9848462216032203e-06\n",
      "Steps : 252500, \t Total Gen Loss : 4022.09033203125, \t Total Dis Loss : 1.267246716452064e-05\n",
      "Steps : 252600, \t Total Gen Loss : 4040.84912109375, \t Total Dis Loss : 8.934381185099483e-06\n",
      "Steps : 252700, \t Total Gen Loss : 4165.97802734375, \t Total Dis Loss : 3.1309828045777977e-06\n",
      "Steps : 252800, \t Total Gen Loss : 3081.018310546875, \t Total Dis Loss : 6.671619757980807e-06\n",
      "Steps : 252900, \t Total Gen Loss : 3557.085693359375, \t Total Dis Loss : 7.4377421697136015e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 253000, \t Total Gen Loss : 3767.697998046875, \t Total Dis Loss : 9.154908866548794e-07\n",
      "Steps : 253100, \t Total Gen Loss : 3467.922607421875, \t Total Dis Loss : 2.033047167060431e-06\n",
      "Time for epoch 45 is 81.94866418838501 sec\n",
      "Steps : 253200, \t Total Gen Loss : 3670.867919921875, \t Total Dis Loss : 3.5415359889157116e-07\n",
      "Steps : 253300, \t Total Gen Loss : 3589.79052734375, \t Total Dis Loss : 1.358390477435023e-06\n",
      "Steps : 253400, \t Total Gen Loss : 3147.4814453125, \t Total Dis Loss : 0.00021738217037636787\n",
      "Steps : 253500, \t Total Gen Loss : 3765.842529296875, \t Total Dis Loss : 0.0008524787263013422\n",
      "Steps : 253600, \t Total Gen Loss : 3688.80712890625, \t Total Dis Loss : 9.053507028511376e-07\n",
      "Steps : 253700, \t Total Gen Loss : 3018.1640625, \t Total Dis Loss : 1.2711713679891545e-05\n",
      "Steps : 253800, \t Total Gen Loss : 3087.0458984375, \t Total Dis Loss : 1.6950593817455228e-06\n",
      "Steps : 253900, \t Total Gen Loss : 3203.859619140625, \t Total Dis Loss : 9.933141882356722e-07\n",
      "Steps : 254000, \t Total Gen Loss : 3349.08251953125, \t Total Dis Loss : 1.0734417628555093e-05\n",
      "Steps : 254100, \t Total Gen Loss : 3477.959716796875, \t Total Dis Loss : 3.127029913230217e-06\n",
      "Steps : 254200, \t Total Gen Loss : 3377.933837890625, \t Total Dis Loss : 7.770919182803482e-05\n",
      "Steps : 254300, \t Total Gen Loss : 3808.72607421875, \t Total Dis Loss : 5.985462121316232e-05\n",
      "Steps : 254400, \t Total Gen Loss : 3808.37158203125, \t Total Dis Loss : 7.93079198047053e-06\n",
      "Steps : 254500, \t Total Gen Loss : 3641.856689453125, \t Total Dis Loss : 2.902728965636925e-06\n",
      "Steps : 254600, \t Total Gen Loss : 3104.773681640625, \t Total Dis Loss : 7.127708158805035e-06\n",
      "Steps : 254700, \t Total Gen Loss : 4264.23486328125, \t Total Dis Loss : 4.429439650266431e-05\n",
      "Steps : 254800, \t Total Gen Loss : 3773.931884765625, \t Total Dis Loss : 1.3986210433358792e-05\n",
      "Steps : 254900, \t Total Gen Loss : 3578.985595703125, \t Total Dis Loss : 4.436631570570171e-05\n",
      "Steps : 255000, \t Total Gen Loss : 3660.09033203125, \t Total Dis Loss : 0.00018187504610978067\n",
      "Steps : 255100, \t Total Gen Loss : 3672.02294921875, \t Total Dis Loss : 0.00031756411772221327\n",
      "Steps : 255200, \t Total Gen Loss : 3704.283447265625, \t Total Dis Loss : 1.1599873687373474e-05\n",
      "Steps : 255300, \t Total Gen Loss : 3671.7412109375, \t Total Dis Loss : 2.8449060209823074e-06\n",
      "Steps : 255400, \t Total Gen Loss : 3727.7734375, \t Total Dis Loss : 1.949434818016016e-06\n",
      "Steps : 255500, \t Total Gen Loss : 3898.32763671875, \t Total Dis Loss : 4.320686457504053e-06\n",
      "Steps : 255600, \t Total Gen Loss : 3802.698974609375, \t Total Dis Loss : 3.1126372050493956e-05\n",
      "Steps : 255700, \t Total Gen Loss : 3766.7998046875, \t Total Dis Loss : 7.715049287071452e-05\n",
      "Steps : 255800, \t Total Gen Loss : 3568.5966796875, \t Total Dis Loss : 0.00023753021378070116\n",
      "Steps : 255900, \t Total Gen Loss : 4100.4091796875, \t Total Dis Loss : 1.6043344658100978e-05\n",
      "Steps : 256000, \t Total Gen Loss : 3594.513671875, \t Total Dis Loss : 1.400188648403855e-05\n",
      "Steps : 256100, \t Total Gen Loss : 3976.696044921875, \t Total Dis Loss : 1.5504805560340174e-05\n",
      "Steps : 256200, \t Total Gen Loss : 3794.564697265625, \t Total Dis Loss : 6.6509851421869826e-06\n",
      "Steps : 256300, \t Total Gen Loss : 3771.323486328125, \t Total Dis Loss : 9.804588444239926e-06\n",
      "Steps : 256400, \t Total Gen Loss : 3972.6923828125, \t Total Dis Loss : 7.157694653869839e-06\n",
      "Steps : 256500, \t Total Gen Loss : 3229.28369140625, \t Total Dis Loss : 3.965159521612804e-06\n",
      "Steps : 256600, \t Total Gen Loss : 3702.65380859375, \t Total Dis Loss : 1.0560521332081407e-05\n",
      "Steps : 256700, \t Total Gen Loss : 3748.775390625, \t Total Dis Loss : 1.2736280950775836e-05\n",
      "Steps : 256800, \t Total Gen Loss : 4002.88232421875, \t Total Dis Loss : 3.2341310998162953e-06\n",
      "Steps : 256900, \t Total Gen Loss : 4256.708984375, \t Total Dis Loss : 1.9844892449327745e-06\n",
      "Steps : 257000, \t Total Gen Loss : 3973.8056640625, \t Total Dis Loss : 4.130741217522882e-06\n",
      "Steps : 257100, \t Total Gen Loss : 3725.48388671875, \t Total Dis Loss : 3.228881496397662e-06\n",
      "Steps : 257200, \t Total Gen Loss : 3337.291015625, \t Total Dis Loss : 4.639281087293057e-06\n",
      "Steps : 257300, \t Total Gen Loss : 3542.993896484375, \t Total Dis Loss : 7.671653293073177e-06\n",
      "Steps : 257400, \t Total Gen Loss : 4264.78759765625, \t Total Dis Loss : 1.3758229215454776e-05\n",
      "Steps : 257500, \t Total Gen Loss : 3745.683837890625, \t Total Dis Loss : 3.991293397120899e-06\n",
      "Steps : 257600, \t Total Gen Loss : 3980.17041015625, \t Total Dis Loss : 3.257403704992612e-06\n",
      "Steps : 257700, \t Total Gen Loss : 3774.59814453125, \t Total Dis Loss : 4.080962753505446e-06\n",
      "Steps : 257800, \t Total Gen Loss : 3800.0810546875, \t Total Dis Loss : 4.43120370618999e-05\n",
      "Steps : 257900, \t Total Gen Loss : 3485.482177734375, \t Total Dis Loss : 1.5283803804777563e-05\n",
      "Steps : 258000, \t Total Gen Loss : 3388.0244140625, \t Total Dis Loss : 1.2297223292989656e-06\n",
      "Steps : 258100, \t Total Gen Loss : 4028.32080078125, \t Total Dis Loss : 1.021901516651269e-05\n",
      "Steps : 258200, \t Total Gen Loss : 4175.46337890625, \t Total Dis Loss : 1.385644372930983e-05\n",
      "Steps : 258300, \t Total Gen Loss : 3252.11962890625, \t Total Dis Loss : 2.479825525369961e-06\n",
      "Steps : 258400, \t Total Gen Loss : 2859.840087890625, \t Total Dis Loss : 2.2427873773267493e-05\n",
      "Steps : 258500, \t Total Gen Loss : 3693.8232421875, \t Total Dis Loss : 6.272466271184385e-05\n",
      "Steps : 258600, \t Total Gen Loss : 2761.79541015625, \t Total Dis Loss : 2.533819269956439e-06\n",
      "Steps : 258700, \t Total Gen Loss : 3211.8876953125, \t Total Dis Loss : 0.00010260703129461035\n",
      "Time for epoch 46 is 81.36541223526001 sec\n",
      "Steps : 258800, \t Total Gen Loss : 3665.833251953125, \t Total Dis Loss : 3.837386248051189e-05\n",
      "Steps : 258900, \t Total Gen Loss : 3899.05615234375, \t Total Dis Loss : 0.00012595295265782624\n",
      "Steps : 259000, \t Total Gen Loss : 2981.07373046875, \t Total Dis Loss : 1.6067671822384e-05\n",
      "Steps : 259100, \t Total Gen Loss : 3064.5283203125, \t Total Dis Loss : 8.471604814985767e-06\n",
      "Steps : 259200, \t Total Gen Loss : 3871.6494140625, \t Total Dis Loss : 3.614205297708395e-06\n",
      "Steps : 259300, \t Total Gen Loss : 3312.059326171875, \t Total Dis Loss : 3.6628447560360655e-05\n",
      "Steps : 259400, \t Total Gen Loss : 3747.484375, \t Total Dis Loss : 9.52880236582132e-06\n",
      "Steps : 259500, \t Total Gen Loss : 3531.770751953125, \t Total Dis Loss : 5.056806912762113e-06\n",
      "Steps : 259600, \t Total Gen Loss : 3993.174072265625, \t Total Dis Loss : 2.443928133288864e-05\n",
      "Steps : 259700, \t Total Gen Loss : 3497.2841796875, \t Total Dis Loss : 8.799284842098132e-05\n",
      "Steps : 259800, \t Total Gen Loss : 3814.5986328125, \t Total Dis Loss : 4.333084507379681e-05\n",
      "Steps : 259900, \t Total Gen Loss : 3305.79638671875, \t Total Dis Loss : 5.1137440095772035e-06\n",
      "Steps : 260000, \t Total Gen Loss : 3510.69384765625, \t Total Dis Loss : 6.5433423515059985e-06\n",
      "Steps : 260100, \t Total Gen Loss : 4077.376708984375, \t Total Dis Loss : 7.856837328290567e-05\n",
      "Steps : 260200, \t Total Gen Loss : 3536.551513671875, \t Total Dis Loss : 9.689143917057663e-05\n",
      "Steps : 260300, \t Total Gen Loss : 3706.865478515625, \t Total Dis Loss : 5.727554525947198e-05\n",
      "Steps : 260400, \t Total Gen Loss : 3391.61328125, \t Total Dis Loss : 3.1711520023236517e-06\n",
      "Steps : 260500, \t Total Gen Loss : 3479.7568359375, \t Total Dis Loss : 1.448167586204363e-05\n",
      "Steps : 260600, \t Total Gen Loss : 3628.041748046875, \t Total Dis Loss : 1.674261875450611e-05\n",
      "Steps : 260700, \t Total Gen Loss : 3586.1123046875, \t Total Dis Loss : 1.449708634027047e-05\n",
      "Steps : 260800, \t Total Gen Loss : 4278.5517578125, \t Total Dis Loss : 1.7326738088740967e-05\n",
      "Steps : 260900, \t Total Gen Loss : 3772.836181640625, \t Total Dis Loss : 1.9533830709406175e-05\n",
      "Steps : 261000, \t Total Gen Loss : 3684.403076171875, \t Total Dis Loss : 3.581020428100601e-05\n",
      "Steps : 261100, \t Total Gen Loss : 3719.3056640625, \t Total Dis Loss : 3.206586552551016e-05\n",
      "Steps : 261200, \t Total Gen Loss : 2921.808837890625, \t Total Dis Loss : 2.0988918549846858e-05\n",
      "Steps : 261300, \t Total Gen Loss : 3326.804443359375, \t Total Dis Loss : 8.872230523593316e-07\n",
      "Steps : 261400, \t Total Gen Loss : 3672.927490234375, \t Total Dis Loss : 6.575735369551694e-06\n",
      "Steps : 261500, \t Total Gen Loss : 3652.77880859375, \t Total Dis Loss : 8.213152796088252e-06\n",
      "Steps : 261600, \t Total Gen Loss : 3598.123291015625, \t Total Dis Loss : 2.0498225694609573e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 261700, \t Total Gen Loss : 3379.47705078125, \t Total Dis Loss : 2.3476986825698987e-06\n",
      "Steps : 261800, \t Total Gen Loss : 3423.96630859375, \t Total Dis Loss : 1.3818039406032767e-05\n",
      "Steps : 261900, \t Total Gen Loss : 3706.331787109375, \t Total Dis Loss : 9.280084668716881e-06\n",
      "Steps : 262000, \t Total Gen Loss : 4112.302734375, \t Total Dis Loss : 3.863467554765521e-06\n",
      "Steps : 262100, \t Total Gen Loss : 3054.627197265625, \t Total Dis Loss : 1.7081076293834485e-05\n",
      "Steps : 262200, \t Total Gen Loss : 3600.38720703125, \t Total Dis Loss : 5.442505425889976e-06\n",
      "Steps : 262300, \t Total Gen Loss : 3380.048828125, \t Total Dis Loss : 9.479261279921047e-06\n",
      "Steps : 262400, \t Total Gen Loss : 4135.109375, \t Total Dis Loss : 9.282753126171883e-06\n",
      "Steps : 262500, \t Total Gen Loss : 3457.535400390625, \t Total Dis Loss : 5.127588156028651e-05\n",
      "Steps : 262600, \t Total Gen Loss : 3355.226318359375, \t Total Dis Loss : 3.977633605245501e-05\n",
      "Steps : 262700, \t Total Gen Loss : 3301.85888671875, \t Total Dis Loss : 3.951587495976128e-05\n",
      "Steps : 262800, \t Total Gen Loss : 3809.453369140625, \t Total Dis Loss : 3.240109890612075e-06\n",
      "Steps : 262900, \t Total Gen Loss : 3831.70361328125, \t Total Dis Loss : 1.7820397260948084e-05\n",
      "Steps : 263000, \t Total Gen Loss : 3592.102294921875, \t Total Dis Loss : 1.1430141057644505e-05\n",
      "Steps : 263100, \t Total Gen Loss : 3406.15380859375, \t Total Dis Loss : 7.057008042465895e-05\n",
      "Steps : 263200, \t Total Gen Loss : 3726.66943359375, \t Total Dis Loss : 7.01939788996242e-05\n",
      "Steps : 263300, \t Total Gen Loss : 3717.9453125, \t Total Dis Loss : 1.2754344425047748e-05\n",
      "Steps : 263400, \t Total Gen Loss : 3924.357666015625, \t Total Dis Loss : 1.0764485523395706e-05\n",
      "Steps : 263500, \t Total Gen Loss : 3509.615234375, \t Total Dis Loss : 6.293789283517981e-06\n",
      "Steps : 263600, \t Total Gen Loss : 3246.76904296875, \t Total Dis Loss : 2.269198557769414e-05\n",
      "Steps : 263700, \t Total Gen Loss : 3835.87451171875, \t Total Dis Loss : 5.923120625084266e-06\n",
      "Steps : 263800, \t Total Gen Loss : 3185.088623046875, \t Total Dis Loss : 4.970567715645302e-06\n",
      "Steps : 263900, \t Total Gen Loss : 3300.350830078125, \t Total Dis Loss : 1.6269783372990787e-05\n",
      "Steps : 264000, \t Total Gen Loss : 3769.799560546875, \t Total Dis Loss : 1.2474207323975861e-05\n",
      "Steps : 264100, \t Total Gen Loss : 2937.287841796875, \t Total Dis Loss : 0.0013620615936815739\n",
      "Steps : 264200, \t Total Gen Loss : 3957.429931640625, \t Total Dis Loss : 4.0949784306576476e-05\n",
      "Steps : 264300, \t Total Gen Loss : 3839.199951171875, \t Total Dis Loss : 0.0009206715621985495\n",
      "Time for epoch 47 is 80.37484979629517 sec\n",
      "Steps : 264400, \t Total Gen Loss : 3765.474365234375, \t Total Dis Loss : 2.9439686841215007e-05\n",
      "Steps : 264500, \t Total Gen Loss : 3991.014404296875, \t Total Dis Loss : 6.69678847771138e-05\n",
      "Steps : 264600, \t Total Gen Loss : 2910.797119140625, \t Total Dis Loss : 0.08530736714601517\n",
      "Steps : 264700, \t Total Gen Loss : 3712.670654296875, \t Total Dis Loss : 4.901581633021124e-06\n",
      "Steps : 264800, \t Total Gen Loss : 3413.907958984375, \t Total Dis Loss : 2.1090720110805705e-05\n",
      "Steps : 264900, \t Total Gen Loss : 3596.38623046875, \t Total Dis Loss : 6.618076440645382e-05\n",
      "Steps : 265000, \t Total Gen Loss : 3984.505126953125, \t Total Dis Loss : 1.5042283848742954e-05\n",
      "Steps : 265100, \t Total Gen Loss : 3821.114990234375, \t Total Dis Loss : 5.87739996262826e-05\n",
      "Steps : 265200, \t Total Gen Loss : 3551.864990234375, \t Total Dis Loss : 0.00011297158198431134\n",
      "Steps : 265300, \t Total Gen Loss : 3581.358642578125, \t Total Dis Loss : 4.770197483594529e-05\n",
      "Steps : 265400, \t Total Gen Loss : 3380.398681640625, \t Total Dis Loss : 4.90959791932255e-06\n",
      "Steps : 265500, \t Total Gen Loss : 4156.3486328125, \t Total Dis Loss : 1.6552394299651496e-05\n",
      "Steps : 265600, \t Total Gen Loss : 4148.28564453125, \t Total Dis Loss : 4.52852827947936e-06\n",
      "Steps : 265700, \t Total Gen Loss : 3632.269775390625, \t Total Dis Loss : 3.529047489791992e-06\n",
      "Steps : 265800, \t Total Gen Loss : 3700.8349609375, \t Total Dis Loss : 6.278464570641518e-05\n",
      "Steps : 265900, \t Total Gen Loss : 3416.79248046875, \t Total Dis Loss : 1.666378739173524e-05\n",
      "Steps : 266000, \t Total Gen Loss : 3723.686279296875, \t Total Dis Loss : 4.124453880649526e-06\n",
      "Steps : 266100, \t Total Gen Loss : 3921.241455078125, \t Total Dis Loss : 3.933624611818232e-06\n",
      "Steps : 266200, \t Total Gen Loss : 3991.33203125, \t Total Dis Loss : 7.3637920650071464e-06\n",
      "Steps : 266300, \t Total Gen Loss : 3583.2861328125, \t Total Dis Loss : 7.532120889663929e-06\n",
      "Steps : 266400, \t Total Gen Loss : 3795.754150390625, \t Total Dis Loss : 1.9157726001139963e-06\n",
      "Steps : 266500, \t Total Gen Loss : 3653.43310546875, \t Total Dis Loss : 4.844813156523742e-05\n",
      "Steps : 266600, \t Total Gen Loss : 3481.316650390625, \t Total Dis Loss : 3.627563046393334e-06\n",
      "Steps : 266700, \t Total Gen Loss : 3567.912841796875, \t Total Dis Loss : 2.4504352040821686e-06\n",
      "Steps : 266800, \t Total Gen Loss : 3703.40380859375, \t Total Dis Loss : 1.1768176591431256e-05\n",
      "Steps : 266900, \t Total Gen Loss : 4039.615234375, \t Total Dis Loss : 2.063168358290568e-05\n",
      "Steps : 267000, \t Total Gen Loss : 3912.08544921875, \t Total Dis Loss : 2.547160647736746e-06\n",
      "Steps : 267100, \t Total Gen Loss : 3611.35302734375, \t Total Dis Loss : 4.12820099882083e-06\n",
      "Steps : 267200, \t Total Gen Loss : 3707.626708984375, \t Total Dis Loss : 3.9802230276109185e-06\n",
      "Steps : 267300, \t Total Gen Loss : 3733.588134765625, \t Total Dis Loss : 5.444921043817885e-06\n",
      "Steps : 267400, \t Total Gen Loss : 3956.32275390625, \t Total Dis Loss : 1.0777374882309232e-05\n",
      "Steps : 267500, \t Total Gen Loss : 2884.066650390625, \t Total Dis Loss : 1.4479190213023685e-05\n",
      "Steps : 267600, \t Total Gen Loss : 3652.523681640625, \t Total Dis Loss : 2.3324541871261317e-06\n",
      "Steps : 267700, \t Total Gen Loss : 4048.21533203125, \t Total Dis Loss : 1.3924716313340468e-06\n",
      "Steps : 267800, \t Total Gen Loss : 3472.4521484375, \t Total Dis Loss : 5.5409032029274385e-06\n",
      "Steps : 267900, \t Total Gen Loss : 3918.10205078125, \t Total Dis Loss : 0.00012515585694927722\n",
      "Steps : 268000, \t Total Gen Loss : 3591.56982421875, \t Total Dis Loss : 0.00011589151836233214\n",
      "Steps : 268100, \t Total Gen Loss : 3399.73095703125, \t Total Dis Loss : 6.723245314788073e-05\n",
      "Steps : 268200, \t Total Gen Loss : 3292.03076171875, \t Total Dis Loss : 3.03920496662613e-05\n",
      "Steps : 268300, \t Total Gen Loss : 3658.535400390625, \t Total Dis Loss : 1.9431096006883308e-05\n",
      "Steps : 268400, \t Total Gen Loss : 3289.27734375, \t Total Dis Loss : 5.281150515656918e-05\n",
      "Steps : 268500, \t Total Gen Loss : 3702.9970703125, \t Total Dis Loss : 0.0005854354240000248\n",
      "Steps : 268600, \t Total Gen Loss : 4086.155517578125, \t Total Dis Loss : 7.572054892079905e-05\n",
      "Steps : 268700, \t Total Gen Loss : 3855.41748046875, \t Total Dis Loss : 1.4575576642528176e-05\n",
      "Steps : 268800, \t Total Gen Loss : 2894.756591796875, \t Total Dis Loss : 2.28090284508653e-05\n",
      "Steps : 268900, \t Total Gen Loss : 3517.839111328125, \t Total Dis Loss : 2.701102312130388e-05\n",
      "Steps : 269000, \t Total Gen Loss : 3048.789306640625, \t Total Dis Loss : 2.5601893867133185e-05\n",
      "Steps : 269100, \t Total Gen Loss : 3554.335693359375, \t Total Dis Loss : 1.85718326974893e-05\n",
      "Steps : 269200, \t Total Gen Loss : 3906.915283203125, \t Total Dis Loss : 3.156003003823571e-05\n",
      "Steps : 269300, \t Total Gen Loss : 3136.744384765625, \t Total Dis Loss : 1.258006705029402e-05\n",
      "Steps : 269400, \t Total Gen Loss : 3232.410400390625, \t Total Dis Loss : 8.847671779221855e-06\n",
      "Steps : 269500, \t Total Gen Loss : 3019.32958984375, \t Total Dis Loss : 0.00012413313379511237\n",
      "Steps : 269600, \t Total Gen Loss : 3243.5908203125, \t Total Dis Loss : 4.756919588544406e-05\n",
      "Steps : 269700, \t Total Gen Loss : 3375.246337890625, \t Total Dis Loss : 7.956669287523255e-05\n",
      "Steps : 269800, \t Total Gen Loss : 2755.76904296875, \t Total Dis Loss : 2.286785456817597e-05\n",
      "Steps : 269900, \t Total Gen Loss : 2976.297607421875, \t Total Dis Loss : 0.0074853855185210705\n",
      "Steps : 270000, \t Total Gen Loss : 4028.08740234375, \t Total Dis Loss : 3.038750492123654e-06\n",
      "Time for epoch 48 is 79.86591172218323 sec\n",
      "Steps : 270100, \t Total Gen Loss : 3474.4521484375, \t Total Dis Loss : 6.00691964791622e-06\n",
      "Steps : 270200, \t Total Gen Loss : 3133.876220703125, \t Total Dis Loss : 7.176657618401805e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 270300, \t Total Gen Loss : 3449.722900390625, \t Total Dis Loss : 3.1948341074894415e-06\n",
      "Steps : 270400, \t Total Gen Loss : 3281.55517578125, \t Total Dis Loss : 8.21344383439282e-06\n",
      "Steps : 270500, \t Total Gen Loss : 3104.78125, \t Total Dis Loss : 5.848688942933222e-06\n",
      "Steps : 270600, \t Total Gen Loss : 3539.819091796875, \t Total Dis Loss : 3.017009476025123e-06\n",
      "Steps : 270700, \t Total Gen Loss : 3553.54541015625, \t Total Dis Loss : 5.613942448690068e-06\n",
      "Steps : 270800, \t Total Gen Loss : 3256.251220703125, \t Total Dis Loss : 1.4269799976318609e-05\n",
      "Steps : 270900, \t Total Gen Loss : 3027.489013671875, \t Total Dis Loss : 2.3729057829768863e-06\n",
      "Steps : 271000, \t Total Gen Loss : 2810.135498046875, \t Total Dis Loss : 1.4844827092019841e-06\n",
      "Steps : 271100, \t Total Gen Loss : 3634.97705078125, \t Total Dis Loss : 2.045303517661523e-06\n",
      "Steps : 271200, \t Total Gen Loss : 3466.5009765625, \t Total Dis Loss : 1.1808619092334993e-05\n",
      "Steps : 271300, \t Total Gen Loss : 3437.5498046875, \t Total Dis Loss : 0.00010553719039307907\n",
      "Steps : 271400, \t Total Gen Loss : 4329.833984375, \t Total Dis Loss : 3.328045568196103e-05\n",
      "Steps : 271500, \t Total Gen Loss : 3242.02587890625, \t Total Dis Loss : 8.820795301289763e-06\n",
      "Steps : 271600, \t Total Gen Loss : 3549.679931640625, \t Total Dis Loss : 0.004367812070995569\n",
      "Steps : 271700, \t Total Gen Loss : 3888.474609375, \t Total Dis Loss : 8.021044777706265e-06\n",
      "Steps : 271800, \t Total Gen Loss : 3487.107177734375, \t Total Dis Loss : 0.0014825657708570361\n",
      "Steps : 271900, \t Total Gen Loss : 3422.1025390625, \t Total Dis Loss : 2.3735557988402434e-05\n",
      "Steps : 272000, \t Total Gen Loss : 4016.973876953125, \t Total Dis Loss : 8.510036423103884e-05\n",
      "Steps : 272100, \t Total Gen Loss : 3319.067138671875, \t Total Dis Loss : 8.164995051629376e-06\n",
      "Steps : 272200, \t Total Gen Loss : 3982.3818359375, \t Total Dis Loss : 0.0017570335185155272\n",
      "Steps : 272300, \t Total Gen Loss : 3723.37060546875, \t Total Dis Loss : 1.7664077631707187e-06\n",
      "Steps : 272400, \t Total Gen Loss : 3049.076904296875, \t Total Dis Loss : 0.00010701235441956669\n",
      "Steps : 272500, \t Total Gen Loss : 3927.264892578125, \t Total Dis Loss : 9.048336505657062e-05\n",
      "Steps : 272600, \t Total Gen Loss : 3785.17138671875, \t Total Dis Loss : 0.00010740245488705114\n",
      "Steps : 272700, \t Total Gen Loss : 3995.11962890625, \t Total Dis Loss : 7.211186311906204e-05\n",
      "Steps : 272800, \t Total Gen Loss : 3683.30419921875, \t Total Dis Loss : 6.03607804805506e-06\n",
      "Steps : 272900, \t Total Gen Loss : 3622.262939453125, \t Total Dis Loss : 5.523765503312461e-05\n",
      "Steps : 273000, \t Total Gen Loss : 3326.0185546875, \t Total Dis Loss : 0.00020376156317070127\n",
      "Steps : 273100, \t Total Gen Loss : 3284.607666015625, \t Total Dis Loss : 1.5543993868050165e-05\n",
      "Steps : 273200, \t Total Gen Loss : 4110.0869140625, \t Total Dis Loss : 1.411652920069173e-05\n",
      "Steps : 273300, \t Total Gen Loss : 3473.302734375, \t Total Dis Loss : 9.2087375378469e-06\n",
      "Steps : 273400, \t Total Gen Loss : 3481.254150390625, \t Total Dis Loss : 4.9987356760539114e-06\n",
      "Steps : 273500, \t Total Gen Loss : 3351.41357421875, \t Total Dis Loss : 3.7459542454598704e-06\n",
      "Steps : 273600, \t Total Gen Loss : 2967.7373046875, \t Total Dis Loss : 3.232676135667134e-06\n",
      "Steps : 273700, \t Total Gen Loss : 3303.9169921875, \t Total Dis Loss : 1.2567372777994024e-06\n",
      "Steps : 273800, \t Total Gen Loss : 3703.091796875, \t Total Dis Loss : 0.0017842658562585711\n",
      "Steps : 273900, \t Total Gen Loss : 3390.266357421875, \t Total Dis Loss : 1.6725336536183022e-05\n",
      "Steps : 274000, \t Total Gen Loss : 4098.1982421875, \t Total Dis Loss : 2.895309080486186e-05\n",
      "Steps : 274100, \t Total Gen Loss : 3575.21728515625, \t Total Dis Loss : 1.0940927495539654e-05\n",
      "Steps : 274200, \t Total Gen Loss : 3492.8291015625, \t Total Dis Loss : 2.1071673472761177e-06\n",
      "Steps : 274300, \t Total Gen Loss : 3114.5439453125, \t Total Dis Loss : 3.773777052629157e-06\n",
      "Steps : 274400, \t Total Gen Loss : 3649.25146484375, \t Total Dis Loss : 1.7535165170556866e-05\n",
      "Steps : 274500, \t Total Gen Loss : 3722.47021484375, \t Total Dis Loss : 7.459005246346351e-06\n",
      "Steps : 274600, \t Total Gen Loss : 3450.774658203125, \t Total Dis Loss : 1.5183166397036985e-05\n",
      "Steps : 274700, \t Total Gen Loss : 2940.49169921875, \t Total Dis Loss : 7.334346264542546e-06\n",
      "Steps : 274800, \t Total Gen Loss : 4139.85498046875, \t Total Dis Loss : 6.66064879624173e-05\n",
      "Steps : 274900, \t Total Gen Loss : 4411.9375, \t Total Dis Loss : 1.3169873454899061e-05\n",
      "Steps : 275000, \t Total Gen Loss : 3811.505615234375, \t Total Dis Loss : 7.733819984423462e-06\n",
      "Steps : 275100, \t Total Gen Loss : 3705.520751953125, \t Total Dis Loss : 9.948641491064336e-06\n",
      "Steps : 275200, \t Total Gen Loss : 4113.486328125, \t Total Dis Loss : 8.994924428407103e-06\n",
      "Steps : 275300, \t Total Gen Loss : 4025.093994140625, \t Total Dis Loss : 2.2040992462279974e-06\n",
      "Steps : 275400, \t Total Gen Loss : 2771.17626953125, \t Total Dis Loss : 3.1120412131713238e-06\n",
      "Steps : 275500, \t Total Gen Loss : 3831.649658203125, \t Total Dis Loss : 2.6914897262031445e-06\n",
      "Steps : 275600, \t Total Gen Loss : 3736.84521484375, \t Total Dis Loss : 4.1681209950183984e-07\n",
      "Time for epoch 49 is 76.63858389854431 sec\n",
      "Steps : 275700, \t Total Gen Loss : 3879.322509765625, \t Total Dis Loss : 2.9227267077658325e-06\n",
      "Steps : 275800, \t Total Gen Loss : 4134.78271484375, \t Total Dis Loss : 9.667687663750257e-07\n",
      "Steps : 275900, \t Total Gen Loss : 3439.5927734375, \t Total Dis Loss : 4.7755311243236065e-06\n",
      "Steps : 276000, \t Total Gen Loss : 3828.148193359375, \t Total Dis Loss : 1.2950773452757858e-05\n",
      "Steps : 276100, \t Total Gen Loss : 3331.192138671875, \t Total Dis Loss : 6.029162250342779e-06\n",
      "Steps : 276200, \t Total Gen Loss : 3661.6865234375, \t Total Dis Loss : 2.0228471839800477e-06\n",
      "Steps : 276300, \t Total Gen Loss : 3387.77880859375, \t Total Dis Loss : 4.901248757960275e-06\n",
      "Steps : 276400, \t Total Gen Loss : 4080.14501953125, \t Total Dis Loss : 5.587518899119459e-06\n",
      "Steps : 276500, \t Total Gen Loss : 3210.77783203125, \t Total Dis Loss : 2.202524683525553e-06\n",
      "Steps : 276600, \t Total Gen Loss : 3562.446533203125, \t Total Dis Loss : 1.927638322740677e-06\n",
      "Steps : 276700, \t Total Gen Loss : 3474.518310546875, \t Total Dis Loss : 2.2433466710936045e-06\n",
      "Steps : 276800, \t Total Gen Loss : 3120.51708984375, \t Total Dis Loss : 1.5703511735409847e-06\n",
      "Steps : 276900, \t Total Gen Loss : 3522.865234375, \t Total Dis Loss : 9.679161848907825e-06\n",
      "Steps : 277000, \t Total Gen Loss : 3638.935791015625, \t Total Dis Loss : 3.430920514801983e-06\n",
      "Steps : 277100, \t Total Gen Loss : 3286.77978515625, \t Total Dis Loss : 2.2851465928397374e-06\n",
      "Steps : 277200, \t Total Gen Loss : 4092.720458984375, \t Total Dis Loss : 6.9682828325312585e-06\n",
      "Steps : 277300, \t Total Gen Loss : 3363.099609375, \t Total Dis Loss : 9.009310815599747e-06\n",
      "Steps : 277400, \t Total Gen Loss : 3987.46533203125, \t Total Dis Loss : 0.0003117096202913672\n",
      "Steps : 277500, \t Total Gen Loss : 3498.315673828125, \t Total Dis Loss : 3.3192552564287325e-06\n",
      "Steps : 277600, \t Total Gen Loss : 3566.96923828125, \t Total Dis Loss : 9.453031566408754e-07\n",
      "Steps : 277700, \t Total Gen Loss : 3693.646484375, \t Total Dis Loss : 1.5218269027172937e-06\n",
      "Steps : 277800, \t Total Gen Loss : 3965.88671875, \t Total Dis Loss : 1.998311017814558e-05\n",
      "Steps : 277900, \t Total Gen Loss : 3821.79736328125, \t Total Dis Loss : 5.802795612908085e-07\n",
      "Steps : 278000, \t Total Gen Loss : 3560.71728515625, \t Total Dis Loss : 9.348989351565251e-07\n",
      "Steps : 278100, \t Total Gen Loss : 3828.013916015625, \t Total Dis Loss : 6.246449629543349e-06\n",
      "Steps : 278200, \t Total Gen Loss : 3706.287353515625, \t Total Dis Loss : 5.996858817525208e-06\n",
      "Steps : 278300, \t Total Gen Loss : 4280.7109375, \t Total Dis Loss : 2.0467302874749294e-06\n",
      "Steps : 278400, \t Total Gen Loss : 3725.369873046875, \t Total Dis Loss : 9.295236850448418e-06\n",
      "Steps : 278500, \t Total Gen Loss : 3357.871826171875, \t Total Dis Loss : 1.1477090083644725e-05\n",
      "Steps : 278600, \t Total Gen Loss : 3616.9267578125, \t Total Dis Loss : 1.5375076145573985e-06\n",
      "Steps : 278700, \t Total Gen Loss : 3406.663330078125, \t Total Dis Loss : 1.2248607163201086e-05\n",
      "Steps : 278800, \t Total Gen Loss : 3061.595947265625, \t Total Dis Loss : 0.0002677561715245247\n",
      "Steps : 278900, \t Total Gen Loss : 3577.305419921875, \t Total Dis Loss : 6.187327380757779e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 279000, \t Total Gen Loss : 3052.561767578125, \t Total Dis Loss : 2.0871028027613647e-05\n",
      "Steps : 279100, \t Total Gen Loss : 3621.0771484375, \t Total Dis Loss : 1.4678496881970204e-05\n",
      "Steps : 279200, \t Total Gen Loss : 3617.873779296875, \t Total Dis Loss : 7.391459803329781e-06\n",
      "Steps : 279300, \t Total Gen Loss : 3649.56689453125, \t Total Dis Loss : 1.3521350410883315e-05\n",
      "Steps : 279400, \t Total Gen Loss : 3894.73291015625, \t Total Dis Loss : 4.198327133053681e-06\n",
      "Steps : 279500, \t Total Gen Loss : 3158.45361328125, \t Total Dis Loss : 2.510285412427038e-06\n",
      "Steps : 279600, \t Total Gen Loss : 3891.5830078125, \t Total Dis Loss : 0.0001058309935615398\n",
      "Steps : 279700, \t Total Gen Loss : 3570.970947265625, \t Total Dis Loss : 4.337570499046706e-05\n",
      "Steps : 279800, \t Total Gen Loss : 4176.7216796875, \t Total Dis Loss : 2.8157855922472663e-05\n",
      "Steps : 279900, \t Total Gen Loss : 3968.724365234375, \t Total Dis Loss : 0.0002347481349715963\n",
      "Steps : 280000, \t Total Gen Loss : 3879.4287109375, \t Total Dis Loss : 0.00020561729616019875\n",
      "Steps : 280100, \t Total Gen Loss : 3707.14013671875, \t Total Dis Loss : 1.0251277672068682e-05\n",
      "Steps : 280200, \t Total Gen Loss : 2966.852294921875, \t Total Dis Loss : 2.8642323741223663e-05\n",
      "Steps : 280300, \t Total Gen Loss : 3969.239501953125, \t Total Dis Loss : 3.189429844496772e-05\n",
      "Steps : 280400, \t Total Gen Loss : 3771.699951171875, \t Total Dis Loss : 7.544205436715856e-05\n",
      "Steps : 280500, \t Total Gen Loss : 3526.254638671875, \t Total Dis Loss : 2.486343146301806e-05\n",
      "Steps : 280600, \t Total Gen Loss : 3612.42919921875, \t Total Dis Loss : 0.0006472200620919466\n",
      "Steps : 280700, \t Total Gen Loss : 3799.264404296875, \t Total Dis Loss : 2.037028389167972e-05\n",
      "Steps : 280800, \t Total Gen Loss : 3567.219970703125, \t Total Dis Loss : 2.3562799469800666e-05\n",
      "Steps : 280900, \t Total Gen Loss : 3630.208740234375, \t Total Dis Loss : 1.937700290000066e-05\n",
      "Steps : 281000, \t Total Gen Loss : 3717.595458984375, \t Total Dis Loss : 0.0005345939425751567\n",
      "Steps : 281100, \t Total Gen Loss : 3152.35546875, \t Total Dis Loss : 6.122673948993906e-05\n",
      "Steps : 281200, \t Total Gen Loss : 3336.959716796875, \t Total Dis Loss : 0.00014757242752239108\n",
      "Time for epoch 50 is 75.55270791053772 sec\n",
      "Steps : 281300, \t Total Gen Loss : 3863.82470703125, \t Total Dis Loss : 7.77387322159484e-05\n",
      "Steps : 281400, \t Total Gen Loss : 3984.267822265625, \t Total Dis Loss : 4.2680854676291347e-05\n",
      "Steps : 281500, \t Total Gen Loss : 3502.22509765625, \t Total Dis Loss : 2.214919550169725e-05\n",
      "Steps : 281600, \t Total Gen Loss : 4302.169921875, \t Total Dis Loss : 3.0803101253695786e-05\n",
      "Steps : 281700, \t Total Gen Loss : 3386.25048828125, \t Total Dis Loss : 8.302441528940108e-06\n",
      "Steps : 281800, \t Total Gen Loss : 3296.349365234375, \t Total Dis Loss : 8.57208033266943e-06\n",
      "Steps : 281900, \t Total Gen Loss : 3313.4892578125, \t Total Dis Loss : 2.4299621145473793e-05\n",
      "Steps : 282000, \t Total Gen Loss : 3793.885498046875, \t Total Dis Loss : 9.247512934962288e-06\n",
      "Steps : 282100, \t Total Gen Loss : 3725.06103515625, \t Total Dis Loss : 6.147127714939415e-05\n",
      "Steps : 282200, \t Total Gen Loss : 2956.414794921875, \t Total Dis Loss : 0.00020894460612908006\n",
      "Steps : 282300, \t Total Gen Loss : 2986.208251953125, \t Total Dis Loss : 7.4004287853313144e-06\n",
      "Steps : 282400, \t Total Gen Loss : 2643.9951171875, \t Total Dis Loss : 3.754598583327606e-05\n",
      "Steps : 282500, \t Total Gen Loss : 3698.984375, \t Total Dis Loss : 9.181375389744062e-06\n",
      "Steps : 282600, \t Total Gen Loss : 4168.02978515625, \t Total Dis Loss : 0.001392603968270123\n",
      "Steps : 282700, \t Total Gen Loss : 3691.685302734375, \t Total Dis Loss : 1.150888783740811e-05\n",
      "Steps : 282800, \t Total Gen Loss : 3316.2880859375, \t Total Dis Loss : 2.291301825607661e-06\n",
      "Steps : 282900, \t Total Gen Loss : 3331.79150390625, \t Total Dis Loss : 3.818709501501871e-06\n",
      "Steps : 283000, \t Total Gen Loss : 3312.637939453125, \t Total Dis Loss : 5.24616043549031e-05\n",
      "Steps : 283100, \t Total Gen Loss : 4168.72509765625, \t Total Dis Loss : 0.00010557690256973729\n",
      "Steps : 283200, \t Total Gen Loss : 3411.89111328125, \t Total Dis Loss : 1.6390113160014153e-05\n",
      "Steps : 283300, \t Total Gen Loss : 3047.76025390625, \t Total Dis Loss : 8.27432086225599e-05\n",
      "Steps : 283400, \t Total Gen Loss : 3302.61962890625, \t Total Dis Loss : 2.5500286938040517e-05\n",
      "Steps : 283500, \t Total Gen Loss : 3168.820556640625, \t Total Dis Loss : 5.576041803578846e-05\n",
      "Steps : 283600, \t Total Gen Loss : 3855.607421875, \t Total Dis Loss : 3.555832108759205e-06\n",
      "Steps : 283700, \t Total Gen Loss : 3695.02783203125, \t Total Dis Loss : 9.582054190104827e-06\n",
      "Steps : 283800, \t Total Gen Loss : 3610.63623046875, \t Total Dis Loss : 1.3341055819182657e-05\n",
      "Steps : 283900, \t Total Gen Loss : 3338.821044921875, \t Total Dis Loss : 1.001126383926021e-05\n",
      "Steps : 284000, \t Total Gen Loss : 4069.12939453125, \t Total Dis Loss : 3.328788443468511e-05\n",
      "Steps : 284100, \t Total Gen Loss : 3220.667724609375, \t Total Dis Loss : 2.586685013739043e-06\n",
      "Steps : 284200, \t Total Gen Loss : 3845.600341796875, \t Total Dis Loss : 0.00011362527584424242\n",
      "Steps : 284300, \t Total Gen Loss : 3160.014892578125, \t Total Dis Loss : 3.123544229310937e-05\n",
      "Steps : 284400, \t Total Gen Loss : 2919.931396484375, \t Total Dis Loss : 4.70143677375745e-05\n",
      "Steps : 284500, \t Total Gen Loss : 3272.765380859375, \t Total Dis Loss : 2.1755222405772656e-05\n",
      "Steps : 284600, \t Total Gen Loss : 3867.31982421875, \t Total Dis Loss : 5.896568109164946e-05\n",
      "Steps : 284700, \t Total Gen Loss : 3825.99951171875, \t Total Dis Loss : 1.2708826488960767e-06\n",
      "Steps : 284800, \t Total Gen Loss : 3311.43798828125, \t Total Dis Loss : 4.947678462485783e-06\n",
      "Steps : 284900, \t Total Gen Loss : 2856.802490234375, \t Total Dis Loss : 4.304948106437223e-06\n",
      "Steps : 285000, \t Total Gen Loss : 3234.77734375, \t Total Dis Loss : 1.5231606994348112e-05\n",
      "Steps : 285100, \t Total Gen Loss : 3312.46337890625, \t Total Dis Loss : 1.0527350241318345e-05\n",
      "Steps : 285200, \t Total Gen Loss : 3752.856689453125, \t Total Dis Loss : 3.943731826439034e-06\n",
      "Steps : 285300, \t Total Gen Loss : 3639.556396484375, \t Total Dis Loss : 8.105004235403612e-05\n",
      "Steps : 285400, \t Total Gen Loss : 4638.8125, \t Total Dis Loss : 1.1818056918855291e-05\n",
      "Steps : 285500, \t Total Gen Loss : 3549.77490234375, \t Total Dis Loss : 6.051189484423958e-05\n",
      "Steps : 285600, \t Total Gen Loss : 3542.26416015625, \t Total Dis Loss : 1.4447909961745609e-06\n",
      "Steps : 285700, \t Total Gen Loss : 3378.845458984375, \t Total Dis Loss : 6.6519044139568e-07\n",
      "Steps : 285800, \t Total Gen Loss : 3162.2216796875, \t Total Dis Loss : 0.0003666590782813728\n",
      "Steps : 285900, \t Total Gen Loss : 3672.275390625, \t Total Dis Loss : 5.2125171350780874e-05\n",
      "Steps : 286000, \t Total Gen Loss : 3135.707275390625, \t Total Dis Loss : 5.129050259711221e-05\n",
      "Steps : 286100, \t Total Gen Loss : 3361.363037109375, \t Total Dis Loss : 0.00013756354746874422\n",
      "Steps : 286200, \t Total Gen Loss : 3560.55419921875, \t Total Dis Loss : 2.948671863123309e-05\n",
      "Steps : 286300, \t Total Gen Loss : 2906.3681640625, \t Total Dis Loss : 0.00021559158631134778\n",
      "Steps : 286400, \t Total Gen Loss : 4162.44189453125, \t Total Dis Loss : 2.354713979002554e-05\n",
      "Steps : 286500, \t Total Gen Loss : 3782.05712890625, \t Total Dis Loss : 1.122597313951701e-05\n",
      "Steps : 286600, \t Total Gen Loss : 3762.583984375, \t Total Dis Loss : 3.643698073574342e-05\n",
      "Steps : 286700, \t Total Gen Loss : 3854.283203125, \t Total Dis Loss : 4.1060677176574245e-05\n",
      "Steps : 286800, \t Total Gen Loss : 3641.848876953125, \t Total Dis Loss : 1.3692879292648286e-05\n",
      "Time for epoch 51 is 74.99410796165466 sec\n",
      "Steps : 286900, \t Total Gen Loss : 3841.9267578125, \t Total Dis Loss : 8.599836291978136e-05\n",
      "Steps : 287000, \t Total Gen Loss : 3769.24951171875, \t Total Dis Loss : 2.2270956833381206e-05\n",
      "Steps : 287100, \t Total Gen Loss : 3712.664794921875, \t Total Dis Loss : 6.108129309723154e-05\n",
      "Steps : 287200, \t Total Gen Loss : 3273.78076171875, \t Total Dis Loss : 9.986243867388112e-07\n",
      "Steps : 287300, \t Total Gen Loss : 4144.77880859375, \t Total Dis Loss : 1.401888425789366e-06\n",
      "Steps : 287400, \t Total Gen Loss : 3666.41357421875, \t Total Dis Loss : 1.2698315913439728e-05\n",
      "Steps : 287500, \t Total Gen Loss : 2749.545166015625, \t Total Dis Loss : 6.328619747364428e-06\n",
      "Steps : 287600, \t Total Gen Loss : 3898.1142578125, \t Total Dis Loss : 5.207391041039955e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 287700, \t Total Gen Loss : 3736.59765625, \t Total Dis Loss : 1.213721020576486e-06\n",
      "Steps : 287800, \t Total Gen Loss : 3318.421875, \t Total Dis Loss : 2.4397802917519584e-05\n",
      "Steps : 287900, \t Total Gen Loss : 3753.886962890625, \t Total Dis Loss : 2.8739028493873775e-05\n",
      "Steps : 288000, \t Total Gen Loss : 3754.906982421875, \t Total Dis Loss : 1.507705201220233e-05\n",
      "Steps : 288100, \t Total Gen Loss : 3574.850341796875, \t Total Dis Loss : 1.1779293345171027e-05\n",
      "Steps : 288200, \t Total Gen Loss : 3680.416259765625, \t Total Dis Loss : 1.5771709513501264e-05\n",
      "Steps : 288300, \t Total Gen Loss : 3823.736083984375, \t Total Dis Loss : 4.319666913943365e-05\n",
      "Steps : 288400, \t Total Gen Loss : 3596.1494140625, \t Total Dis Loss : 1.602133852429688e-05\n",
      "Steps : 288500, \t Total Gen Loss : 3514.024658203125, \t Total Dis Loss : 6.652961019426584e-05\n",
      "Steps : 288600, \t Total Gen Loss : 3667.72412109375, \t Total Dis Loss : 5.352654170565074e-06\n",
      "Steps : 288700, \t Total Gen Loss : 3733.133056640625, \t Total Dis Loss : 1.8824077415047213e-05\n",
      "Steps : 288800, \t Total Gen Loss : 3538.102294921875, \t Total Dis Loss : 5.087443241791334e-06\n",
      "Steps : 288900, \t Total Gen Loss : 3342.236328125, \t Total Dis Loss : 6.18320336798206e-06\n",
      "Steps : 289000, \t Total Gen Loss : 4050.4765625, \t Total Dis Loss : 7.568809451186098e-06\n",
      "Steps : 289100, \t Total Gen Loss : 3938.390869140625, \t Total Dis Loss : 1.2358423191471957e-05\n",
      "Steps : 289200, \t Total Gen Loss : 3934.5732421875, \t Total Dis Loss : 4.0819058995111845e-06\n",
      "Steps : 289300, \t Total Gen Loss : 3310.78857421875, \t Total Dis Loss : 5.180333118914859e-06\n",
      "Steps : 289400, \t Total Gen Loss : 3874.614990234375, \t Total Dis Loss : 5.256034910416929e-06\n",
      "Steps : 289500, \t Total Gen Loss : 3641.436767578125, \t Total Dis Loss : 6.356467565638013e-06\n",
      "Steps : 289600, \t Total Gen Loss : 3750.980224609375, \t Total Dis Loss : 8.326488568854984e-06\n",
      "Steps : 289700, \t Total Gen Loss : 3933.104248046875, \t Total Dis Loss : 1.2829496881749947e-05\n",
      "Steps : 289800, \t Total Gen Loss : 4160.4326171875, \t Total Dis Loss : 3.7759200495202094e-06\n",
      "Steps : 289900, \t Total Gen Loss : 3166.0380859375, \t Total Dis Loss : 1.1421579984016716e-05\n",
      "Steps : 290000, \t Total Gen Loss : 3879.8359375, \t Total Dis Loss : 7.223173724923981e-06\n",
      "Steps : 290100, \t Total Gen Loss : 3327.93896484375, \t Total Dis Loss : 5.165374204807449e-06\n",
      "Steps : 290200, \t Total Gen Loss : 3174.1640625, \t Total Dis Loss : 3.3946657822525594e-06\n",
      "Steps : 290300, \t Total Gen Loss : 2935.54345703125, \t Total Dis Loss : 7.696526881773025e-05\n",
      "Steps : 290400, \t Total Gen Loss : 3351.567138671875, \t Total Dis Loss : 2.8230317184352316e-05\n",
      "Steps : 290500, \t Total Gen Loss : 3334.203125, \t Total Dis Loss : 2.8836911951657385e-06\n",
      "Steps : 290600, \t Total Gen Loss : 3662.83349609375, \t Total Dis Loss : 4.2931413190672174e-05\n",
      "Steps : 290700, \t Total Gen Loss : 3338.624755859375, \t Total Dis Loss : 0.0002458826929796487\n",
      "Steps : 290800, \t Total Gen Loss : 3216.9462890625, \t Total Dis Loss : 2.7665206289384514e-05\n",
      "Steps : 290900, \t Total Gen Loss : 3851.041015625, \t Total Dis Loss : 1.3955555914435536e-05\n",
      "Steps : 291000, \t Total Gen Loss : 4187.03271484375, \t Total Dis Loss : 7.718163396930322e-05\n",
      "Steps : 291100, \t Total Gen Loss : 3642.04150390625, \t Total Dis Loss : 4.1016221075551584e-05\n",
      "Steps : 291200, \t Total Gen Loss : 3263.666259765625, \t Total Dis Loss : 1.635958869883325e-05\n",
      "Steps : 291300, \t Total Gen Loss : 3844.825439453125, \t Total Dis Loss : 7.317283598240465e-05\n",
      "Steps : 291400, \t Total Gen Loss : 3668.99951171875, \t Total Dis Loss : 1.3804976333631203e-05\n",
      "Steps : 291500, \t Total Gen Loss : 4181.19482421875, \t Total Dis Loss : 3.287553045083769e-05\n",
      "Steps : 291600, \t Total Gen Loss : 3267.93408203125, \t Total Dis Loss : 0.0008804588578641415\n",
      "Steps : 291700, \t Total Gen Loss : 3623.527099609375, \t Total Dis Loss : 5.798499478260055e-05\n",
      "Steps : 291800, \t Total Gen Loss : 3626.382080078125, \t Total Dis Loss : 2.051798401225824e-05\n",
      "Steps : 291900, \t Total Gen Loss : 3751.38232421875, \t Total Dis Loss : 7.312413345061941e-06\n",
      "Steps : 292000, \t Total Gen Loss : 3581.38330078125, \t Total Dis Loss : 1.0657086022547446e-05\n",
      "Steps : 292100, \t Total Gen Loss : 3780.172119140625, \t Total Dis Loss : 4.467761391424574e-05\n",
      "Steps : 292200, \t Total Gen Loss : 3813.87060546875, \t Total Dis Loss : 6.128247332526371e-05\n",
      "Steps : 292300, \t Total Gen Loss : 3623.27001953125, \t Total Dis Loss : 7.798767182976007e-05\n",
      "Steps : 292400, \t Total Gen Loss : 3197.00244140625, \t Total Dis Loss : 0.00014206598279997706\n",
      "Steps : 292500, \t Total Gen Loss : 4119.060546875, \t Total Dis Loss : 6.101823601056822e-06\n",
      "Time for epoch 52 is 76.23332691192627 sec\n",
      "Steps : 292600, \t Total Gen Loss : 3617.763671875, \t Total Dis Loss : 6.142023266875185e-06\n",
      "Steps : 292700, \t Total Gen Loss : 3961.331298828125, \t Total Dis Loss : 0.00014591756917070597\n",
      "Steps : 292800, \t Total Gen Loss : 3664.451171875, \t Total Dis Loss : 5.482096639752854e-06\n",
      "Steps : 292900, \t Total Gen Loss : 3226.74755859375, \t Total Dis Loss : 8.722720849618781e-06\n",
      "Steps : 293000, \t Total Gen Loss : 3944.800537109375, \t Total Dis Loss : 3.334506618557498e-05\n",
      "Steps : 293100, \t Total Gen Loss : 3423.18359375, \t Total Dis Loss : 2.328715163457673e-05\n",
      "Steps : 293200, \t Total Gen Loss : 4142.43115234375, \t Total Dis Loss : 5.127950316818897e-06\n",
      "Steps : 293300, \t Total Gen Loss : 3542.742431640625, \t Total Dis Loss : 1.3058055628789589e-05\n",
      "Steps : 293400, \t Total Gen Loss : 3592.37255859375, \t Total Dis Loss : 6.675058102700859e-05\n",
      "Steps : 293500, \t Total Gen Loss : 3437.952880859375, \t Total Dis Loss : 3.304021811345592e-05\n",
      "Steps : 293600, \t Total Gen Loss : 3823.090576171875, \t Total Dis Loss : 5.249011155683547e-05\n",
      "Steps : 293700, \t Total Gen Loss : 3806.393310546875, \t Total Dis Loss : 2.0464609406189993e-05\n",
      "Steps : 293800, \t Total Gen Loss : 4214.82373046875, \t Total Dis Loss : 1.2218059964652639e-05\n",
      "Steps : 293900, \t Total Gen Loss : 3290.055419921875, \t Total Dis Loss : 1.8061338778352365e-05\n",
      "Steps : 294000, \t Total Gen Loss : 3815.8447265625, \t Total Dis Loss : 7.798128353897482e-05\n",
      "Steps : 294100, \t Total Gen Loss : 2984.400634765625, \t Total Dis Loss : 3.1475698051508516e-05\n",
      "Steps : 294200, \t Total Gen Loss : 3288.245361328125, \t Total Dis Loss : 1.8128479496226646e-05\n",
      "Steps : 294300, \t Total Gen Loss : 3501.341552734375, \t Total Dis Loss : 9.190583114104811e-06\n",
      "Steps : 294400, \t Total Gen Loss : 3686.585205078125, \t Total Dis Loss : 6.394454976543784e-05\n",
      "Steps : 294500, \t Total Gen Loss : 3275.42138671875, \t Total Dis Loss : 8.215533853217494e-06\n",
      "Steps : 294600, \t Total Gen Loss : 3613.90966796875, \t Total Dis Loss : 1.8362310584052466e-05\n",
      "Steps : 294700, \t Total Gen Loss : 4297.50341796875, \t Total Dis Loss : 9.243155363947153e-05\n",
      "Steps : 294800, \t Total Gen Loss : 3451.447265625, \t Total Dis Loss : 1.6676274753990583e-05\n",
      "Steps : 294900, \t Total Gen Loss : 3423.467529296875, \t Total Dis Loss : 2.198602305725217e-05\n",
      "Steps : 295000, \t Total Gen Loss : 3517.985107421875, \t Total Dis Loss : 3.883510362356901e-05\n",
      "Steps : 295100, \t Total Gen Loss : 3753.451171875, \t Total Dis Loss : 2.8989437851123512e-05\n",
      "Steps : 295200, \t Total Gen Loss : 3576.877197265625, \t Total Dis Loss : 1.1789141353801824e-05\n",
      "Steps : 295300, \t Total Gen Loss : 4312.22314453125, \t Total Dis Loss : 1.8485316104488447e-05\n",
      "Steps : 295400, \t Total Gen Loss : 4531.3203125, \t Total Dis Loss : 4.3201221160416026e-06\n",
      "Steps : 295500, \t Total Gen Loss : 3393.24267578125, \t Total Dis Loss : 4.253306087775854e-06\n",
      "Steps : 295600, \t Total Gen Loss : 3997.451416015625, \t Total Dis Loss : 1.765946944942698e-05\n",
      "Steps : 295700, \t Total Gen Loss : 3731.5810546875, \t Total Dis Loss : 7.0550536293012556e-06\n",
      "Steps : 295800, \t Total Gen Loss : 3284.656005859375, \t Total Dis Loss : 7.968764293764252e-06\n",
      "Steps : 295900, \t Total Gen Loss : 3647.23583984375, \t Total Dis Loss : 1.3296046745381318e-05\n",
      "Steps : 296000, \t Total Gen Loss : 3036.288818359375, \t Total Dis Loss : 5.611778306047199e-06\n",
      "Steps : 296100, \t Total Gen Loss : 3849.083984375, \t Total Dis Loss : 1.3365294762479607e-05\n",
      "Steps : 296200, \t Total Gen Loss : 3722.624267578125, \t Total Dis Loss : 2.2453600649896543e-06\n",
      "Steps : 296300, \t Total Gen Loss : 3050.188720703125, \t Total Dis Loss : 1.006586717267055e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 296400, \t Total Gen Loss : 3788.266845703125, \t Total Dis Loss : 6.728426797053544e-06\n",
      "Steps : 296500, \t Total Gen Loss : 3434.521240234375, \t Total Dis Loss : 9.402148862136528e-06\n",
      "Steps : 296600, \t Total Gen Loss : 3494.05712890625, \t Total Dis Loss : 9.14953307074029e-06\n",
      "Steps : 296700, \t Total Gen Loss : 3343.838623046875, \t Total Dis Loss : 1.3228915122454055e-05\n",
      "Steps : 296800, \t Total Gen Loss : 4278.611328125, \t Total Dis Loss : 8.034285201574676e-06\n",
      "Steps : 296900, \t Total Gen Loss : 3455.180419921875, \t Total Dis Loss : 9.682204836281016e-05\n",
      "Steps : 297000, \t Total Gen Loss : 4154.06396484375, \t Total Dis Loss : 4.175519734417321e-06\n",
      "Steps : 297100, \t Total Gen Loss : 4140.7802734375, \t Total Dis Loss : 2.975697725560167e-06\n",
      "Steps : 297200, \t Total Gen Loss : 3512.800048828125, \t Total Dis Loss : 1.1268140042375308e-05\n",
      "Steps : 297300, \t Total Gen Loss : 3691.185302734375, \t Total Dis Loss : 2.2598740542889573e-05\n",
      "Steps : 297400, \t Total Gen Loss : 4042.251220703125, \t Total Dis Loss : 1.3320945072337054e-05\n",
      "Steps : 297500, \t Total Gen Loss : 3757.50048828125, \t Total Dis Loss : 9.627421604818664e-06\n",
      "Steps : 297600, \t Total Gen Loss : 3997.36279296875, \t Total Dis Loss : 3.701333525896189e-06\n",
      "Steps : 297700, \t Total Gen Loss : 3452.91357421875, \t Total Dis Loss : 0.00250593782402575\n",
      "Steps : 297800, \t Total Gen Loss : 3644.7783203125, \t Total Dis Loss : 8.271170372609049e-05\n",
      "Steps : 297900, \t Total Gen Loss : 3807.62890625, \t Total Dis Loss : 5.989454166410724e-06\n",
      "Steps : 298000, \t Total Gen Loss : 3804.569091796875, \t Total Dis Loss : 0.002125459024682641\n",
      "Steps : 298100, \t Total Gen Loss : 3617.952880859375, \t Total Dis Loss : 1.4034443665877916e-05\n",
      "Time for epoch 53 is 76.21445226669312 sec\n",
      "Steps : 298200, \t Total Gen Loss : 3996.465087890625, \t Total Dis Loss : 8.62959677760955e-06\n",
      "Steps : 298300, \t Total Gen Loss : 2919.537353515625, \t Total Dis Loss : 3.064875636482611e-05\n",
      "Steps : 298400, \t Total Gen Loss : 3304.043701171875, \t Total Dis Loss : 0.0009453202947042882\n",
      "Steps : 298500, \t Total Gen Loss : 3415.92431640625, \t Total Dis Loss : 2.1330173694877885e-05\n",
      "Steps : 298600, \t Total Gen Loss : 3952.408203125, \t Total Dis Loss : 3.754113640752621e-05\n",
      "Steps : 298700, \t Total Gen Loss : 3049.49462890625, \t Total Dis Loss : 8.18149510450894e-06\n",
      "Steps : 298800, \t Total Gen Loss : 4110.26123046875, \t Total Dis Loss : 1.6310725186485797e-05\n",
      "Steps : 298900, \t Total Gen Loss : 3618.794189453125, \t Total Dis Loss : 5.515317388926633e-05\n",
      "Steps : 299000, \t Total Gen Loss : 3791.523193359375, \t Total Dis Loss : 0.0002839104854501784\n",
      "Steps : 299100, \t Total Gen Loss : 3864.761962890625, \t Total Dis Loss : 9.611080167815089e-05\n",
      "Steps : 299200, \t Total Gen Loss : 3799.675048828125, \t Total Dis Loss : 8.400214028370101e-06\n",
      "Steps : 299300, \t Total Gen Loss : 4090.132568359375, \t Total Dis Loss : 1.7343392755719833e-05\n",
      "Steps : 299400, \t Total Gen Loss : 3569.957763671875, \t Total Dis Loss : 2.896856858569663e-05\n",
      "Steps : 299500, \t Total Gen Loss : 3243.756591796875, \t Total Dis Loss : 1.3197712178225629e-05\n",
      "Steps : 299600, \t Total Gen Loss : 3305.44091796875, \t Total Dis Loss : 4.260668720235117e-05\n",
      "Steps : 299700, \t Total Gen Loss : 3554.95654296875, \t Total Dis Loss : 2.6677403184294235e-06\n",
      "Steps : 299800, \t Total Gen Loss : 3487.132080078125, \t Total Dis Loss : 4.62750977021642e-05\n",
      "Steps : 299900, \t Total Gen Loss : 3357.484619140625, \t Total Dis Loss : 0.00011092077329521999\n",
      "Steps : 300000, \t Total Gen Loss : 3208.268798828125, \t Total Dis Loss : 0.0001630338520044461\n",
      "Steps : 300100, \t Total Gen Loss : 3790.8955078125, \t Total Dis Loss : 2.763590237009339e-05\n",
      "Steps : 300200, \t Total Gen Loss : 3074.862060546875, \t Total Dis Loss : 7.503588221879909e-06\n",
      "Steps : 300300, \t Total Gen Loss : 3668.581787109375, \t Total Dis Loss : 5.009521919419058e-05\n",
      "Steps : 300400, \t Total Gen Loss : 3703.963623046875, \t Total Dis Loss : 1.85229964699829e-05\n",
      "Steps : 300500, \t Total Gen Loss : 3537.4384765625, \t Total Dis Loss : 5.419605804490857e-05\n",
      "Steps : 300600, \t Total Gen Loss : 3941.156005859375, \t Total Dis Loss : 1.512092330813175e-05\n",
      "Steps : 300700, \t Total Gen Loss : 3527.49755859375, \t Total Dis Loss : 1.603165037522558e-05\n",
      "Steps : 300800, \t Total Gen Loss : 3292.673583984375, \t Total Dis Loss : 5.780006631539436e-06\n",
      "Steps : 300900, \t Total Gen Loss : 3131.239501953125, \t Total Dis Loss : 0.00010822479089256376\n",
      "Steps : 301000, \t Total Gen Loss : 3176.814208984375, \t Total Dis Loss : 9.448880518903024e-06\n",
      "Steps : 301100, \t Total Gen Loss : 3679.91552734375, \t Total Dis Loss : 0.00011288059613434598\n",
      "Steps : 301200, \t Total Gen Loss : 4312.939453125, \t Total Dis Loss : 5.811283244838705e-06\n",
      "Steps : 301300, \t Total Gen Loss : 3037.46142578125, \t Total Dis Loss : 4.15518934460124e-06\n",
      "Steps : 301400, \t Total Gen Loss : 3319.444091796875, \t Total Dis Loss : 8.95174980541924e-06\n",
      "Steps : 301500, \t Total Gen Loss : 3255.232421875, \t Total Dis Loss : 9.829052032728214e-06\n",
      "Steps : 301600, \t Total Gen Loss : 3887.86328125, \t Total Dis Loss : 3.6508710763882846e-05\n",
      "Steps : 301700, \t Total Gen Loss : 3249.573486328125, \t Total Dis Loss : 9.75010061665671e-06\n",
      "Steps : 301800, \t Total Gen Loss : 3009.84130859375, \t Total Dis Loss : 1.3552582458942197e-05\n",
      "Steps : 301900, \t Total Gen Loss : 3643.340576171875, \t Total Dis Loss : 7.275190910149831e-06\n",
      "Steps : 302000, \t Total Gen Loss : 3840.42236328125, \t Total Dis Loss : 1.8554455891717225e-05\n",
      "Steps : 302100, \t Total Gen Loss : 3250.649169921875, \t Total Dis Loss : 1.4562003798346268e-06\n",
      "Steps : 302200, \t Total Gen Loss : 3214.506591796875, \t Total Dis Loss : 2.655090793268755e-05\n",
      "Steps : 302300, \t Total Gen Loss : 3459.372802734375, \t Total Dis Loss : 0.00013966816186439246\n",
      "Steps : 302400, \t Total Gen Loss : 3366.823974609375, \t Total Dis Loss : 8.496132863911043e-07\n",
      "Steps : 302500, \t Total Gen Loss : 3768.529296875, \t Total Dis Loss : 2.2406791231333045e-06\n",
      "Steps : 302600, \t Total Gen Loss : 3423.8662109375, \t Total Dis Loss : 4.879408606939251e-06\n",
      "Steps : 302700, \t Total Gen Loss : 3743.7021484375, \t Total Dis Loss : 0.00030792041798122227\n",
      "Steps : 302800, \t Total Gen Loss : 3674.698974609375, \t Total Dis Loss : 5.992502337903716e-06\n",
      "Steps : 302900, \t Total Gen Loss : 3681.279052734375, \t Total Dis Loss : 3.579381882445887e-05\n",
      "Steps : 303000, \t Total Gen Loss : 3220.1044921875, \t Total Dis Loss : 1.1529264156706631e-05\n",
      "Steps : 303100, \t Total Gen Loss : 3887.1845703125, \t Total Dis Loss : 1.7680079054116504e-06\n",
      "Steps : 303200, \t Total Gen Loss : 4019.39013671875, \t Total Dis Loss : 3.63185718015302e-05\n",
      "Steps : 303300, \t Total Gen Loss : 3354.513671875, \t Total Dis Loss : 5.763693479821086e-06\n",
      "Steps : 303400, \t Total Gen Loss : 3703.134521484375, \t Total Dis Loss : 8.681630788487382e-06\n",
      "Steps : 303500, \t Total Gen Loss : 3535.28515625, \t Total Dis Loss : 3.319397001178004e-05\n",
      "Steps : 303600, \t Total Gen Loss : 4000.585693359375, \t Total Dis Loss : 0.00023631160729564726\n",
      "Steps : 303700, \t Total Gen Loss : 3632.479248046875, \t Total Dis Loss : 0.0002801504160743207\n",
      "Time for epoch 54 is 75.72145462036133 sec\n",
      "Steps : 303800, \t Total Gen Loss : 3650.8603515625, \t Total Dis Loss : 1.8446149624651298e-05\n",
      "Steps : 303900, \t Total Gen Loss : 4096.2890625, \t Total Dis Loss : 3.1146450055530295e-05\n",
      "Steps : 304000, \t Total Gen Loss : 3880.17041015625, \t Total Dis Loss : 2.9301559152372647e-06\n",
      "Steps : 304100, \t Total Gen Loss : 4074.368408203125, \t Total Dis Loss : 1.0357456631027162e-05\n",
      "Steps : 304200, \t Total Gen Loss : 3675.001708984375, \t Total Dis Loss : 3.7536942727456335e-06\n",
      "Steps : 304300, \t Total Gen Loss : 3718.555908203125, \t Total Dis Loss : 9.956887879525311e-06\n",
      "Steps : 304400, \t Total Gen Loss : 3623.4736328125, \t Total Dis Loss : 2.44114312408783e-06\n",
      "Steps : 304500, \t Total Gen Loss : 3828.542236328125, \t Total Dis Loss : 3.121885129075963e-06\n",
      "Steps : 304600, \t Total Gen Loss : 3635.911865234375, \t Total Dis Loss : 3.985920557170175e-06\n",
      "Steps : 304700, \t Total Gen Loss : 3520.378662109375, \t Total Dis Loss : 2.3118539047572995e-06\n",
      "Steps : 304800, \t Total Gen Loss : 3976.456787109375, \t Total Dis Loss : 2.016931102843955e-05\n",
      "Steps : 304900, \t Total Gen Loss : 3646.774658203125, \t Total Dis Loss : 0.0038795024156570435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 305000, \t Total Gen Loss : 3397.722900390625, \t Total Dis Loss : 2.6557470846455544e-05\n",
      "Steps : 305100, \t Total Gen Loss : 3386.1796875, \t Total Dis Loss : 2.514807965781074e-05\n",
      "Steps : 305200, \t Total Gen Loss : 3824.17041015625, \t Total Dis Loss : 4.015918239019811e-05\n",
      "Steps : 305300, \t Total Gen Loss : 3613.599853515625, \t Total Dis Loss : 0.00010004745854530483\n",
      "Steps : 305400, \t Total Gen Loss : 3448.83154296875, \t Total Dis Loss : 2.325182140339166e-05\n",
      "Steps : 305500, \t Total Gen Loss : 3438.25244140625, \t Total Dis Loss : 0.0002003056142712012\n",
      "Steps : 305600, \t Total Gen Loss : 3300.414306640625, \t Total Dis Loss : 7.402255505439825e-06\n",
      "Steps : 305700, \t Total Gen Loss : 3615.91357421875, \t Total Dis Loss : 8.352870281669311e-06\n",
      "Steps : 305800, \t Total Gen Loss : 3288.88134765625, \t Total Dis Loss : 1.2382613476802362e-06\n",
      "Steps : 305900, \t Total Gen Loss : 3417.292236328125, \t Total Dis Loss : 2.2086619537731167e-06\n",
      "Steps : 306000, \t Total Gen Loss : 3682.55126953125, \t Total Dis Loss : 9.123584277404007e-06\n",
      "Steps : 306100, \t Total Gen Loss : 3483.472900390625, \t Total Dis Loss : 7.161089797591558e-06\n",
      "Steps : 306200, \t Total Gen Loss : 3550.701171875, \t Total Dis Loss : 5.826194683322683e-05\n",
      "Steps : 306300, \t Total Gen Loss : 3379.628173828125, \t Total Dis Loss : 0.00026608811458572745\n",
      "Steps : 306400, \t Total Gen Loss : 3431.601318359375, \t Total Dis Loss : 1.4125510460871737e-05\n",
      "Steps : 306500, \t Total Gen Loss : 3423.083984375, \t Total Dis Loss : 4.5204062189441174e-05\n",
      "Steps : 306600, \t Total Gen Loss : 3406.131103515625, \t Total Dis Loss : 1.4343160728458315e-05\n",
      "Steps : 306700, \t Total Gen Loss : 3561.16650390625, \t Total Dis Loss : 0.00013554956240113825\n",
      "Steps : 306800, \t Total Gen Loss : 3603.053466796875, \t Total Dis Loss : 0.0006108006346039474\n",
      "Steps : 306900, \t Total Gen Loss : 3896.01318359375, \t Total Dis Loss : 0.00016700128617230803\n",
      "Steps : 307000, \t Total Gen Loss : 3973.9658203125, \t Total Dis Loss : 3.228489731554873e-05\n",
      "Steps : 307100, \t Total Gen Loss : 3700.3642578125, \t Total Dis Loss : 1.2030785001115873e-05\n",
      "Steps : 307200, \t Total Gen Loss : 4078.219970703125, \t Total Dis Loss : 2.465609577484429e-05\n",
      "Steps : 307300, \t Total Gen Loss : 3613.66259765625, \t Total Dis Loss : 0.00010162264516111463\n",
      "Steps : 307400, \t Total Gen Loss : 3616.692138671875, \t Total Dis Loss : 3.5306020436109975e-06\n",
      "Steps : 307500, \t Total Gen Loss : 3525.099609375, \t Total Dis Loss : 1.9792163584497757e-05\n",
      "Steps : 307600, \t Total Gen Loss : 3359.32568359375, \t Total Dis Loss : 1.790281203284394e-05\n",
      "Steps : 307700, \t Total Gen Loss : 3114.587646484375, \t Total Dis Loss : 2.5054734578588977e-05\n",
      "Steps : 307800, \t Total Gen Loss : 3443.927001953125, \t Total Dis Loss : 4.737044946523383e-05\n",
      "Steps : 307900, \t Total Gen Loss : 3938.11376953125, \t Total Dis Loss : 6.17099431110546e-05\n",
      "Steps : 308000, \t Total Gen Loss : 3392.5986328125, \t Total Dis Loss : 1.9084327504970133e-05\n",
      "Steps : 308100, \t Total Gen Loss : 3341.736572265625, \t Total Dis Loss : 6.287414726102725e-05\n",
      "Steps : 308200, \t Total Gen Loss : 3041.859130859375, \t Total Dis Loss : 4.709904897026718e-05\n",
      "Steps : 308300, \t Total Gen Loss : 3637.986572265625, \t Total Dis Loss : 3.983639999205479e-06\n",
      "Steps : 308400, \t Total Gen Loss : 3688.664794921875, \t Total Dis Loss : 1.6890078768483363e-05\n",
      "Steps : 308500, \t Total Gen Loss : 4182.12158203125, \t Total Dis Loss : 1.262178375327494e-05\n",
      "Steps : 308600, \t Total Gen Loss : 3389.147216796875, \t Total Dis Loss : 0.0001454546145396307\n",
      "Steps : 308700, \t Total Gen Loss : 3649.93115234375, \t Total Dis Loss : 0.0014792680740356445\n",
      "Steps : 308800, \t Total Gen Loss : 3476.141357421875, \t Total Dis Loss : 2.803225470415782e-06\n",
      "Steps : 308900, \t Total Gen Loss : 3457.33447265625, \t Total Dis Loss : 3.39809025717841e-06\n",
      "Steps : 309000, \t Total Gen Loss : 3130.861083984375, \t Total Dis Loss : 2.6770014756039018e-06\n",
      "Steps : 309100, \t Total Gen Loss : 3677.578125, \t Total Dis Loss : 0.001885796431452036\n",
      "Steps : 309200, \t Total Gen Loss : 4033.23486328125, \t Total Dis Loss : 9.557520570524503e-06\n",
      "Steps : 309300, \t Total Gen Loss : 3594.315185546875, \t Total Dis Loss : 2.732761095103342e-05\n",
      "Time for epoch 55 is 76.5344831943512 sec\n",
      "Steps : 309400, \t Total Gen Loss : 3683.514892578125, \t Total Dis Loss : 8.109432201308664e-06\n",
      "Steps : 309500, \t Total Gen Loss : 3672.948486328125, \t Total Dis Loss : 0.00023392419097945094\n",
      "Steps : 309600, \t Total Gen Loss : 4098.34375, \t Total Dis Loss : 2.6090961910085753e-05\n",
      "Steps : 309700, \t Total Gen Loss : 3427.2333984375, \t Total Dis Loss : 8.042750778258778e-06\n",
      "Steps : 309800, \t Total Gen Loss : 3663.234619140625, \t Total Dis Loss : 6.8051463131268974e-06\n",
      "Steps : 309900, \t Total Gen Loss : 3914.248046875, \t Total Dis Loss : 3.3388339943485335e-05\n",
      "Steps : 310000, \t Total Gen Loss : 3874.654052734375, \t Total Dis Loss : 0.0001556072529638186\n",
      "Steps : 310100, \t Total Gen Loss : 3547.4306640625, \t Total Dis Loss : 2.0563371435855515e-05\n",
      "Steps : 310200, \t Total Gen Loss : 3621.89501953125, \t Total Dis Loss : 5.4410880693467334e-05\n",
      "Steps : 310300, \t Total Gen Loss : 3223.942138671875, \t Total Dis Loss : 9.162888090941124e-06\n",
      "Steps : 310400, \t Total Gen Loss : 3001.146240234375, \t Total Dis Loss : 0.05041923001408577\n",
      "Steps : 310500, \t Total Gen Loss : 3846.235595703125, \t Total Dis Loss : 2.1625715817208402e-05\n",
      "Steps : 310600, \t Total Gen Loss : 3353.76513671875, \t Total Dis Loss : 1.106151376006892e-05\n",
      "Steps : 310700, \t Total Gen Loss : 3172.956787109375, \t Total Dis Loss : 1.789371708582621e-05\n",
      "Steps : 310800, \t Total Gen Loss : 3497.906982421875, \t Total Dis Loss : 2.5936069505405612e-05\n",
      "Steps : 310900, \t Total Gen Loss : 3460.395751953125, \t Total Dis Loss : 1.1847572750411928e-05\n",
      "Steps : 311000, \t Total Gen Loss : 2770.11669921875, \t Total Dis Loss : 8.744848855712917e-06\n",
      "Steps : 311100, \t Total Gen Loss : 3195.69580078125, \t Total Dis Loss : 6.590266821149271e-06\n",
      "Steps : 311200, \t Total Gen Loss : 3865.285888671875, \t Total Dis Loss : 3.0229835829231888e-05\n",
      "Steps : 311300, \t Total Gen Loss : 3230.788330078125, \t Total Dis Loss : 6.708559340040665e-06\n",
      "Steps : 311400, \t Total Gen Loss : 3799.600341796875, \t Total Dis Loss : 2.950776251964271e-05\n",
      "Steps : 311500, \t Total Gen Loss : 4188.8203125, \t Total Dis Loss : 4.1061584852286614e-06\n",
      "Steps : 311600, \t Total Gen Loss : 3707.56201171875, \t Total Dis Loss : 1.0754411050584167e-05\n",
      "Steps : 311700, \t Total Gen Loss : 3326.302490234375, \t Total Dis Loss : 2.816664164129179e-06\n",
      "Steps : 311800, \t Total Gen Loss : 3347.41552734375, \t Total Dis Loss : 6.9660954977734946e-06\n",
      "Steps : 311900, \t Total Gen Loss : 3916.224609375, \t Total Dis Loss : 5.748244802816771e-06\n",
      "Steps : 312000, \t Total Gen Loss : 3422.2861328125, \t Total Dis Loss : 1.3409342500381172e-05\n",
      "Steps : 312100, \t Total Gen Loss : 3311.419189453125, \t Total Dis Loss : 8.899485692381859e-06\n",
      "Steps : 312200, \t Total Gen Loss : 3621.532470703125, \t Total Dis Loss : 4.4339894884615205e-06\n",
      "Steps : 312300, \t Total Gen Loss : 3873.533203125, \t Total Dis Loss : 8.064898429438472e-05\n",
      "Steps : 312400, \t Total Gen Loss : 3433.2783203125, \t Total Dis Loss : 1.6241720004472882e-06\n",
      "Steps : 312500, \t Total Gen Loss : 4033.21923828125, \t Total Dis Loss : 3.395908606762532e-06\n",
      "Steps : 312600, \t Total Gen Loss : 3776.273193359375, \t Total Dis Loss : 4.480816642171703e-06\n",
      "Steps : 312700, \t Total Gen Loss : 3350.569580078125, \t Total Dis Loss : 3.322819611639716e-05\n",
      "Steps : 312800, \t Total Gen Loss : 3386.067138671875, \t Total Dis Loss : 1.3602601939055603e-05\n",
      "Steps : 312900, \t Total Gen Loss : 3076.103271484375, \t Total Dis Loss : 1.0243067663395777e-05\n",
      "Steps : 313000, \t Total Gen Loss : 3819.717529296875, \t Total Dis Loss : 6.052759999874979e-06\n",
      "Steps : 313100, \t Total Gen Loss : 3583.544189453125, \t Total Dis Loss : 8.605772563896608e-06\n",
      "Steps : 313200, \t Total Gen Loss : 3774.935791015625, \t Total Dis Loss : 5.093640993436566e-06\n",
      "Steps : 313300, \t Total Gen Loss : 3279.899169921875, \t Total Dis Loss : 0.00018426931637804955\n",
      "Steps : 313400, \t Total Gen Loss : 3701.861572265625, \t Total Dis Loss : 8.325705130118877e-05\n",
      "Steps : 313500, \t Total Gen Loss : 3352.163818359375, \t Total Dis Loss : 0.00010939573985524476\n",
      "Steps : 313600, \t Total Gen Loss : 3968.9443359375, \t Total Dis Loss : 5.649233571602963e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 313700, \t Total Gen Loss : 4166.97998046875, \t Total Dis Loss : 2.1127361833350733e-06\n",
      "Steps : 313800, \t Total Gen Loss : 3739.652099609375, \t Total Dis Loss : 8.453893315163441e-06\n",
      "Steps : 313900, \t Total Gen Loss : 3732.22314453125, \t Total Dis Loss : 1.7842343368101865e-05\n",
      "Steps : 314000, \t Total Gen Loss : 3757.844970703125, \t Total Dis Loss : 5.424276241683401e-05\n",
      "Steps : 314100, \t Total Gen Loss : 3567.927734375, \t Total Dis Loss : 0.00028210494201630354\n",
      "Steps : 314200, \t Total Gen Loss : 3825.6376953125, \t Total Dis Loss : 2.6093952328665182e-05\n",
      "Steps : 314300, \t Total Gen Loss : 3611.4609375, \t Total Dis Loss : 2.41301331698196e-05\n",
      "Steps : 314400, \t Total Gen Loss : 3314.6748046875, \t Total Dis Loss : 3.645562537712976e-05\n",
      "Steps : 314500, \t Total Gen Loss : 3515.41796875, \t Total Dis Loss : 6.889607175253332e-05\n",
      "Steps : 314600, \t Total Gen Loss : 3600.0205078125, \t Total Dis Loss : 7.341639616242901e-07\n",
      "Steps : 314700, \t Total Gen Loss : 3452.467041015625, \t Total Dis Loss : 5.344696091924561e-06\n",
      "Steps : 314800, \t Total Gen Loss : 3359.86767578125, \t Total Dis Loss : 6.830085567344213e-06\n",
      "Steps : 314900, \t Total Gen Loss : 3491.436767578125, \t Total Dis Loss : 4.519963840721175e-05\n",
      "Steps : 315000, \t Total Gen Loss : 3234.26806640625, \t Total Dis Loss : 2.7843987481901422e-05\n",
      "Time for epoch 56 is 75.54018092155457 sec\n",
      "Steps : 315100, \t Total Gen Loss : 3897.96533203125, \t Total Dis Loss : 9.372487511427607e-06\n",
      "Steps : 315200, \t Total Gen Loss : 4214.31494140625, \t Total Dis Loss : 7.539269972767215e-06\n",
      "Steps : 315300, \t Total Gen Loss : 3933.89794921875, \t Total Dis Loss : 1.1362594705133233e-05\n",
      "Steps : 315400, \t Total Gen Loss : 3170.8125, \t Total Dis Loss : 6.061737076379359e-05\n",
      "Steps : 315500, \t Total Gen Loss : 3601.7529296875, \t Total Dis Loss : 4.2821458919206634e-05\n",
      "Steps : 315600, \t Total Gen Loss : 3247.450927734375, \t Total Dis Loss : 2.706507257244084e-05\n",
      "Steps : 315700, \t Total Gen Loss : 3685.194091796875, \t Total Dis Loss : 7.937671762192622e-05\n",
      "Steps : 315800, \t Total Gen Loss : 3179.623291015625, \t Total Dis Loss : 1.65220280905487e-05\n",
      "Steps : 315900, \t Total Gen Loss : 3619.121826171875, \t Total Dis Loss : 0.0003840622375719249\n",
      "Steps : 316000, \t Total Gen Loss : 3310.052490234375, \t Total Dis Loss : 0.00010483268124517053\n",
      "Steps : 316100, \t Total Gen Loss : 3483.784423828125, \t Total Dis Loss : 2.1736123017035425e-05\n",
      "Steps : 316200, \t Total Gen Loss : 3730.446533203125, \t Total Dis Loss : 6.687302084174007e-05\n",
      "Steps : 316300, \t Total Gen Loss : 3618.834228515625, \t Total Dis Loss : 3.0382603654288687e-05\n",
      "Steps : 316400, \t Total Gen Loss : 2972.780029296875, \t Total Dis Loss : 6.611708522541448e-05\n",
      "Steps : 316500, \t Total Gen Loss : 3604.61962890625, \t Total Dis Loss : 0.00043541763443499804\n",
      "Steps : 316600, \t Total Gen Loss : 3398.13037109375, \t Total Dis Loss : 3.962488699471578e-05\n",
      "Steps : 316700, \t Total Gen Loss : 3712.2802734375, \t Total Dis Loss : 2.0463137843762524e-05\n",
      "Steps : 316800, \t Total Gen Loss : 3769.148193359375, \t Total Dis Loss : 1.5202733266050927e-05\n",
      "Steps : 316900, \t Total Gen Loss : 3985.76025390625, \t Total Dis Loss : 7.590818131575361e-05\n",
      "Steps : 317000, \t Total Gen Loss : 3580.61474609375, \t Total Dis Loss : 4.747704224428162e-05\n",
      "Steps : 317100, \t Total Gen Loss : 3493.2275390625, \t Total Dis Loss : 2.574637437646743e-05\n",
      "Steps : 317200, \t Total Gen Loss : 4108.78857421875, \t Total Dis Loss : 1.7884614862850867e-05\n",
      "Steps : 317300, \t Total Gen Loss : 3468.428466796875, \t Total Dis Loss : 1.3446828234009445e-05\n",
      "Steps : 317400, \t Total Gen Loss : 3525.844970703125, \t Total Dis Loss : 3.92594447475858e-05\n",
      "Steps : 317500, \t Total Gen Loss : 3215.761962890625, \t Total Dis Loss : 8.684836211614311e-05\n",
      "Steps : 317600, \t Total Gen Loss : 3613.603759765625, \t Total Dis Loss : 9.468901225773152e-06\n",
      "Steps : 317700, \t Total Gen Loss : 3800.0205078125, \t Total Dis Loss : 3.0422959753195755e-05\n",
      "Steps : 317800, \t Total Gen Loss : 3795.743896484375, \t Total Dis Loss : 1.458073893445544e-05\n",
      "Steps : 317900, \t Total Gen Loss : 3969.983642578125, \t Total Dis Loss : 2.269359174533747e-05\n",
      "Steps : 318000, \t Total Gen Loss : 3400.11962890625, \t Total Dis Loss : 2.0170245988992974e-05\n",
      "Steps : 318100, \t Total Gen Loss : 3151.4140625, \t Total Dis Loss : 5.5995933507801965e-05\n",
      "Steps : 318200, \t Total Gen Loss : 3877.603271484375, \t Total Dis Loss : 1.640536356717348e-05\n",
      "Steps : 318300, \t Total Gen Loss : 3934.255615234375, \t Total Dis Loss : 4.1817311284830794e-05\n",
      "Steps : 318400, \t Total Gen Loss : 3869.501708984375, \t Total Dis Loss : 1.266394610865973e-05\n",
      "Steps : 318500, \t Total Gen Loss : 4564.29150390625, \t Total Dis Loss : 1.3195307474234141e-05\n",
      "Steps : 318600, \t Total Gen Loss : 3302.559814453125, \t Total Dis Loss : 1.851943125075195e-05\n",
      "Steps : 318700, \t Total Gen Loss : 3956.087158203125, \t Total Dis Loss : 0.00011220994929317385\n",
      "Steps : 318800, \t Total Gen Loss : 2847.0673828125, \t Total Dis Loss : 1.4127771464700345e-05\n",
      "Steps : 318900, \t Total Gen Loss : 4085.94140625, \t Total Dis Loss : 5.05465095557156e-06\n",
      "Steps : 319000, \t Total Gen Loss : 3396.620361328125, \t Total Dis Loss : 3.88936314266175e-06\n",
      "Steps : 319100, \t Total Gen Loss : 3381.468505859375, \t Total Dis Loss : 5.8428240663488396e-06\n",
      "Steps : 319200, \t Total Gen Loss : 3968.825439453125, \t Total Dis Loss : 1.273832003789721e-05\n",
      "Steps : 319300, \t Total Gen Loss : 3792.080078125, \t Total Dis Loss : 0.00019536909530870616\n",
      "Steps : 319400, \t Total Gen Loss : 3794.4306640625, \t Total Dis Loss : 7.282683509401977e-05\n",
      "Steps : 319500, \t Total Gen Loss : 3402.8212890625, \t Total Dis Loss : 0.00011920321412617341\n",
      "Steps : 319600, \t Total Gen Loss : 3327.285888671875, \t Total Dis Loss : 2.8081145501346327e-05\n",
      "Steps : 319700, \t Total Gen Loss : 3208.678955078125, \t Total Dis Loss : 2.3211678126244806e-05\n",
      "Steps : 319800, \t Total Gen Loss : 3621.09765625, \t Total Dis Loss : 1.2728596630040556e-05\n",
      "Steps : 319900, \t Total Gen Loss : 3189.02197265625, \t Total Dis Loss : 3.267456486355513e-05\n",
      "Steps : 320000, \t Total Gen Loss : 3412.0185546875, \t Total Dis Loss : 3.1082392524695024e-05\n",
      "Steps : 320100, \t Total Gen Loss : 3667.310302734375, \t Total Dis Loss : 1.531835187051911e-05\n",
      "Steps : 320200, \t Total Gen Loss : 3422.536865234375, \t Total Dis Loss : 1.0104593457072042e-05\n",
      "Steps : 320300, \t Total Gen Loss : 3778.9169921875, \t Total Dis Loss : 4.090288257430075e-06\n",
      "Steps : 320400, \t Total Gen Loss : 3502.594482421875, \t Total Dis Loss : 0.00012139314640080556\n",
      "Steps : 320500, \t Total Gen Loss : 3741.121826171875, \t Total Dis Loss : 3.099589957855642e-05\n",
      "Steps : 320600, \t Total Gen Loss : 3363.635986328125, \t Total Dis Loss : 4.425268798513571e-06\n",
      "Time for epoch 57 is 75.0039324760437 sec\n",
      "Steps : 320700, \t Total Gen Loss : 4213.89208984375, \t Total Dis Loss : 6.695811862300616e-06\n",
      "Steps : 320800, \t Total Gen Loss : 3579.48779296875, \t Total Dis Loss : 2.8750805540767033e-06\n",
      "Steps : 320900, \t Total Gen Loss : 3639.756103515625, \t Total Dis Loss : 6.724247214151546e-06\n",
      "Steps : 321000, \t Total Gen Loss : 4152.29833984375, \t Total Dis Loss : 2.679738463484682e-05\n",
      "Steps : 321100, \t Total Gen Loss : 3575.565185546875, \t Total Dis Loss : 5.95934488956118e-06\n",
      "Steps : 321200, \t Total Gen Loss : 3161.473388671875, \t Total Dis Loss : 1.2553737178677693e-05\n",
      "Steps : 321300, \t Total Gen Loss : 3340.52197265625, \t Total Dis Loss : 8.12804682936985e-06\n",
      "Steps : 321400, \t Total Gen Loss : 3090.864501953125, \t Total Dis Loss : 0.001196896773763001\n",
      "Steps : 321500, \t Total Gen Loss : 3391.55322265625, \t Total Dis Loss : 5.144771876075538e-06\n",
      "Steps : 321600, \t Total Gen Loss : 3429.752197265625, \t Total Dis Loss : 1.5322269973694347e-05\n",
      "Steps : 321700, \t Total Gen Loss : 3581.444091796875, \t Total Dis Loss : 3.781306759265135e-06\n",
      "Steps : 321800, \t Total Gen Loss : 3556.49267578125, \t Total Dis Loss : 4.592372988554416e-06\n",
      "Steps : 321900, \t Total Gen Loss : 3328.4833984375, \t Total Dis Loss : 4.781341885973234e-06\n",
      "Steps : 322000, \t Total Gen Loss : 3298.035400390625, \t Total Dis Loss : 2.2055839963286417e-06\n",
      "Steps : 322100, \t Total Gen Loss : 2976.3046875, \t Total Dis Loss : 6.52212065688218e-06\n",
      "Steps : 322200, \t Total Gen Loss : 3537.96728515625, \t Total Dis Loss : 4.519449248618912e-06\n",
      "Steps : 322300, \t Total Gen Loss : 3900.548583984375, \t Total Dis Loss : 2.2917879505257588e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 322400, \t Total Gen Loss : 3486.13525390625, \t Total Dis Loss : 7.583986189274583e-06\n",
      "Steps : 322500, \t Total Gen Loss : 2876.755615234375, \t Total Dis Loss : 2.1221349015831947e-06\n",
      "Steps : 322600, \t Total Gen Loss : 4461.98583984375, \t Total Dis Loss : 1.617809357412625e-05\n",
      "Steps : 322700, \t Total Gen Loss : 3753.784423828125, \t Total Dis Loss : 1.4687922202938353e-06\n",
      "Steps : 322800, \t Total Gen Loss : 3193.643798828125, \t Total Dis Loss : 4.906380127067678e-06\n",
      "Steps : 322900, \t Total Gen Loss : 3585.057861328125, \t Total Dis Loss : 2.276750365126645e-06\n",
      "Steps : 323000, \t Total Gen Loss : 3799.69677734375, \t Total Dis Loss : 2.9944551442895317e-06\n",
      "Steps : 323100, \t Total Gen Loss : 4341.4716796875, \t Total Dis Loss : 3.4268266517756274e-06\n",
      "Steps : 323200, \t Total Gen Loss : 4340.39111328125, \t Total Dis Loss : 7.960205039125867e-06\n",
      "Steps : 323300, \t Total Gen Loss : 3109.4140625, \t Total Dis Loss : 2.5906538212439045e-06\n",
      "Steps : 323400, \t Total Gen Loss : 3531.66552734375, \t Total Dis Loss : 9.66871994023677e-07\n",
      "Steps : 323500, \t Total Gen Loss : 3547.9501953125, \t Total Dis Loss : 2.314124685653951e-06\n",
      "Steps : 323600, \t Total Gen Loss : 2927.201904296875, \t Total Dis Loss : 2.151576609321637e-06\n",
      "Steps : 323700, \t Total Gen Loss : 4266.26611328125, \t Total Dis Loss : 5.6945073083625175e-06\n",
      "Steps : 323800, \t Total Gen Loss : 3269.515380859375, \t Total Dis Loss : 5.953950676484965e-06\n",
      "Steps : 323900, \t Total Gen Loss : 3842.585693359375, \t Total Dis Loss : 0.00010132725583389401\n",
      "Steps : 324000, \t Total Gen Loss : 3477.728271484375, \t Total Dis Loss : 1.4506214938592166e-05\n",
      "Steps : 324100, \t Total Gen Loss : 4199.8671875, \t Total Dis Loss : 5.75948888581479e-06\n",
      "Steps : 324200, \t Total Gen Loss : 3548.10888671875, \t Total Dis Loss : 1.5261863154591992e-05\n",
      "Steps : 324300, \t Total Gen Loss : 3139.022705078125, \t Total Dis Loss : 4.948255536874058e-06\n",
      "Steps : 324400, \t Total Gen Loss : 3663.396240234375, \t Total Dis Loss : 7.860865480324719e-06\n",
      "Steps : 324500, \t Total Gen Loss : 3626.1259765625, \t Total Dis Loss : 2.772579318843782e-05\n",
      "Steps : 324600, \t Total Gen Loss : 3417.315673828125, \t Total Dis Loss : 2.336178113182541e-05\n",
      "Steps : 324700, \t Total Gen Loss : 4003.890380859375, \t Total Dis Loss : 2.2602767785429023e-05\n",
      "Steps : 324800, \t Total Gen Loss : 3633.2275390625, \t Total Dis Loss : 2.9207589250290766e-05\n",
      "Steps : 324900, \t Total Gen Loss : 4034.98291015625, \t Total Dis Loss : 0.012136947363615036\n",
      "Steps : 325000, \t Total Gen Loss : 3877.385498046875, \t Total Dis Loss : 5.116857664688723e-06\n",
      "Steps : 325100, \t Total Gen Loss : 3523.99658203125, \t Total Dis Loss : 1.916098199217231e-06\n",
      "Steps : 325200, \t Total Gen Loss : 2845.095703125, \t Total Dis Loss : 9.861356602414162e-07\n",
      "Steps : 325300, \t Total Gen Loss : 3602.640625, \t Total Dis Loss : 2.754902197921183e-06\n",
      "Steps : 325400, \t Total Gen Loss : 3447.188720703125, \t Total Dis Loss : 7.778657163726166e-06\n",
      "Steps : 325500, \t Total Gen Loss : 3680.4599609375, \t Total Dis Loss : 3.2766397453087848e-06\n",
      "Steps : 325600, \t Total Gen Loss : 3765.2021484375, \t Total Dis Loss : 1.0552610092418035e-06\n",
      "Steps : 325700, \t Total Gen Loss : 3380.119384765625, \t Total Dis Loss : 4.819464720640099e-06\n",
      "Steps : 325800, \t Total Gen Loss : 4047.663330078125, \t Total Dis Loss : 8.291976882901508e-07\n",
      "Steps : 325900, \t Total Gen Loss : 4301.88818359375, \t Total Dis Loss : 4.870210887020221e-06\n",
      "Steps : 326000, \t Total Gen Loss : 4117.1787109375, \t Total Dis Loss : 1.1458113249318558e-06\n",
      "Steps : 326100, \t Total Gen Loss : 3984.025390625, \t Total Dis Loss : 5.1385239203227684e-05\n",
      "Steps : 326200, \t Total Gen Loss : 3766.4462890625, \t Total Dis Loss : 2.7202109777135774e-05\n",
      "Time for epoch 58 is 74.98381757736206 sec\n",
      "Steps : 326300, \t Total Gen Loss : 3713.958740234375, \t Total Dis Loss : 2.9401315259747207e-05\n",
      "Steps : 326400, \t Total Gen Loss : 3962.045166015625, \t Total Dis Loss : 1.2574681932164822e-05\n",
      "Steps : 326500, \t Total Gen Loss : 3492.29150390625, \t Total Dis Loss : 2.083509934891481e-05\n",
      "Steps : 326600, \t Total Gen Loss : 3421.98583984375, \t Total Dis Loss : 2.2228205125429668e-05\n",
      "Steps : 326700, \t Total Gen Loss : 3751.245361328125, \t Total Dis Loss : 3.989372089563403e-06\n",
      "Steps : 326800, \t Total Gen Loss : 3809.376220703125, \t Total Dis Loss : 2.4124697119987104e-06\n",
      "Steps : 326900, \t Total Gen Loss : 3485.567138671875, \t Total Dis Loss : 3.572636342141777e-05\n",
      "Steps : 327000, \t Total Gen Loss : 3706.801025390625, \t Total Dis Loss : 2.9649063435499556e-05\n",
      "Steps : 327100, \t Total Gen Loss : 3415.622802734375, \t Total Dis Loss : 8.8929409685079e-06\n",
      "Steps : 327200, \t Total Gen Loss : 3272.983154296875, \t Total Dis Loss : 9.023081474879291e-06\n",
      "Steps : 327300, \t Total Gen Loss : 2943.324951171875, \t Total Dis Loss : 1.9690753106260672e-05\n",
      "Steps : 327400, \t Total Gen Loss : 3305.26318359375, \t Total Dis Loss : 5.1624811021611094e-06\n",
      "Steps : 327500, \t Total Gen Loss : 4003.631591796875, \t Total Dis Loss : 1.1209502190467902e-05\n",
      "Steps : 327600, \t Total Gen Loss : 3766.032470703125, \t Total Dis Loss : 5.5711198001517914e-06\n",
      "Steps : 327700, \t Total Gen Loss : 3794.001953125, \t Total Dis Loss : 1.9892821001121774e-05\n",
      "Steps : 327800, \t Total Gen Loss : 3624.428466796875, \t Total Dis Loss : 5.133179001859389e-06\n",
      "Steps : 327900, \t Total Gen Loss : 4003.410888671875, \t Total Dis Loss : 1.3169938029022887e-06\n",
      "Steps : 328000, \t Total Gen Loss : 4105.44091796875, \t Total Dis Loss : 2.675675023056101e-05\n",
      "Steps : 328100, \t Total Gen Loss : 3648.388427734375, \t Total Dis Loss : 0.0004201798583380878\n",
      "Steps : 328200, \t Total Gen Loss : 3939.765869140625, \t Total Dis Loss : 4.498947419051547e-06\n",
      "Steps : 328300, \t Total Gen Loss : 3373.135986328125, \t Total Dis Loss : 2.5692876079119742e-05\n",
      "Steps : 328400, \t Total Gen Loss : 3966.709228515625, \t Total Dis Loss : 1.8366112044532201e-06\n",
      "Steps : 328500, \t Total Gen Loss : 4065.390380859375, \t Total Dis Loss : 7.506669135182165e-06\n",
      "Steps : 328600, \t Total Gen Loss : 3727.231201171875, \t Total Dis Loss : 2.1126772935531335e-06\n",
      "Steps : 328700, \t Total Gen Loss : 3486.7666015625, \t Total Dis Loss : 3.391973950783722e-05\n",
      "Steps : 328800, \t Total Gen Loss : 3197.2607421875, \t Total Dis Loss : 4.576919764076592e-06\n",
      "Steps : 328900, \t Total Gen Loss : 3942.713623046875, \t Total Dis Loss : 2.7321448214934207e-06\n",
      "Steps : 329000, \t Total Gen Loss : 2808.579833984375, \t Total Dis Loss : 2.4061716885626083e-06\n",
      "Steps : 329100, \t Total Gen Loss : 3439.338134765625, \t Total Dis Loss : 2.5628096409491263e-06\n",
      "Steps : 329200, \t Total Gen Loss : 3182.5380859375, \t Total Dis Loss : 7.110164006007835e-06\n",
      "Steps : 329300, \t Total Gen Loss : 3297.09716796875, \t Total Dis Loss : 0.00017675072012934834\n",
      "Steps : 329400, \t Total Gen Loss : 3480.092529296875, \t Total Dis Loss : 5.27759630131186e-06\n",
      "Steps : 329500, \t Total Gen Loss : 3647.603271484375, \t Total Dis Loss : 1.214521580550354e-05\n",
      "Steps : 329600, \t Total Gen Loss : 3653.960205078125, \t Total Dis Loss : 7.277301847352646e-06\n",
      "Steps : 329700, \t Total Gen Loss : 4100.2265625, \t Total Dis Loss : 1.7771304555935785e-05\n",
      "Steps : 329800, \t Total Gen Loss : 3615.818359375, \t Total Dis Loss : 6.462742931034882e-06\n",
      "Steps : 329900, \t Total Gen Loss : 3683.940185546875, \t Total Dis Loss : 1.790848000382539e-05\n",
      "Steps : 330000, \t Total Gen Loss : 3455.95556640625, \t Total Dis Loss : 2.3297234292840585e-05\n",
      "Steps : 330100, \t Total Gen Loss : 3339.61181640625, \t Total Dis Loss : 1.1682825970638078e-05\n",
      "Steps : 330200, \t Total Gen Loss : 3661.081298828125, \t Total Dis Loss : 2.6310137855034554e-06\n",
      "Steps : 330300, \t Total Gen Loss : 3720.83349609375, \t Total Dis Loss : 1.130369673774112e-05\n",
      "Steps : 330400, \t Total Gen Loss : 4314.82177734375, \t Total Dis Loss : 4.389124660519883e-05\n",
      "Steps : 330500, \t Total Gen Loss : 3886.8994140625, \t Total Dis Loss : 9.020293873618357e-06\n",
      "Steps : 330600, \t Total Gen Loss : 3809.954833984375, \t Total Dis Loss : 1.0632040812197374e-06\n",
      "Steps : 330700, \t Total Gen Loss : 3357.737060546875, \t Total Dis Loss : 7.80974642111687e-06\n",
      "Steps : 330800, \t Total Gen Loss : 3574.496826171875, \t Total Dis Loss : 3.911311068804935e-06\n",
      "Steps : 330900, \t Total Gen Loss : 3757.065185546875, \t Total Dis Loss : 2.9119748887751484e-06\n",
      "Steps : 331000, \t Total Gen Loss : 3877.5712890625, \t Total Dis Loss : 5.617573606286896e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 331100, \t Total Gen Loss : 3859.40380859375, \t Total Dis Loss : 4.437525603862014e-06\n",
      "Steps : 331200, \t Total Gen Loss : 3353.6787109375, \t Total Dis Loss : 0.04005385935306549\n",
      "Steps : 331300, \t Total Gen Loss : 3673.353759765625, \t Total Dis Loss : 4.357604484539479e-05\n",
      "Steps : 331400, \t Total Gen Loss : 3094.3564453125, \t Total Dis Loss : 1.2792947018169798e-05\n",
      "Steps : 331500, \t Total Gen Loss : 3398.067138671875, \t Total Dis Loss : 2.713838966883486e-06\n",
      "Steps : 331600, \t Total Gen Loss : 4323.794921875, \t Total Dis Loss : 2.9402282962109894e-05\n",
      "Steps : 331700, \t Total Gen Loss : 3803.038818359375, \t Total Dis Loss : 3.986176670878194e-05\n",
      "Steps : 331800, \t Total Gen Loss : 3731.493408203125, \t Total Dis Loss : 5.376475746743381e-05\n",
      "Time for epoch 59 is 75.0213041305542 sec\n",
      "Steps : 331900, \t Total Gen Loss : 3609.494140625, \t Total Dis Loss : 4.0069939132081345e-05\n",
      "Steps : 332000, \t Total Gen Loss : 3506.3662109375, \t Total Dis Loss : 4.966906999470666e-06\n",
      "Steps : 332100, \t Total Gen Loss : 3319.54296875, \t Total Dis Loss : 3.8752959881094284e-06\n",
      "Steps : 332200, \t Total Gen Loss : 3821.05224609375, \t Total Dis Loss : 2.8559172733366722e-06\n",
      "Steps : 332300, \t Total Gen Loss : 3558.680419921875, \t Total Dis Loss : 9.345313628728036e-06\n",
      "Steps : 332400, \t Total Gen Loss : 3676.9873046875, \t Total Dis Loss : 4.3132213249919005e-06\n",
      "Steps : 332500, \t Total Gen Loss : 3658.935791015625, \t Total Dis Loss : 6.814413609390613e-06\n",
      "Steps : 332600, \t Total Gen Loss : 3379.781005859375, \t Total Dis Loss : 1.00806664704578e-05\n",
      "Steps : 332700, \t Total Gen Loss : 3215.46728515625, \t Total Dis Loss : 9.304614650318399e-06\n",
      "Steps : 332800, \t Total Gen Loss : 3531.47900390625, \t Total Dis Loss : 0.0015279465587809682\n",
      "Steps : 332900, \t Total Gen Loss : 4027.931396484375, \t Total Dis Loss : 2.2540774807566777e-05\n",
      "Steps : 333000, \t Total Gen Loss : 3892.8857421875, \t Total Dis Loss : 2.7195815164304804e-06\n",
      "Steps : 333100, \t Total Gen Loss : 3025.718505859375, \t Total Dis Loss : 7.519925020460505e-06\n",
      "Steps : 333200, \t Total Gen Loss : 3876.824951171875, \t Total Dis Loss : 4.331766831455752e-06\n",
      "Steps : 333300, \t Total Gen Loss : 3810.746826171875, \t Total Dis Loss : 4.276193976693321e-06\n",
      "Steps : 333400, \t Total Gen Loss : 3384.3857421875, \t Total Dis Loss : 1.9809453988273162e-06\n",
      "Steps : 333500, \t Total Gen Loss : 3155.412841796875, \t Total Dis Loss : 3.7406064166134456e-06\n",
      "Steps : 333600, \t Total Gen Loss : 3910.3095703125, \t Total Dis Loss : 2.8192196168674855e-06\n",
      "Steps : 333700, \t Total Gen Loss : 3875.841552734375, \t Total Dis Loss : 9.937960385286715e-06\n",
      "Steps : 333800, \t Total Gen Loss : 3116.660888671875, \t Total Dis Loss : 8.222335054597352e-06\n",
      "Steps : 333900, \t Total Gen Loss : 3612.15478515625, \t Total Dis Loss : 3.170770014548907e-06\n",
      "Steps : 334000, \t Total Gen Loss : 3866.63037109375, \t Total Dis Loss : 1.786725079000462e-05\n",
      "Steps : 334100, \t Total Gen Loss : 4185.65087890625, \t Total Dis Loss : 5.1242535846540704e-05\n",
      "Steps : 334200, \t Total Gen Loss : 3425.073486328125, \t Total Dis Loss : 7.736070074315649e-06\n",
      "Steps : 334300, \t Total Gen Loss : 3373.39501953125, \t Total Dis Loss : 2.9112386982887983e-05\n",
      "Steps : 334400, \t Total Gen Loss : 3020.911865234375, \t Total Dis Loss : 4.816592991119251e-06\n",
      "Steps : 334500, \t Total Gen Loss : 3925.641357421875, \t Total Dis Loss : 2.686747939151246e-05\n",
      "Steps : 334600, \t Total Gen Loss : 3222.083984375, \t Total Dis Loss : 1.3970604868518421e-06\n",
      "Steps : 334700, \t Total Gen Loss : 3608.01513671875, \t Total Dis Loss : 3.841601483145496e-06\n",
      "Steps : 334800, \t Total Gen Loss : 3588.580078125, \t Total Dis Loss : 2.731761742325034e-05\n",
      "Steps : 334900, \t Total Gen Loss : 4054.6005859375, \t Total Dis Loss : 2.2817315766587853e-05\n",
      "Steps : 335000, \t Total Gen Loss : 3424.070556640625, \t Total Dis Loss : 0.00021912834199611098\n",
      "Steps : 335100, \t Total Gen Loss : 3203.99609375, \t Total Dis Loss : 6.514369033538969e-06\n",
      "Steps : 335200, \t Total Gen Loss : 3869.2783203125, \t Total Dis Loss : 7.864275175961666e-06\n",
      "Steps : 335300, \t Total Gen Loss : 3816.990966796875, \t Total Dis Loss : 0.0012409327318891883\n",
      "Steps : 335400, \t Total Gen Loss : 3316.35205078125, \t Total Dis Loss : 8.48144554765895e-05\n",
      "Steps : 335500, \t Total Gen Loss : 3252.440185546875, \t Total Dis Loss : 1.0717627446865663e-05\n",
      "Steps : 335600, \t Total Gen Loss : 3387.12890625, \t Total Dis Loss : 1.179548507934669e-05\n",
      "Steps : 335700, \t Total Gen Loss : 4331.861328125, \t Total Dis Loss : 9.89865802694112e-05\n",
      "Steps : 335800, \t Total Gen Loss : 3826.926025390625, \t Total Dis Loss : 2.3096336008165963e-05\n",
      "Steps : 335900, \t Total Gen Loss : 4112.2080078125, \t Total Dis Loss : 2.6259617698087823e-06\n",
      "Steps : 336000, \t Total Gen Loss : 3581.5009765625, \t Total Dis Loss : 8.797544069238938e-06\n",
      "Steps : 336100, \t Total Gen Loss : 3013.072265625, \t Total Dis Loss : 2.8079311960027553e-06\n",
      "Steps : 336200, \t Total Gen Loss : 2909.468017578125, \t Total Dis Loss : 2.3242964743985794e-06\n",
      "Steps : 336300, \t Total Gen Loss : 3413.4169921875, \t Total Dis Loss : 3.961003130825702e-06\n",
      "Steps : 336400, \t Total Gen Loss : 3033.87255859375, \t Total Dis Loss : 1.1131754035886843e-05\n",
      "Steps : 336500, \t Total Gen Loss : 3620.82958984375, \t Total Dis Loss : 3.5934397146775154e-06\n",
      "Steps : 336600, \t Total Gen Loss : 3209.0673828125, \t Total Dis Loss : 5.869075721420813e-06\n",
      "Steps : 336700, \t Total Gen Loss : 3968.789306640625, \t Total Dis Loss : 2.352080400669365e-06\n",
      "Steps : 336800, \t Total Gen Loss : 3336.830078125, \t Total Dis Loss : 2.7086238674201013e-07\n",
      "Steps : 336900, \t Total Gen Loss : 3402.33740234375, \t Total Dis Loss : 1.7818432752392255e-05\n",
      "Steps : 337000, \t Total Gen Loss : 4483.45654296875, \t Total Dis Loss : 8.26017276267521e-06\n",
      "Steps : 337100, \t Total Gen Loss : 4063.177734375, \t Total Dis Loss : 3.490348535706289e-05\n",
      "Steps : 337200, \t Total Gen Loss : 4280.15478515625, \t Total Dis Loss : 6.5028043536585756e-06\n",
      "Steps : 337300, \t Total Gen Loss : 2970.794189453125, \t Total Dis Loss : 2.89800186692446e-06\n",
      "Steps : 337400, \t Total Gen Loss : 3190.064208984375, \t Total Dis Loss : 1.0613526683300734e-05\n",
      "Steps : 337500, \t Total Gen Loss : 3483.89013671875, \t Total Dis Loss : 3.546991365510621e-06\n",
      "Time for epoch 60 is 75.16194152832031 sec\n",
      "Steps : 337600, \t Total Gen Loss : 3541.755859375, \t Total Dis Loss : 2.066024171654135e-05\n",
      "Steps : 337700, \t Total Gen Loss : 3231.29638671875, \t Total Dis Loss : 1.7591615687706508e-05\n",
      "Steps : 337800, \t Total Gen Loss : 3331.130615234375, \t Total Dis Loss : 1.8822345282387687e-06\n",
      "Steps : 337900, \t Total Gen Loss : 3794.39111328125, \t Total Dis Loss : 1.976837211259408e-06\n",
      "Steps : 338000, \t Total Gen Loss : 3920.245361328125, \t Total Dis Loss : 1.0382721029600361e-06\n",
      "Steps : 338100, \t Total Gen Loss : 3374.50439453125, \t Total Dis Loss : 4.787898888025666e-06\n",
      "Steps : 338200, \t Total Gen Loss : 3380.413818359375, \t Total Dis Loss : 2.3833897557778982e-06\n",
      "Steps : 338300, \t Total Gen Loss : 3638.764404296875, \t Total Dis Loss : 1.9467946913209744e-05\n",
      "Steps : 338400, \t Total Gen Loss : 3814.34228515625, \t Total Dis Loss : 2.9024301966273924e-06\n",
      "Steps : 338500, \t Total Gen Loss : 3556.69091796875, \t Total Dis Loss : 2.866806426027324e-06\n",
      "Steps : 338600, \t Total Gen Loss : 3588.773193359375, \t Total Dis Loss : 9.042991223395802e-06\n",
      "Steps : 338700, \t Total Gen Loss : 3921.337646484375, \t Total Dis Loss : 3.2978759918478318e-06\n",
      "Steps : 338800, \t Total Gen Loss : 2567.7880859375, \t Total Dis Loss : 3.171361868226086e-06\n",
      "Steps : 338900, \t Total Gen Loss : 3683.57763671875, \t Total Dis Loss : 7.5143514550291e-05\n",
      "Steps : 339000, \t Total Gen Loss : 4329.96337890625, \t Total Dis Loss : 1.361242902930826e-05\n",
      "Steps : 339100, \t Total Gen Loss : 3906.54248046875, \t Total Dis Loss : 6.127043161541224e-05\n",
      "Steps : 339200, \t Total Gen Loss : 3547.259521484375, \t Total Dis Loss : 3.239805664634332e-06\n",
      "Steps : 339300, \t Total Gen Loss : 3671.252197265625, \t Total Dis Loss : 3.6536951029120246e-06\n",
      "Steps : 339400, \t Total Gen Loss : 3975.277587890625, \t Total Dis Loss : 1.0335791557736229e-05\n",
      "Steps : 339500, \t Total Gen Loss : 3739.120361328125, \t Total Dis Loss : 3.747996743186377e-05\n",
      "Steps : 339600, \t Total Gen Loss : 3399.20068359375, \t Total Dis Loss : 1.7030737353707082e-06\n",
      "Steps : 339700, \t Total Gen Loss : 3299.304931640625, \t Total Dis Loss : 3.353338570377673e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 339800, \t Total Gen Loss : 3756.955322265625, \t Total Dis Loss : 4.122161954001058e-06\n",
      "Steps : 339900, \t Total Gen Loss : 3910.948974609375, \t Total Dis Loss : 1.0139736332348548e-05\n",
      "Steps : 340000, \t Total Gen Loss : 3757.37939453125, \t Total Dis Loss : 3.845926585199777e-06\n",
      "Steps : 340100, \t Total Gen Loss : 3534.28564453125, \t Total Dis Loss : 2.24423956751707e-06\n",
      "Steps : 340200, \t Total Gen Loss : 3841.78173828125, \t Total Dis Loss : 2.470748768246267e-06\n",
      "Steps : 340300, \t Total Gen Loss : 3430.147216796875, \t Total Dis Loss : 1.0031355941464426e-06\n",
      "Steps : 340400, \t Total Gen Loss : 3923.839111328125, \t Total Dis Loss : 1.8771996792565915e-06\n",
      "Steps : 340500, \t Total Gen Loss : 3765.430908203125, \t Total Dis Loss : 9.74098475126084e-06\n",
      "Steps : 340600, \t Total Gen Loss : 4022.29833984375, \t Total Dis Loss : 1.8339367215958191e-06\n",
      "Steps : 340700, \t Total Gen Loss : 3485.7099609375, \t Total Dis Loss : 1.542047357361298e-06\n",
      "Steps : 340800, \t Total Gen Loss : 3815.494384765625, \t Total Dis Loss : 4.141443241678644e-06\n",
      "Steps : 340900, \t Total Gen Loss : 3630.353271484375, \t Total Dis Loss : 9.329442036687396e-07\n",
      "Steps : 341000, \t Total Gen Loss : 3328.90185546875, \t Total Dis Loss : 1.2674367098952644e-05\n",
      "Steps : 341100, \t Total Gen Loss : 4487.75146484375, \t Total Dis Loss : 1.1413362699386198e-06\n",
      "Steps : 341200, \t Total Gen Loss : 3433.16552734375, \t Total Dis Loss : 8.053746682890051e-07\n",
      "Steps : 341300, \t Total Gen Loss : 3295.5322265625, \t Total Dis Loss : 2.9125603759894148e-06\n",
      "Steps : 341400, \t Total Gen Loss : 3606.2939453125, \t Total Dis Loss : 2.0659845176851377e-06\n",
      "Steps : 341500, \t Total Gen Loss : 3822.6025390625, \t Total Dis Loss : 5.729002623411361e-07\n",
      "Steps : 341600, \t Total Gen Loss : 3438.504150390625, \t Total Dis Loss : 7.014640868874267e-05\n",
      "Steps : 341700, \t Total Gen Loss : 3860.12158203125, \t Total Dis Loss : 1.2923525218866416e-06\n",
      "Steps : 341800, \t Total Gen Loss : 3525.918701171875, \t Total Dis Loss : 2.101852487612632e-06\n",
      "Steps : 341900, \t Total Gen Loss : 3757.436767578125, \t Total Dis Loss : 8.168722160917241e-06\n",
      "Steps : 342000, \t Total Gen Loss : 3335.827880859375, \t Total Dis Loss : 0.0003546546504367143\n",
      "Steps : 342100, \t Total Gen Loss : 3650.290771484375, \t Total Dis Loss : 8.785742465988733e-06\n",
      "Steps : 342200, \t Total Gen Loss : 3189.96240234375, \t Total Dis Loss : 4.6600493078585714e-05\n",
      "Steps : 342300, \t Total Gen Loss : 4060.1982421875, \t Total Dis Loss : 0.0001964896364370361\n",
      "Steps : 342400, \t Total Gen Loss : 3814.63037109375, \t Total Dis Loss : 2.1335606561478926e-06\n",
      "Steps : 342500, \t Total Gen Loss : 3572.803466796875, \t Total Dis Loss : 4.751351298182271e-06\n",
      "Steps : 342600, \t Total Gen Loss : 3135.28466796875, \t Total Dis Loss : 8.767510735196993e-06\n",
      "Steps : 342700, \t Total Gen Loss : 3527.069580078125, \t Total Dis Loss : 6.790562110836618e-06\n",
      "Steps : 342800, \t Total Gen Loss : 3408.4443359375, \t Total Dis Loss : 0.00029173801885917783\n",
      "Steps : 342900, \t Total Gen Loss : 3151.343017578125, \t Total Dis Loss : 1.302469627262326e-05\n",
      "Steps : 343000, \t Total Gen Loss : 3601.63427734375, \t Total Dis Loss : 5.477652666741051e-05\n",
      "Steps : 343100, \t Total Gen Loss : 3741.491455078125, \t Total Dis Loss : 2.2460542822955176e-05\n",
      "Time for epoch 61 is 74.96760940551758 sec\n",
      "Steps : 343200, \t Total Gen Loss : 4060.87060546875, \t Total Dis Loss : 5.908150342293084e-06\n",
      "Steps : 343300, \t Total Gen Loss : 4019.81982421875, \t Total Dis Loss : 9.993364983529318e-06\n",
      "Steps : 343400, \t Total Gen Loss : 4019.86279296875, \t Total Dis Loss : 3.55874340129958e-06\n",
      "Steps : 343500, \t Total Gen Loss : 3627.06494140625, \t Total Dis Loss : 8.046111361181829e-06\n",
      "Steps : 343600, \t Total Gen Loss : 3916.9853515625, \t Total Dis Loss : 3.571954948711209e-06\n",
      "Steps : 343700, \t Total Gen Loss : 4253.04296875, \t Total Dis Loss : 7.303834081540117e-06\n",
      "Steps : 343800, \t Total Gen Loss : 3908.768310546875, \t Total Dis Loss : 2.187361678807065e-05\n",
      "Steps : 343900, \t Total Gen Loss : 3387.366943359375, \t Total Dis Loss : 0.0003920242306776345\n",
      "Steps : 344000, \t Total Gen Loss : 3603.26025390625, \t Total Dis Loss : 2.1265906980261207e-05\n",
      "Steps : 344100, \t Total Gen Loss : 3980.736328125, \t Total Dis Loss : 5.322768629412167e-06\n",
      "Steps : 344200, \t Total Gen Loss : 3194.34033203125, \t Total Dis Loss : 5.239434358372819e-06\n",
      "Steps : 344300, \t Total Gen Loss : 3673.478271484375, \t Total Dis Loss : 8.461711331619881e-06\n",
      "Steps : 344400, \t Total Gen Loss : 3685.95458984375, \t Total Dis Loss : 1.350227466900833e-05\n",
      "Steps : 344500, \t Total Gen Loss : 3167.788818359375, \t Total Dis Loss : 3.701627065311186e-06\n",
      "Steps : 344600, \t Total Gen Loss : 3839.744140625, \t Total Dis Loss : 4.617256308847573e-06\n",
      "Steps : 344700, \t Total Gen Loss : 3798.28173828125, \t Total Dis Loss : 1.9104631064692512e-05\n",
      "Steps : 344800, \t Total Gen Loss : 4325.85888671875, \t Total Dis Loss : 1.2973224329471122e-05\n",
      "Steps : 344900, \t Total Gen Loss : 3307.276123046875, \t Total Dis Loss : 9.540850442135707e-06\n",
      "Steps : 345000, \t Total Gen Loss : 3511.048095703125, \t Total Dis Loss : 1.3801069144392386e-05\n",
      "Steps : 345100, \t Total Gen Loss : 3276.08837890625, \t Total Dis Loss : 2.2531889044330455e-05\n",
      "Steps : 345200, \t Total Gen Loss : 3855.150390625, \t Total Dis Loss : 5.380959464673651e-06\n",
      "Steps : 345300, \t Total Gen Loss : 3350.44677734375, \t Total Dis Loss : 6.389106602000538e-06\n",
      "Steps : 345400, \t Total Gen Loss : 3480.100830078125, \t Total Dis Loss : 4.607640676113078e-06\n",
      "Steps : 345500, \t Total Gen Loss : 3819.852783203125, \t Total Dis Loss : 2.360774487897288e-06\n",
      "Steps : 345600, \t Total Gen Loss : 3246.890625, \t Total Dis Loss : 6.136456249805633e-06\n",
      "Steps : 345700, \t Total Gen Loss : 4260.86474609375, \t Total Dis Loss : 1.5967892977641895e-05\n",
      "Steps : 345800, \t Total Gen Loss : 3550.407470703125, \t Total Dis Loss : 1.3375790331338067e-05\n",
      "Steps : 345900, \t Total Gen Loss : 3559.66064453125, \t Total Dis Loss : 1.8829153987098834e-06\n",
      "Steps : 346000, \t Total Gen Loss : 4119.4609375, \t Total Dis Loss : 2.3650934508623322e-06\n",
      "Steps : 346100, \t Total Gen Loss : 4075.897216796875, \t Total Dis Loss : 4.26877068093745e-06\n",
      "Steps : 346200, \t Total Gen Loss : 3232.66162109375, \t Total Dis Loss : 1.770533117451123e-06\n",
      "Steps : 346300, \t Total Gen Loss : 3909.54736328125, \t Total Dis Loss : 1.3799566431771382e-06\n",
      "Steps : 346400, \t Total Gen Loss : 3369.43896484375, \t Total Dis Loss : 9.07248686417006e-05\n",
      "Steps : 346500, \t Total Gen Loss : 3971.182373046875, \t Total Dis Loss : 4.412479938764591e-06\n",
      "Steps : 346600, \t Total Gen Loss : 3093.15771484375, \t Total Dis Loss : 7.090306098689325e-06\n",
      "Steps : 346700, \t Total Gen Loss : 3510.4306640625, \t Total Dis Loss : 1.1439253739808919e-06\n",
      "Steps : 346800, \t Total Gen Loss : 3820.273681640625, \t Total Dis Loss : 2.998067657244974e-06\n",
      "Steps : 346900, \t Total Gen Loss : 3541.672119140625, \t Total Dis Loss : 2.694200702535454e-06\n",
      "Steps : 347000, \t Total Gen Loss : 3026.233154296875, \t Total Dis Loss : 3.7534855437115766e-06\n",
      "Steps : 347100, \t Total Gen Loss : 3657.3916015625, \t Total Dis Loss : 2.408752880000975e-05\n",
      "Steps : 347200, \t Total Gen Loss : 3818.9775390625, \t Total Dis Loss : 2.0913080334139522e-06\n",
      "Steps : 347300, \t Total Gen Loss : 3324.2255859375, \t Total Dis Loss : 7.054642992443405e-06\n",
      "Steps : 347400, \t Total Gen Loss : 3368.7529296875, \t Total Dis Loss : 1.9473929569358006e-05\n",
      "Steps : 347500, \t Total Gen Loss : 3458.69970703125, \t Total Dis Loss : 3.105414634774206e-06\n",
      "Steps : 347600, \t Total Gen Loss : 3874.334228515625, \t Total Dis Loss : 4.225398697599303e-06\n",
      "Steps : 347700, \t Total Gen Loss : 4331.64111328125, \t Total Dis Loss : 3.5304368793731555e-05\n",
      "Steps : 347800, \t Total Gen Loss : 2946.710205078125, \t Total Dis Loss : 9.260005026590079e-05\n",
      "Steps : 347900, \t Total Gen Loss : 3727.25927734375, \t Total Dis Loss : 2.6242127205478027e-05\n",
      "Steps : 348000, \t Total Gen Loss : 2907.714111328125, \t Total Dis Loss : 2.943438403235632e-06\n",
      "Steps : 348100, \t Total Gen Loss : 3304.01220703125, \t Total Dis Loss : 3.5866773941961583e-06\n",
      "Steps : 348200, \t Total Gen Loss : 3308.504150390625, \t Total Dis Loss : 2.4589169242972275e-06\n",
      "Steps : 348300, \t Total Gen Loss : 3620.587646484375, \t Total Dis Loss : 2.2217604964680504e-06\n",
      "Steps : 348400, \t Total Gen Loss : 3663.923583984375, \t Total Dis Loss : 7.243900768116873e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 348500, \t Total Gen Loss : 4086.53564453125, \t Total Dis Loss : 4.655050361179747e-06\n",
      "Steps : 348600, \t Total Gen Loss : 3554.4150390625, \t Total Dis Loss : 2.492792418706813e-06\n",
      "Steps : 348700, \t Total Gen Loss : 3943.1181640625, \t Total Dis Loss : 1.4324009498523083e-05\n",
      "Time for epoch 62 is 75.01643919944763 sec\n",
      "Steps : 348800, \t Total Gen Loss : 3795.257080078125, \t Total Dis Loss : 4.153107056481531e-06\n",
      "Steps : 348900, \t Total Gen Loss : 3609.154052734375, \t Total Dis Loss : 4.850919594900915e-06\n",
      "Steps : 349000, \t Total Gen Loss : 3493.373046875, \t Total Dis Loss : 1.146031263488112e-05\n",
      "Steps : 349100, \t Total Gen Loss : 3239.6328125, \t Total Dis Loss : 1.5887419067439623e-05\n",
      "Steps : 349200, \t Total Gen Loss : 4069.218505859375, \t Total Dis Loss : 2.7049920845456654e-06\n",
      "Steps : 349300, \t Total Gen Loss : 3705.59814453125, \t Total Dis Loss : 8.130247806548141e-06\n",
      "Steps : 349400, \t Total Gen Loss : 2971.865966796875, \t Total Dis Loss : 3.786683737416752e-05\n",
      "Steps : 349500, \t Total Gen Loss : 3506.820556640625, \t Total Dis Loss : 3.0136972782202065e-05\n",
      "Steps : 349600, \t Total Gen Loss : 3736.52783203125, \t Total Dis Loss : 3.2164170988835394e-06\n",
      "Steps : 349700, \t Total Gen Loss : 3789.29248046875, \t Total Dis Loss : 8.054317731875926e-06\n",
      "Steps : 349800, \t Total Gen Loss : 3243.8408203125, \t Total Dis Loss : 7.074673703755252e-06\n",
      "Steps : 349900, \t Total Gen Loss : 3638.246337890625, \t Total Dis Loss : 3.839493729174137e-05\n",
      "Steps : 350000, \t Total Gen Loss : 4247.017578125, \t Total Dis Loss : 1.3339144970814232e-05\n",
      "Steps : 350100, \t Total Gen Loss : 3844.511474609375, \t Total Dis Loss : 8.580328540119808e-06\n",
      "Steps : 350200, \t Total Gen Loss : 3819.7373046875, \t Total Dis Loss : 6.564556679222733e-05\n",
      "Steps : 350300, \t Total Gen Loss : 3545.811767578125, \t Total Dis Loss : 5.175163551029982e-06\n",
      "Steps : 350400, \t Total Gen Loss : 3332.000244140625, \t Total Dis Loss : 1.959502242243616e-06\n",
      "Steps : 350500, \t Total Gen Loss : 3236.741455078125, \t Total Dis Loss : 4.433853973750956e-06\n",
      "Steps : 350600, \t Total Gen Loss : 3867.37890625, \t Total Dis Loss : 3.7348736441344954e-06\n",
      "Steps : 350700, \t Total Gen Loss : 3590.331298828125, \t Total Dis Loss : 3.8651018257951364e-05\n",
      "Steps : 350800, \t Total Gen Loss : 3801.022705078125, \t Total Dis Loss : 2.2171960154082626e-05\n",
      "Steps : 350900, \t Total Gen Loss : 2816.21337890625, \t Total Dis Loss : 2.8189851946081035e-05\n",
      "Steps : 351000, \t Total Gen Loss : 3015.940673828125, \t Total Dis Loss : 4.148669904679991e-05\n",
      "Steps : 351100, \t Total Gen Loss : 3202.441162109375, \t Total Dis Loss : 2.2190697563928552e-05\n",
      "Steps : 351200, \t Total Gen Loss : 3836.861572265625, \t Total Dis Loss : 7.105983058863785e-06\n",
      "Steps : 351300, \t Total Gen Loss : 3144.478515625, \t Total Dis Loss : 6.884969479870051e-05\n",
      "Steps : 351400, \t Total Gen Loss : 3781.681396484375, \t Total Dis Loss : 6.560540896316525e-06\n",
      "Steps : 351500, \t Total Gen Loss : 3877.072021484375, \t Total Dis Loss : 6.281648893491365e-06\n",
      "Steps : 351600, \t Total Gen Loss : 3533.46240234375, \t Total Dis Loss : 1.572140899952501e-05\n",
      "Steps : 351700, \t Total Gen Loss : 3364.12939453125, \t Total Dis Loss : 0.07006461918354034\n",
      "Steps : 351800, \t Total Gen Loss : 3328.197998046875, \t Total Dis Loss : 4.3796058889711276e-05\n",
      "Steps : 351900, \t Total Gen Loss : 3796.901611328125, \t Total Dis Loss : 0.0003740203392226249\n",
      "Steps : 352000, \t Total Gen Loss : 3420.11767578125, \t Total Dis Loss : 2.833823782566469e-05\n",
      "Steps : 352100, \t Total Gen Loss : 3470.971923828125, \t Total Dis Loss : 8.261336006398778e-06\n",
      "Steps : 352200, \t Total Gen Loss : 3606.819580078125, \t Total Dis Loss : 1.9384344795980724e-06\n",
      "Steps : 352300, \t Total Gen Loss : 3942.614990234375, \t Total Dis Loss : 7.148835265979869e-06\n",
      "Steps : 352400, \t Total Gen Loss : 3325.583740234375, \t Total Dis Loss : 0.0003875556285493076\n",
      "Steps : 352500, \t Total Gen Loss : 3442.808349609375, \t Total Dis Loss : 3.646815002866788e-06\n",
      "Steps : 352600, \t Total Gen Loss : 3460.048095703125, \t Total Dis Loss : 7.809852831996977e-06\n",
      "Steps : 352700, \t Total Gen Loss : 3478.847900390625, \t Total Dis Loss : 2.8201564418850467e-05\n",
      "Steps : 352800, \t Total Gen Loss : 3406.074462890625, \t Total Dis Loss : 2.9271081984916236e-06\n",
      "Steps : 352900, \t Total Gen Loss : 2916.322509765625, \t Total Dis Loss : 6.272455721045844e-06\n",
      "Steps : 353000, \t Total Gen Loss : 3287.743896484375, \t Total Dis Loss : 7.661675226700027e-06\n",
      "Steps : 353100, \t Total Gen Loss : 4089.386962890625, \t Total Dis Loss : 4.3553409341257066e-06\n",
      "Steps : 353200, \t Total Gen Loss : 3610.4677734375, \t Total Dis Loss : 3.099018067587167e-05\n",
      "Steps : 353300, \t Total Gen Loss : 3579.70263671875, \t Total Dis Loss : 3.430043989283149e-06\n",
      "Steps : 353400, \t Total Gen Loss : 3391.758544921875, \t Total Dis Loss : 1.2984963177586906e-05\n",
      "Steps : 353500, \t Total Gen Loss : 3513.01611328125, \t Total Dis Loss : 3.3095104299718514e-05\n",
      "Steps : 353600, \t Total Gen Loss : 3497.25927734375, \t Total Dis Loss : 2.22480739466846e-05\n",
      "Steps : 353700, \t Total Gen Loss : 3474.595458984375, \t Total Dis Loss : 1.3003083040530328e-05\n",
      "Steps : 353800, \t Total Gen Loss : 3457.695068359375, \t Total Dis Loss : 3.6029309740115423e-06\n",
      "Steps : 353900, \t Total Gen Loss : 3948.190673828125, \t Total Dis Loss : 6.086683242756408e-06\n",
      "Steps : 354000, \t Total Gen Loss : 3291.76953125, \t Total Dis Loss : 1.673163978921366e-06\n",
      "Steps : 354100, \t Total Gen Loss : 4332.32861328125, \t Total Dis Loss : 1.4384515907295281e-06\n",
      "Steps : 354200, \t Total Gen Loss : 3688.382080078125, \t Total Dis Loss : 4.447620995051693e-06\n",
      "Steps : 354300, \t Total Gen Loss : 3620.158203125, \t Total Dis Loss : 2.5564595489413477e-06\n",
      "Time for epoch 63 is 74.97755455970764 sec\n",
      "Steps : 354400, \t Total Gen Loss : 3010.72900390625, \t Total Dis Loss : 1.0909516277024522e-06\n",
      "Steps : 354500, \t Total Gen Loss : 3722.05908203125, \t Total Dis Loss : 9.719107765704393e-06\n",
      "Steps : 354600, \t Total Gen Loss : 3863.293212890625, \t Total Dis Loss : 7.2690418164711446e-06\n",
      "Steps : 354700, \t Total Gen Loss : 3744.125244140625, \t Total Dis Loss : 8.965714187070262e-06\n",
      "Steps : 354800, \t Total Gen Loss : 3586.889892578125, \t Total Dis Loss : 1.1283579624432605e-05\n",
      "Steps : 354900, \t Total Gen Loss : 3778.291748046875, \t Total Dis Loss : 1.6267283626802964e-06\n",
      "Steps : 355000, \t Total Gen Loss : 3752.88720703125, \t Total Dis Loss : 2.016041435126681e-06\n",
      "Steps : 355100, \t Total Gen Loss : 3339.310791015625, \t Total Dis Loss : 7.944327990117017e-06\n",
      "Steps : 355200, \t Total Gen Loss : 3601.240966796875, \t Total Dis Loss : 3.095745341852307e-05\n",
      "Steps : 355300, \t Total Gen Loss : 3558.322265625, \t Total Dis Loss : 9.576247066434007e-06\n",
      "Steps : 355400, \t Total Gen Loss : 3671.246826171875, \t Total Dis Loss : 3.859644948533969e-06\n",
      "Steps : 355500, \t Total Gen Loss : 4088.169189453125, \t Total Dis Loss : 7.14516272637411e-06\n",
      "Steps : 355600, \t Total Gen Loss : 3596.912109375, \t Total Dis Loss : 2.4801242943794932e-06\n",
      "Steps : 355700, \t Total Gen Loss : 4158.8583984375, \t Total Dis Loss : 2.3470136511605233e-05\n",
      "Steps : 355800, \t Total Gen Loss : 4137.32958984375, \t Total Dis Loss : 1.7836053302744403e-05\n",
      "Steps : 355900, \t Total Gen Loss : 3736.126708984375, \t Total Dis Loss : 9.004839739645831e-06\n",
      "Steps : 356000, \t Total Gen Loss : 4456.1904296875, \t Total Dis Loss : 6.095313801779412e-05\n",
      "Steps : 356100, \t Total Gen Loss : 4159.52392578125, \t Total Dis Loss : 3.881248994730413e-05\n",
      "Steps : 356200, \t Total Gen Loss : 3851.53076171875, \t Total Dis Loss : 1.9801016151177464e-06\n",
      "Steps : 356300, \t Total Gen Loss : 3356.357666015625, \t Total Dis Loss : 6.015024155203719e-06\n",
      "Steps : 356400, \t Total Gen Loss : 3789.56298828125, \t Total Dis Loss : 2.1869950614927802e-06\n",
      "Steps : 356500, \t Total Gen Loss : 3416.267578125, \t Total Dis Loss : 4.392056780488929e-06\n",
      "Steps : 356600, \t Total Gen Loss : 4152.55322265625, \t Total Dis Loss : 5.3058674893691204e-06\n",
      "Steps : 356700, \t Total Gen Loss : 4032.632080078125, \t Total Dis Loss : 7.4959634730475955e-06\n",
      "Steps : 356800, \t Total Gen Loss : 3947.28369140625, \t Total Dis Loss : 6.678120371361729e-06\n",
      "Steps : 356900, \t Total Gen Loss : 4350.3388671875, \t Total Dis Loss : 4.942435680277413e-06\n",
      "Steps : 357000, \t Total Gen Loss : 3134.115234375, \t Total Dis Loss : 7.658994945813902e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 357100, \t Total Gen Loss : 3430.09228515625, \t Total Dis Loss : 1.619206955183472e-06\n",
      "Steps : 357200, \t Total Gen Loss : 3800.98681640625, \t Total Dis Loss : 2.094350975312409e-06\n",
      "Steps : 357300, \t Total Gen Loss : 3528.917724609375, \t Total Dis Loss : 5.450450771604665e-06\n",
      "Steps : 357400, \t Total Gen Loss : 3468.147216796875, \t Total Dis Loss : 1.46465094985615e-06\n",
      "Steps : 357500, \t Total Gen Loss : 3422.744873046875, \t Total Dis Loss : 4.151551365794148e-06\n",
      "Steps : 357600, \t Total Gen Loss : 3492.860107421875, \t Total Dis Loss : 1.5897570619927137e-06\n",
      "Steps : 357700, \t Total Gen Loss : 4091.25390625, \t Total Dis Loss : 0.00025228745653294027\n",
      "Steps : 357800, \t Total Gen Loss : 4031.96240234375, \t Total Dis Loss : 6.6254397097509354e-06\n",
      "Steps : 357900, \t Total Gen Loss : 3745.053955078125, \t Total Dis Loss : 2.8351012588245794e-05\n",
      "Steps : 358000, \t Total Gen Loss : 3754.760498046875, \t Total Dis Loss : 1.554874143039342e-05\n",
      "Steps : 358100, \t Total Gen Loss : 3586.71484375, \t Total Dis Loss : 4.91592800244689e-06\n",
      "Steps : 358200, \t Total Gen Loss : 3448.42431640625, \t Total Dis Loss : 6.414148811018094e-05\n",
      "Steps : 358300, \t Total Gen Loss : 4321.17919921875, \t Total Dis Loss : 1.699686072242912e-05\n",
      "Steps : 358400, \t Total Gen Loss : 4169.35595703125, \t Total Dis Loss : 8.52732227940578e-06\n",
      "Steps : 358500, \t Total Gen Loss : 3411.05322265625, \t Total Dis Loss : 3.0507367227983195e-06\n",
      "Steps : 358600, \t Total Gen Loss : 4096.35546875, \t Total Dis Loss : 4.7540004743495956e-05\n",
      "Steps : 358700, \t Total Gen Loss : 3530.677001953125, \t Total Dis Loss : 4.121095571463229e-06\n",
      "Steps : 358800, \t Total Gen Loss : 4212.5625, \t Total Dis Loss : 4.472255113796564e-06\n",
      "Steps : 358900, \t Total Gen Loss : 3454.607421875, \t Total Dis Loss : 2.2628860278928187e-06\n",
      "Steps : 359000, \t Total Gen Loss : 3352.859619140625, \t Total Dis Loss : 9.556955774314702e-06\n",
      "Steps : 359100, \t Total Gen Loss : 2937.309814453125, \t Total Dis Loss : 1.4894729929437744e-06\n",
      "Steps : 359200, \t Total Gen Loss : 3669.483642578125, \t Total Dis Loss : 1.8719461536420567e-07\n",
      "Steps : 359300, \t Total Gen Loss : 3440.66845703125, \t Total Dis Loss : 4.920872243019403e-07\n",
      "Steps : 359400, \t Total Gen Loss : 3996.58203125, \t Total Dis Loss : 1.8682283098314656e-06\n",
      "Steps : 359500, \t Total Gen Loss : 3777.503662109375, \t Total Dis Loss : 1.119473745347932e-05\n",
      "Steps : 359600, \t Total Gen Loss : 3337.59130859375, \t Total Dis Loss : 1.4426791494770441e-05\n",
      "Steps : 359700, \t Total Gen Loss : 3421.771240234375, \t Total Dis Loss : 0.00017133702931459993\n",
      "Steps : 359800, \t Total Gen Loss : 3991.9658203125, \t Total Dis Loss : 4.591236574924551e-06\n",
      "Steps : 359900, \t Total Gen Loss : 3450.019287109375, \t Total Dis Loss : 1.4939496395527385e-05\n",
      "Steps : 360000, \t Total Gen Loss : 3813.05126953125, \t Total Dis Loss : 1.505583441030467e-05\n",
      "Time for epoch 64 is 77.00468873977661 sec\n",
      "Steps : 360100, \t Total Gen Loss : 4045.765869140625, \t Total Dis Loss : 2.313084223715123e-06\n",
      "Steps : 360200, \t Total Gen Loss : 3777.781982421875, \t Total Dis Loss : 2.212257186329225e-06\n",
      "Steps : 360300, \t Total Gen Loss : 3818.385986328125, \t Total Dis Loss : 1.460568341826729e-06\n",
      "Steps : 360400, \t Total Gen Loss : 3524.24072265625, \t Total Dis Loss : 7.579037628602237e-05\n",
      "Steps : 360500, \t Total Gen Loss : 3900.48095703125, \t Total Dis Loss : 5.445780516311061e-06\n",
      "Steps : 360600, \t Total Gen Loss : 3777.13134765625, \t Total Dis Loss : 2.2962071852816734e-06\n",
      "Steps : 360700, \t Total Gen Loss : 3652.23095703125, \t Total Dis Loss : 1.9467245238047326e-06\n",
      "Steps : 360800, \t Total Gen Loss : 3632.2197265625, \t Total Dis Loss : 8.088889444479719e-06\n",
      "Steps : 360900, \t Total Gen Loss : 3599.118896484375, \t Total Dis Loss : 1.1665462125165504e-06\n",
      "Steps : 361000, \t Total Gen Loss : 3303.238525390625, \t Total Dis Loss : 1.8145152580473223e-06\n",
      "Steps : 361100, \t Total Gen Loss : 3720.611328125, \t Total Dis Loss : 6.72496207698714e-06\n",
      "Steps : 361200, \t Total Gen Loss : 3537.68798828125, \t Total Dis Loss : 2.541573167036404e-06\n",
      "Steps : 361300, \t Total Gen Loss : 3405.780517578125, \t Total Dis Loss : 1.3899887107982067e-06\n",
      "Steps : 361400, \t Total Gen Loss : 3423.093017578125, \t Total Dis Loss : 3.7861744317524426e-07\n",
      "Steps : 361500, \t Total Gen Loss : 3550.565673828125, \t Total Dis Loss : 2.692168663998018e-06\n",
      "Steps : 361600, \t Total Gen Loss : 3905.55810546875, \t Total Dis Loss : 1.3501727153197862e-05\n",
      "Steps : 361700, \t Total Gen Loss : 3535.181396484375, \t Total Dis Loss : 2.494544787623454e-06\n",
      "Steps : 361800, \t Total Gen Loss : 3746.293212890625, \t Total Dis Loss : 4.567524683807278e-06\n",
      "Steps : 361900, \t Total Gen Loss : 4079.479736328125, \t Total Dis Loss : 8.432259164692368e-06\n",
      "Steps : 362000, \t Total Gen Loss : 3836.693359375, \t Total Dis Loss : 2.0984192815376446e-05\n",
      "Steps : 362100, \t Total Gen Loss : 3613.17626953125, \t Total Dis Loss : 4.631363117368892e-05\n",
      "Steps : 362200, \t Total Gen Loss : 3725.93017578125, \t Total Dis Loss : 5.323295226844493e-06\n",
      "Steps : 362300, \t Total Gen Loss : 3151.701904296875, \t Total Dis Loss : 1.0399578513897723e-06\n",
      "Steps : 362400, \t Total Gen Loss : 3757.77734375, \t Total Dis Loss : 5.956786480965093e-05\n",
      "Steps : 362500, \t Total Gen Loss : 3413.685791015625, \t Total Dis Loss : 6.672505151072983e-06\n",
      "Steps : 362600, \t Total Gen Loss : 3280.448486328125, \t Total Dis Loss : 3.568890406313585e-06\n",
      "Steps : 362700, \t Total Gen Loss : 3767.543212890625, \t Total Dis Loss : 1.4202466445567552e-05\n",
      "Steps : 362800, \t Total Gen Loss : 3543.647705078125, \t Total Dis Loss : 7.150670171540696e-06\n",
      "Steps : 362900, \t Total Gen Loss : 3595.28759765625, \t Total Dis Loss : 7.006247528806853e-07\n",
      "Steps : 363000, \t Total Gen Loss : 3721.811279296875, \t Total Dis Loss : 6.714638516314153e-07\n",
      "Steps : 363100, \t Total Gen Loss : 3786.05078125, \t Total Dis Loss : 3.7774668726342497e-06\n",
      "Steps : 363200, \t Total Gen Loss : 3627.34716796875, \t Total Dis Loss : 5.743021029047668e-05\n",
      "Steps : 363300, \t Total Gen Loss : 4133.041015625, \t Total Dis Loss : 1.1159484074596548e-06\n",
      "Steps : 363400, \t Total Gen Loss : 3273.6875, \t Total Dis Loss : 3.938217105314834e-06\n",
      "Steps : 363500, \t Total Gen Loss : 3359.053466796875, \t Total Dis Loss : 4.440045086084865e-05\n",
      "Steps : 363600, \t Total Gen Loss : 4026.138916015625, \t Total Dis Loss : 4.529346369963605e-06\n",
      "Steps : 363700, \t Total Gen Loss : 3183.007568359375, \t Total Dis Loss : 0.0001681749417912215\n",
      "Steps : 363800, \t Total Gen Loss : 3520.52734375, \t Total Dis Loss : 5.123880555402138e-07\n",
      "Steps : 363900, \t Total Gen Loss : 3763.744140625, \t Total Dis Loss : 9.395313895765867e-07\n",
      "Steps : 364000, \t Total Gen Loss : 4183.388671875, \t Total Dis Loss : 2.9060688575555105e-06\n",
      "Steps : 364100, \t Total Gen Loss : 3330.3017578125, \t Total Dis Loss : 2.9799018648191122e-06\n",
      "Steps : 364200, \t Total Gen Loss : 3329.87109375, \t Total Dis Loss : 1.77643869392341e-05\n",
      "Steps : 364300, \t Total Gen Loss : 3505.83740234375, \t Total Dis Loss : 0.00022790030925534666\n",
      "Steps : 364400, \t Total Gen Loss : 3712.211181640625, \t Total Dis Loss : 5.083548785478342e-06\n",
      "Steps : 364500, \t Total Gen Loss : 3198.018310546875, \t Total Dis Loss : 4.4643096771324053e-05\n",
      "Steps : 364600, \t Total Gen Loss : 3956.647216796875, \t Total Dis Loss : 0.00013783124450128525\n",
      "Steps : 364700, \t Total Gen Loss : 3123.65966796875, \t Total Dis Loss : 5.458255145640578e-06\n",
      "Steps : 364800, \t Total Gen Loss : 3101.258544921875, \t Total Dis Loss : 1.9323824744788e-05\n",
      "Steps : 364900, \t Total Gen Loss : 2909.08740234375, \t Total Dis Loss : 2.4475306418025866e-05\n",
      "Steps : 365000, \t Total Gen Loss : 3737.8955078125, \t Total Dis Loss : 4.4039959902875125e-05\n",
      "Steps : 365100, \t Total Gen Loss : 3369.75146484375, \t Total Dis Loss : 3.242314778617583e-05\n",
      "Steps : 365200, \t Total Gen Loss : 3726.249755859375, \t Total Dis Loss : 9.642738405091222e-06\n",
      "Steps : 365300, \t Total Gen Loss : 3986.86669921875, \t Total Dis Loss : 9.392784704687074e-05\n",
      "Steps : 365400, \t Total Gen Loss : 4047.331298828125, \t Total Dis Loss : 9.69308166531846e-06\n",
      "Steps : 365500, \t Total Gen Loss : 3291.673583984375, \t Total Dis Loss : 1.0764448234112933e-05\n",
      "Steps : 365600, \t Total Gen Loss : 3501.89697265625, \t Total Dis Loss : 7.078436965457513e-07\n",
      "Time for epoch 65 is 77.67501521110535 sec\n",
      "Steps : 365700, \t Total Gen Loss : 4054.949462890625, \t Total Dis Loss : 3.6125937185715884e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 365800, \t Total Gen Loss : 3618.172607421875, \t Total Dis Loss : 0.00011801940127043054\n",
      "Steps : 365900, \t Total Gen Loss : 3872.738525390625, \t Total Dis Loss : 4.182394604868023e-06\n",
      "Steps : 366000, \t Total Gen Loss : 4181.94140625, \t Total Dis Loss : 8.18524495116435e-06\n",
      "Steps : 366100, \t Total Gen Loss : 3013.420166015625, \t Total Dis Loss : 5.411110578279477e-06\n",
      "Steps : 366200, \t Total Gen Loss : 3697.48046875, \t Total Dis Loss : 7.9573362654628e-07\n",
      "Steps : 366300, \t Total Gen Loss : 3857.148681640625, \t Total Dis Loss : 2.5553561044944217e-06\n",
      "Steps : 366400, \t Total Gen Loss : 3568.056884765625, \t Total Dis Loss : 1.731973679852672e-05\n",
      "Steps : 366500, \t Total Gen Loss : 3739.7685546875, \t Total Dis Loss : 1.9050870832870714e-05\n",
      "Steps : 366600, \t Total Gen Loss : 3143.31201171875, \t Total Dis Loss : 6.210520950844511e-06\n",
      "Steps : 366700, \t Total Gen Loss : 3381.02392578125, \t Total Dis Loss : 0.06342176347970963\n",
      "Steps : 366800, \t Total Gen Loss : 3100.771484375, \t Total Dis Loss : 0.000716820009984076\n",
      "Steps : 366900, \t Total Gen Loss : 3423.44677734375, \t Total Dis Loss : 6.57065993436845e-06\n",
      "Steps : 367000, \t Total Gen Loss : 3452.319580078125, \t Total Dis Loss : 1.796061042114161e-05\n",
      "Steps : 367100, \t Total Gen Loss : 3757.354736328125, \t Total Dis Loss : 7.901772391960549e-07\n",
      "Steps : 367200, \t Total Gen Loss : 3454.064208984375, \t Total Dis Loss : 2.334199052711483e-06\n",
      "Steps : 367300, \t Total Gen Loss : 3870.0078125, \t Total Dis Loss : 7.162941619753838e-05\n",
      "Steps : 367400, \t Total Gen Loss : 3810.404052734375, \t Total Dis Loss : 1.3762707567366306e-05\n",
      "Steps : 367500, \t Total Gen Loss : 3253.72705078125, \t Total Dis Loss : 2.405494342383463e-06\n",
      "Steps : 367600, \t Total Gen Loss : 3260.850341796875, \t Total Dis Loss : 6.369192533384194e-07\n",
      "Steps : 367700, \t Total Gen Loss : 3482.095458984375, \t Total Dis Loss : 0.0013150890590623021\n",
      "Steps : 367800, \t Total Gen Loss : 3836.74658203125, \t Total Dis Loss : 2.0971707272110507e-05\n",
      "Steps : 367900, \t Total Gen Loss : 3483.747314453125, \t Total Dis Loss : 3.547203732523485e-06\n",
      "Steps : 368000, \t Total Gen Loss : 3343.03076171875, \t Total Dis Loss : 2.9870740036130883e-05\n",
      "Steps : 368100, \t Total Gen Loss : 3405.203125, \t Total Dis Loss : 5.149119897396304e-05\n",
      "Steps : 368200, \t Total Gen Loss : 3574.991943359375, \t Total Dis Loss : 9.017609045258723e-06\n",
      "Steps : 368300, \t Total Gen Loss : 3707.509521484375, \t Total Dis Loss : 6.956598099350231e-06\n",
      "Steps : 368400, \t Total Gen Loss : 3385.37255859375, \t Total Dis Loss : 5.559405053645605e-06\n",
      "Steps : 368500, \t Total Gen Loss : 3849.923095703125, \t Total Dis Loss : 1.523796709079761e-05\n",
      "Steps : 368600, \t Total Gen Loss : 3551.606201171875, \t Total Dis Loss : 0.0006406333995983005\n",
      "Steps : 368700, \t Total Gen Loss : 3324.26806640625, \t Total Dis Loss : 4.244932824803982e-06\n",
      "Steps : 368800, \t Total Gen Loss : 3868.088134765625, \t Total Dis Loss : 1.211175572279899e-06\n",
      "Steps : 368900, \t Total Gen Loss : 3497.409423828125, \t Total Dis Loss : 3.4414049423503457e-06\n",
      "Steps : 369000, \t Total Gen Loss : 4018.7236328125, \t Total Dis Loss : 5.819781108584721e-06\n",
      "Steps : 369100, \t Total Gen Loss : 3348.781494140625, \t Total Dis Loss : 3.1602246508555254e-06\n",
      "Steps : 369200, \t Total Gen Loss : 3698.00244140625, \t Total Dis Loss : 3.159449306622264e-06\n",
      "Steps : 369300, \t Total Gen Loss : 3528.2998046875, \t Total Dis Loss : 3.602810693337233e-06\n",
      "Steps : 369400, \t Total Gen Loss : 3915.00537109375, \t Total Dis Loss : 3.579330268621561e-06\n",
      "Steps : 369500, \t Total Gen Loss : 3744.88330078125, \t Total Dis Loss : 7.086018285917817e-06\n",
      "Steps : 369600, \t Total Gen Loss : 3343.361572265625, \t Total Dis Loss : 1.6497247088409495e-06\n",
      "Steps : 369700, \t Total Gen Loss : 3196.424072265625, \t Total Dis Loss : 1.213928044307977e-05\n",
      "Steps : 369800, \t Total Gen Loss : 3901.4921875, \t Total Dis Loss : 7.192625344032422e-05\n",
      "Steps : 369900, \t Total Gen Loss : 3469.296875, \t Total Dis Loss : 7.1238409873330966e-06\n",
      "Steps : 370000, \t Total Gen Loss : 3417.276611328125, \t Total Dis Loss : 4.489948696573265e-05\n",
      "Steps : 370100, \t Total Gen Loss : 3279.482666015625, \t Total Dis Loss : 8.085758963716216e-06\n",
      "Steps : 370200, \t Total Gen Loss : 3862.1826171875, \t Total Dis Loss : 4.36626942246221e-06\n",
      "Steps : 370300, \t Total Gen Loss : 2969.78955078125, \t Total Dis Loss : 1.823410093493294e-05\n",
      "Steps : 370400, \t Total Gen Loss : 3569.827392578125, \t Total Dis Loss : 3.84771828976227e-06\n",
      "Steps : 370500, \t Total Gen Loss : 3146.79345703125, \t Total Dis Loss : 1.4763223816771642e-06\n",
      "Steps : 370600, \t Total Gen Loss : 3636.937255859375, \t Total Dis Loss : 1.9685019651660696e-05\n",
      "Steps : 370700, \t Total Gen Loss : 3588.446044921875, \t Total Dis Loss : 7.139055469451705e-06\n",
      "Steps : 370800, \t Total Gen Loss : 3460.69287109375, \t Total Dis Loss : 6.354306606226601e-06\n",
      "Steps : 370900, \t Total Gen Loss : 3798.84130859375, \t Total Dis Loss : 3.242423917981796e-05\n",
      "Steps : 371000, \t Total Gen Loss : 3190.951171875, \t Total Dis Loss : 6.566363299498335e-05\n",
      "Steps : 371100, \t Total Gen Loss : 3543.490966796875, \t Total Dis Loss : 1.7586095054866746e-05\n",
      "Steps : 371200, \t Total Gen Loss : 3673.451904296875, \t Total Dis Loss : 4.6873378778400365e-06\n",
      "Time for epoch 66 is 76.3957302570343 sec\n",
      "Steps : 371300, \t Total Gen Loss : 3490.386962890625, \t Total Dis Loss : 6.484633195213974e-05\n",
      "Steps : 371400, \t Total Gen Loss : 3183.635009765625, \t Total Dis Loss : 1.49193947436288e-05\n",
      "Steps : 371500, \t Total Gen Loss : 3662.02880859375, \t Total Dis Loss : 3.2026582630351186e-06\n",
      "Steps : 371600, \t Total Gen Loss : 3583.13427734375, \t Total Dis Loss : 2.7778931325883605e-05\n",
      "Steps : 371700, \t Total Gen Loss : 3433.218994140625, \t Total Dis Loss : 4.5106804464012384e-05\n",
      "Steps : 371800, \t Total Gen Loss : 3152.35693359375, \t Total Dis Loss : 4.8540827265242115e-05\n",
      "Steps : 371900, \t Total Gen Loss : 3193.610107421875, \t Total Dis Loss : 2.054187461908441e-05\n",
      "Steps : 372000, \t Total Gen Loss : 3504.94677734375, \t Total Dis Loss : 6.981109436310362e-06\n",
      "Steps : 372100, \t Total Gen Loss : 2882.990234375, \t Total Dis Loss : 4.622498636308592e-06\n",
      "Steps : 372200, \t Total Gen Loss : 2690.580322265625, \t Total Dis Loss : 3.101023366980371e-06\n",
      "Steps : 372300, \t Total Gen Loss : 4388.34814453125, \t Total Dis Loss : 1.2900592082587536e-05\n",
      "Steps : 372400, \t Total Gen Loss : 3743.65380859375, \t Total Dis Loss : 1.0722491424530745e-05\n",
      "Steps : 372500, \t Total Gen Loss : 3071.74853515625, \t Total Dis Loss : 1.4479915080300998e-06\n",
      "Steps : 372600, \t Total Gen Loss : 3170.336181640625, \t Total Dis Loss : 1.0273898624291178e-05\n",
      "Steps : 372700, \t Total Gen Loss : 3946.31201171875, \t Total Dis Loss : 3.351749228386325e-06\n",
      "Steps : 372800, \t Total Gen Loss : 3566.278564453125, \t Total Dis Loss : 3.266432759119198e-05\n",
      "Steps : 372900, \t Total Gen Loss : 3380.549560546875, \t Total Dis Loss : 8.472472472931258e-06\n",
      "Steps : 373000, \t Total Gen Loss : 3358.979736328125, \t Total Dis Loss : 1.5254693153110566e-06\n",
      "Steps : 373100, \t Total Gen Loss : 4085.6923828125, \t Total Dis Loss : 1.129462179960683e-05\n",
      "Steps : 373200, \t Total Gen Loss : 3965.785888671875, \t Total Dis Loss : 1.1681035175570287e-05\n",
      "Steps : 373300, \t Total Gen Loss : 3159.275390625, \t Total Dis Loss : 4.401231763040414e-06\n",
      "Steps : 373400, \t Total Gen Loss : 3203.165771484375, \t Total Dis Loss : 4.924847416987177e-06\n",
      "Steps : 373500, \t Total Gen Loss : 3921.339111328125, \t Total Dis Loss : 5.957957455393625e-06\n",
      "Steps : 373600, \t Total Gen Loss : 4062.86767578125, \t Total Dis Loss : 1.515549115538306e-06\n",
      "Steps : 373700, \t Total Gen Loss : 3138.18701171875, \t Total Dis Loss : 1.0097734957525972e-05\n",
      "Steps : 373800, \t Total Gen Loss : 3274.249267578125, \t Total Dis Loss : 1.1507219142004033e-06\n",
      "Steps : 373900, \t Total Gen Loss : 3212.02978515625, \t Total Dis Loss : 4.515130058280192e-06\n",
      "Steps : 374000, \t Total Gen Loss : 3201.242431640625, \t Total Dis Loss : 1.4179056961438619e-05\n",
      "Steps : 374100, \t Total Gen Loss : 2966.833984375, \t Total Dis Loss : 1.0912001926044468e-05\n",
      "Steps : 374200, \t Total Gen Loss : 3094.74609375, \t Total Dis Loss : 2.850480541383149e-06\n",
      "Steps : 374300, \t Total Gen Loss : 3843.444091796875, \t Total Dis Loss : 3.3126223570434377e-05\n",
      "Steps : 374400, \t Total Gen Loss : 3677.065185546875, \t Total Dis Loss : 6.925057095941156e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 374500, \t Total Gen Loss : 3559.745361328125, \t Total Dis Loss : 2.045080873358529e-05\n",
      "Steps : 374600, \t Total Gen Loss : 3364.0224609375, \t Total Dis Loss : 1.0815580026246607e-05\n",
      "Steps : 374700, \t Total Gen Loss : 3184.030029296875, \t Total Dis Loss : 6.521636532852426e-05\n",
      "Steps : 374800, \t Total Gen Loss : 4168.943359375, \t Total Dis Loss : 8.886270734365098e-06\n",
      "Steps : 374900, \t Total Gen Loss : 3632.8017578125, \t Total Dis Loss : 1.339356185781071e-05\n",
      "Steps : 375000, \t Total Gen Loss : 3606.029052734375, \t Total Dis Loss : 8.22955939838721e-07\n",
      "Steps : 375100, \t Total Gen Loss : 2957.68701171875, \t Total Dis Loss : 3.256017180319759e-06\n",
      "Steps : 375200, \t Total Gen Loss : 3452.88623046875, \t Total Dis Loss : 3.969986664742464e-06\n",
      "Steps : 375300, \t Total Gen Loss : 3637.689208984375, \t Total Dis Loss : 2.926749857579125e-06\n",
      "Steps : 375400, \t Total Gen Loss : 3757.326904296875, \t Total Dis Loss : 1.916757355502341e-06\n",
      "Steps : 375500, \t Total Gen Loss : 3863.076171875, \t Total Dis Loss : 1.0859660505957436e-05\n",
      "Steps : 375600, \t Total Gen Loss : 3535.620361328125, \t Total Dis Loss : 7.444262882927433e-05\n",
      "Steps : 375700, \t Total Gen Loss : 3728.858642578125, \t Total Dis Loss : 6.591304554603994e-06\n",
      "Steps : 375800, \t Total Gen Loss : 3336.54296875, \t Total Dis Loss : 6.772765573259676e-06\n",
      "Steps : 375900, \t Total Gen Loss : 3557.20166015625, \t Total Dis Loss : 2.1765872588730417e-05\n",
      "Steps : 376000, \t Total Gen Loss : 3727.3642578125, \t Total Dis Loss : 5.9887210227316245e-05\n",
      "Steps : 376100, \t Total Gen Loss : 3767.30859375, \t Total Dis Loss : 2.0385539301059907e-06\n",
      "Steps : 376200, \t Total Gen Loss : 3746.281494140625, \t Total Dis Loss : 1.5354514744103653e-06\n",
      "Steps : 376300, \t Total Gen Loss : 4245.8720703125, \t Total Dis Loss : 1.0343150051994598e-06\n",
      "Steps : 376400, \t Total Gen Loss : 3472.230224609375, \t Total Dis Loss : 2.146621000065352e-06\n",
      "Steps : 376500, \t Total Gen Loss : 3785.017333984375, \t Total Dis Loss : 1.528593656985322e-06\n",
      "Steps : 376600, \t Total Gen Loss : 3919.798095703125, \t Total Dis Loss : 7.055171408865135e-06\n",
      "Steps : 376700, \t Total Gen Loss : 3835.0927734375, \t Total Dis Loss : 2.1959267542115413e-06\n",
      "Steps : 376800, \t Total Gen Loss : 3617.58544921875, \t Total Dis Loss : 4.63301921627135e-06\n",
      "Time for epoch 67 is 77.91644763946533 sec\n",
      "Steps : 376900, \t Total Gen Loss : 2960.688232421875, \t Total Dis Loss : 0.00011006248678313568\n",
      "Steps : 377000, \t Total Gen Loss : 4079.587890625, \t Total Dis Loss : 3.3540527510922402e-06\n",
      "Steps : 377100, \t Total Gen Loss : 3395.06884765625, \t Total Dis Loss : 7.58146052248776e-06\n",
      "Steps : 377200, \t Total Gen Loss : 3414.5244140625, \t Total Dis Loss : 1.381265428790357e-05\n",
      "Steps : 377300, \t Total Gen Loss : 3679.926513671875, \t Total Dis Loss : 1.7236985740964883e-06\n",
      "Steps : 377400, \t Total Gen Loss : 3816.0634765625, \t Total Dis Loss : 8.222450560424477e-05\n",
      "Steps : 377500, \t Total Gen Loss : 3590.712158203125, \t Total Dis Loss : 0.004594399593770504\n",
      "Steps : 377600, \t Total Gen Loss : 3762.127197265625, \t Total Dis Loss : 2.005799615290016e-05\n",
      "Steps : 377700, \t Total Gen Loss : 3852.412353515625, \t Total Dis Loss : 4.581584562401986e-06\n",
      "Steps : 377800, \t Total Gen Loss : 3765.69775390625, \t Total Dis Loss : 2.5216264475602657e-05\n",
      "Steps : 377900, \t Total Gen Loss : 3801.572509765625, \t Total Dis Loss : 1.2168263310741168e-05\n",
      "Steps : 378000, \t Total Gen Loss : 3544.159912109375, \t Total Dis Loss : 1.6030445522119408e-06\n",
      "Steps : 378100, \t Total Gen Loss : 3727.315673828125, \t Total Dis Loss : 1.5184773474175017e-05\n",
      "Steps : 378200, \t Total Gen Loss : 3742.206298828125, \t Total Dis Loss : 1.6056810636655428e-05\n",
      "Steps : 378300, \t Total Gen Loss : 3197.096435546875, \t Total Dis Loss : 7.772208846290596e-06\n",
      "Steps : 378400, \t Total Gen Loss : 3847.016845703125, \t Total Dis Loss : 9.797989832804888e-07\n",
      "Steps : 378500, \t Total Gen Loss : 3140.59228515625, \t Total Dis Loss : 4.611097665474517e-06\n",
      "Steps : 378600, \t Total Gen Loss : 3841.928466796875, \t Total Dis Loss : 1.3532807315641548e-05\n",
      "Steps : 378700, \t Total Gen Loss : 3396.122802734375, \t Total Dis Loss : 7.851603731978685e-05\n",
      "Steps : 378800, \t Total Gen Loss : 3748.199951171875, \t Total Dis Loss : 1.1343059668433852e-05\n",
      "Steps : 378900, \t Total Gen Loss : 3906.34130859375, \t Total Dis Loss : 2.782210231089266e-06\n",
      "Steps : 379000, \t Total Gen Loss : 3355.333740234375, \t Total Dis Loss : 6.886964911245741e-06\n",
      "Steps : 379100, \t Total Gen Loss : 3542.186767578125, \t Total Dis Loss : 6.357878987728327e-07\n",
      "Steps : 379200, \t Total Gen Loss : 3847.912841796875, \t Total Dis Loss : 1.279442244594975e-06\n",
      "Steps : 379300, \t Total Gen Loss : 3689.27392578125, \t Total Dis Loss : 1.4508438653138e-05\n",
      "Steps : 379400, \t Total Gen Loss : 3546.41650390625, \t Total Dis Loss : 1.7570690033608116e-05\n",
      "Steps : 379500, \t Total Gen Loss : 3337.52978515625, \t Total Dis Loss : 2.206523504355573e-06\n",
      "Steps : 379600, \t Total Gen Loss : 3782.2138671875, \t Total Dis Loss : 3.109741555817891e-06\n",
      "Steps : 379700, \t Total Gen Loss : 3521.81298828125, \t Total Dis Loss : 2.76970308732416e-06\n",
      "Steps : 379800, \t Total Gen Loss : 3816.119384765625, \t Total Dis Loss : 7.452579779965163e-07\n",
      "Steps : 379900, \t Total Gen Loss : 3227.6796875, \t Total Dis Loss : 4.021334461867809e-05\n",
      "Steps : 380000, \t Total Gen Loss : 3552.08935546875, \t Total Dis Loss : 3.969474619225366e-06\n",
      "Steps : 380100, \t Total Gen Loss : 3818.21923828125, \t Total Dis Loss : 4.963485025655245e-06\n",
      "Steps : 380200, \t Total Gen Loss : 3433.73388671875, \t Total Dis Loss : 4.522409653873183e-06\n",
      "Steps : 380300, \t Total Gen Loss : 3784.931640625, \t Total Dis Loss : 1.963909426194732e-06\n",
      "Steps : 380400, \t Total Gen Loss : 3659.111083984375, \t Total Dis Loss : 2.5365534384036437e-06\n",
      "Steps : 380500, \t Total Gen Loss : 3160.86328125, \t Total Dis Loss : 9.48333581618499e-06\n",
      "Steps : 380600, \t Total Gen Loss : 3072.879638671875, \t Total Dis Loss : 2.750787416516687e-06\n",
      "Steps : 380700, \t Total Gen Loss : 3508.999755859375, \t Total Dis Loss : 6.177017439767951e-06\n",
      "Steps : 380800, \t Total Gen Loss : 3907.92919921875, \t Total Dis Loss : 3.0405066354433075e-05\n",
      "Steps : 380900, \t Total Gen Loss : 3435.318115234375, \t Total Dis Loss : 0.0014008196303620934\n",
      "Steps : 381000, \t Total Gen Loss : 3714.753662109375, \t Total Dis Loss : 1.993365685848403e-06\n",
      "Steps : 381100, \t Total Gen Loss : 3340.49267578125, \t Total Dis Loss : 1.3533733181247953e-05\n",
      "Steps : 381200, \t Total Gen Loss : 3685.7529296875, \t Total Dis Loss : 5.99116037847125e-06\n",
      "Steps : 381300, \t Total Gen Loss : 3433.845458984375, \t Total Dis Loss : 6.11965442658402e-05\n",
      "Steps : 381400, \t Total Gen Loss : 4231.240234375, \t Total Dis Loss : 4.0671620809007436e-05\n",
      "Steps : 381500, \t Total Gen Loss : 3674.38037109375, \t Total Dis Loss : 3.996455234300811e-06\n",
      "Steps : 381600, \t Total Gen Loss : 3521.09814453125, \t Total Dis Loss : 1.2321524081926327e-05\n",
      "Steps : 381700, \t Total Gen Loss : 3487.286376953125, \t Total Dis Loss : 2.7547368517844006e-05\n",
      "Steps : 381800, \t Total Gen Loss : 3879.68017578125, \t Total Dis Loss : 5.2403156587388366e-05\n",
      "Steps : 381900, \t Total Gen Loss : 3579.339111328125, \t Total Dis Loss : 5.7675140851642936e-05\n",
      "Steps : 382000, \t Total Gen Loss : 3903.312744140625, \t Total Dis Loss : 1.4836207355983788e-06\n",
      "Steps : 382100, \t Total Gen Loss : 3565.484619140625, \t Total Dis Loss : 6.050588581274496e-07\n",
      "Steps : 382200, \t Total Gen Loss : 2961.65283203125, \t Total Dis Loss : 1.3114030252836528e-06\n",
      "Steps : 382300, \t Total Gen Loss : 3513.662353515625, \t Total Dis Loss : 3.2594246022199513e-06\n",
      "Steps : 382400, \t Total Gen Loss : 3454.73046875, \t Total Dis Loss : 1.910280843731016e-05\n",
      "Steps : 382500, \t Total Gen Loss : 3691.36572265625, \t Total Dis Loss : 4.625651126843877e-05\n",
      "Time for epoch 68 is 77.21705460548401 sec\n",
      "Steps : 382600, \t Total Gen Loss : 3962.15087890625, \t Total Dis Loss : 7.1918398134585004e-06\n",
      "Steps : 382700, \t Total Gen Loss : 3846.86376953125, \t Total Dis Loss : 6.947234396648128e-06\n",
      "Steps : 382800, \t Total Gen Loss : 3842.331787109375, \t Total Dis Loss : 1.2968243936484214e-06\n",
      "Steps : 382900, \t Total Gen Loss : 3757.3251953125, \t Total Dis Loss : 9.691522109278594e-07\n",
      "Steps : 383000, \t Total Gen Loss : 3145.301025390625, \t Total Dis Loss : 1.050777200362063e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 383100, \t Total Gen Loss : 3413.9892578125, \t Total Dis Loss : 6.180485797813162e-06\n",
      "Steps : 383200, \t Total Gen Loss : 3546.97802734375, \t Total Dis Loss : 5.66013295610901e-06\n",
      "Steps : 383300, \t Total Gen Loss : 3747.6396484375, \t Total Dis Loss : 7.876030281295243e-07\n",
      "Steps : 383400, \t Total Gen Loss : 2914.29638671875, \t Total Dis Loss : 9.095841733142151e-07\n",
      "Steps : 383500, \t Total Gen Loss : 3721.657958984375, \t Total Dis Loss : 6.49852336209733e-06\n",
      "Steps : 383600, \t Total Gen Loss : 3667.689208984375, \t Total Dis Loss : 4.691485628427472e-06\n",
      "Steps : 383700, \t Total Gen Loss : 3171.889892578125, \t Total Dis Loss : 4.549267032416537e-06\n",
      "Steps : 383800, \t Total Gen Loss : 4185.958984375, \t Total Dis Loss : 2.4750683223828673e-05\n",
      "Steps : 383900, \t Total Gen Loss : 4085.0087890625, \t Total Dis Loss : 8.51381537358975e-06\n",
      "Steps : 384000, \t Total Gen Loss : 3227.974853515625, \t Total Dis Loss : 7.655812623852398e-06\n",
      "Steps : 384100, \t Total Gen Loss : 3570.4931640625, \t Total Dis Loss : 5.390716978581622e-06\n",
      "Steps : 384200, \t Total Gen Loss : 3424.612060546875, \t Total Dis Loss : 2.7456126190372743e-06\n",
      "Steps : 384300, \t Total Gen Loss : 3533.68212890625, \t Total Dis Loss : 1.0135052434634417e-05\n",
      "Steps : 384400, \t Total Gen Loss : 3795.541748046875, \t Total Dis Loss : 1.3424112239590613e-06\n",
      "Steps : 384500, \t Total Gen Loss : 4205.4658203125, \t Total Dis Loss : 3.2747163913882105e-07\n",
      "Steps : 384600, \t Total Gen Loss : 3823.83447265625, \t Total Dis Loss : 5.542702183447545e-06\n",
      "Steps : 384700, \t Total Gen Loss : 3156.177734375, \t Total Dis Loss : 0.00027924805181100965\n",
      "Steps : 384800, \t Total Gen Loss : 3337.962890625, \t Total Dis Loss : 5.3655774536309764e-05\n",
      "Steps : 384900, \t Total Gen Loss : 3998.2666015625, \t Total Dis Loss : 5.734112846766948e-07\n",
      "Steps : 385000, \t Total Gen Loss : 4172.2802734375, \t Total Dis Loss : 7.878403266659006e-05\n",
      "Steps : 385100, \t Total Gen Loss : 3394.069091796875, \t Total Dis Loss : 2.5735394046932925e-06\n",
      "Steps : 385200, \t Total Gen Loss : 3905.3525390625, \t Total Dis Loss : 2.8744318115059286e-05\n",
      "Steps : 385300, \t Total Gen Loss : 3820.34619140625, \t Total Dis Loss : 6.770314939785749e-06\n",
      "Steps : 385400, \t Total Gen Loss : 3475.761962890625, \t Total Dis Loss : 0.0002455406647641212\n",
      "Steps : 385500, \t Total Gen Loss : 3785.30419921875, \t Total Dis Loss : 1.8974029671880999e-06\n",
      "Steps : 385600, \t Total Gen Loss : 3251.967529296875, \t Total Dis Loss : 1.1969418665103149e-06\n",
      "Steps : 385700, \t Total Gen Loss : 3039.518310546875, \t Total Dis Loss : 7.852383987483336e-07\n",
      "Steps : 385800, \t Total Gen Loss : 3584.43701171875, \t Total Dis Loss : 2.109391289195628e-06\n",
      "Steps : 385900, \t Total Gen Loss : 3726.81201171875, \t Total Dis Loss : 9.883909797281376e-07\n",
      "Steps : 386000, \t Total Gen Loss : 3782.614990234375, \t Total Dis Loss : 1.6805489622129244e-06\n",
      "Steps : 386100, \t Total Gen Loss : 3675.3955078125, \t Total Dis Loss : 5.381649316404946e-06\n",
      "Steps : 386200, \t Total Gen Loss : 3302.98583984375, \t Total Dis Loss : 9.776073284228914e-07\n",
      "Steps : 386300, \t Total Gen Loss : 3999.697021484375, \t Total Dis Loss : 2.659026904439088e-06\n",
      "Steps : 386400, \t Total Gen Loss : 3864.79345703125, \t Total Dis Loss : 7.151985528253135e-07\n",
      "Steps : 386500, \t Total Gen Loss : 3685.185546875, \t Total Dis Loss : 7.212623813757091e-07\n",
      "Steps : 386600, \t Total Gen Loss : 3357.88427734375, \t Total Dis Loss : 8.472889589938859e-07\n",
      "Steps : 386700, \t Total Gen Loss : 3468.0546875, \t Total Dis Loss : 1.9754359072976513e-06\n",
      "Steps : 386800, \t Total Gen Loss : 3689.30859375, \t Total Dis Loss : 1.490849854235421e-06\n",
      "Steps : 386900, \t Total Gen Loss : 3605.709716796875, \t Total Dis Loss : 1.5313264611904742e-06\n",
      "Steps : 387000, \t Total Gen Loss : 3319.37109375, \t Total Dis Loss : 1.0893485296037397e-06\n",
      "Steps : 387100, \t Total Gen Loss : 3114.21826171875, \t Total Dis Loss : 1.4748076182513614e-06\n",
      "Steps : 387200, \t Total Gen Loss : 3338.85693359375, \t Total Dis Loss : 1.3836177458870225e-05\n",
      "Steps : 387300, \t Total Gen Loss : 4131.8720703125, \t Total Dis Loss : 3.374863581484533e-06\n",
      "Steps : 387400, \t Total Gen Loss : 3540.3740234375, \t Total Dis Loss : 2.832437530742027e-06\n",
      "Steps : 387500, \t Total Gen Loss : 3820.212646484375, \t Total Dis Loss : 3.0153403258736944e-06\n",
      "Steps : 387600, \t Total Gen Loss : 3426.02197265625, \t Total Dis Loss : 8.759285492487834e-07\n",
      "Steps : 387700, \t Total Gen Loss : 3550.776611328125, \t Total Dis Loss : 1.2646856930587091e-06\n",
      "Steps : 387800, \t Total Gen Loss : 3397.06103515625, \t Total Dis Loss : 3.7830108112757443e-07\n",
      "Steps : 387900, \t Total Gen Loss : 3605.208251953125, \t Total Dis Loss : 6.418936777663475e-07\n",
      "Steps : 388000, \t Total Gen Loss : 3182.6884765625, \t Total Dis Loss : 3.4754298212646972e-06\n",
      "Steps : 388100, \t Total Gen Loss : 3509.776611328125, \t Total Dis Loss : 4.060134870087495e-06\n",
      "Time for epoch 69 is 76.42409825325012 sec\n",
      "Steps : 388200, \t Total Gen Loss : 3825.6806640625, \t Total Dis Loss : 7.191113581939135e-06\n",
      "Steps : 388300, \t Total Gen Loss : 4289.9140625, \t Total Dis Loss : 1.580412799739861e-06\n",
      "Steps : 388400, \t Total Gen Loss : 3455.017578125, \t Total Dis Loss : 9.388522244080377e-07\n",
      "Steps : 388500, \t Total Gen Loss : 3369.83935546875, \t Total Dis Loss : 4.227313547744416e-05\n",
      "Steps : 388600, \t Total Gen Loss : 3702.543701171875, \t Total Dis Loss : 2.1324047338566743e-05\n",
      "Steps : 388700, \t Total Gen Loss : 4110.57373046875, \t Total Dis Loss : 1.0837690069820383e-06\n",
      "Steps : 388800, \t Total Gen Loss : 3600.875, \t Total Dis Loss : 9.966859124688199e-07\n",
      "Steps : 388900, \t Total Gen Loss : 3263.6083984375, \t Total Dis Loss : 1.3807168670609826e-06\n",
      "Steps : 389000, \t Total Gen Loss : 4024.736083984375, \t Total Dis Loss : 7.435380666720448e-07\n",
      "Steps : 389100, \t Total Gen Loss : 3135.362060546875, \t Total Dis Loss : 3.388718323549256e-05\n",
      "Steps : 389200, \t Total Gen Loss : 3460.383544921875, \t Total Dis Loss : 2.456457423249958e-06\n",
      "Steps : 389300, \t Total Gen Loss : 3604.4453125, \t Total Dis Loss : 2.419064003333915e-06\n",
      "Steps : 389400, \t Total Gen Loss : 3777.300048828125, \t Total Dis Loss : 1.951513468156918e-06\n",
      "Steps : 389500, \t Total Gen Loss : 3486.2998046875, \t Total Dis Loss : 3.1628512715542456e-06\n",
      "Steps : 389600, \t Total Gen Loss : 3455.63818359375, \t Total Dis Loss : 1.5612620700267144e-05\n",
      "Steps : 389700, \t Total Gen Loss : 2808.674072265625, \t Total Dis Loss : 1.1947874554607552e-05\n",
      "Steps : 389800, \t Total Gen Loss : 3320.992919921875, \t Total Dis Loss : 5.436589617602294e-06\n",
      "Steps : 389900, \t Total Gen Loss : 3294.92626953125, \t Total Dis Loss : 6.625743026233977e-06\n",
      "Steps : 390000, \t Total Gen Loss : 3586.124755859375, \t Total Dis Loss : 2.323632998013636e-06\n",
      "Steps : 390100, \t Total Gen Loss : 3722.294189453125, \t Total Dis Loss : 1.0559893780737184e-05\n",
      "Steps : 390200, \t Total Gen Loss : 3445.5126953125, \t Total Dis Loss : 4.608470590028446e-06\n",
      "Steps : 390300, \t Total Gen Loss : 2996.44140625, \t Total Dis Loss : 5.222333129495382e-06\n",
      "Steps : 390400, \t Total Gen Loss : 3126.345458984375, \t Total Dis Loss : 3.017724566234392e-06\n",
      "Steps : 390500, \t Total Gen Loss : 3678.863525390625, \t Total Dis Loss : 8.300225999846589e-06\n",
      "Steps : 390600, \t Total Gen Loss : 3812.755126953125, \t Total Dis Loss : 2.0726143702631816e-05\n",
      "Steps : 390700, \t Total Gen Loss : 3919.7978515625, \t Total Dis Loss : 1.5604080090270145e-06\n",
      "Steps : 390800, \t Total Gen Loss : 3099.338134765625, \t Total Dis Loss : 4.6495479182340205e-06\n",
      "Steps : 390900, \t Total Gen Loss : 4075.991455078125, \t Total Dis Loss : 1.073450857802527e-05\n",
      "Steps : 391000, \t Total Gen Loss : 3524.47021484375, \t Total Dis Loss : 2.8443064366001636e-05\n",
      "Steps : 391100, \t Total Gen Loss : 3139.075439453125, \t Total Dis Loss : 7.0342621256713755e-06\n",
      "Steps : 391200, \t Total Gen Loss : 3856.0498046875, \t Total Dis Loss : 4.2383413756397204e-07\n",
      "Steps : 391300, \t Total Gen Loss : 3739.75244140625, \t Total Dis Loss : 2.000700533244526e-06\n",
      "Steps : 391400, \t Total Gen Loss : 3585.088623046875, \t Total Dis Loss : 1.059601117958664e-06\n",
      "Steps : 391500, \t Total Gen Loss : 3426.075439453125, \t Total Dis Loss : 1.0598409971862566e-05\n",
      "Steps : 391600, \t Total Gen Loss : 3768.260009765625, \t Total Dis Loss : 2.133438329110504e-06\n",
      "Steps : 391700, \t Total Gen Loss : 3893.851806640625, \t Total Dis Loss : 1.118184627557639e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 391800, \t Total Gen Loss : 3560.966064453125, \t Total Dis Loss : 2.6103371055796742e-06\n",
      "Steps : 391900, \t Total Gen Loss : 3749.714599609375, \t Total Dis Loss : 3.7543603070844256e-07\n",
      "Steps : 392000, \t Total Gen Loss : 3298.748779296875, \t Total Dis Loss : 1.7692894971332862e-06\n",
      "Steps : 392100, \t Total Gen Loss : 3775.93505859375, \t Total Dis Loss : 0.1893516182899475\n",
      "Steps : 392200, \t Total Gen Loss : 3313.421875, \t Total Dis Loss : 6.489199222414754e-06\n",
      "Steps : 392300, \t Total Gen Loss : 3993.335205078125, \t Total Dis Loss : 2.074332996926387e-06\n",
      "Steps : 392400, \t Total Gen Loss : 3892.7041015625, \t Total Dis Loss : 2.8892341106256936e-06\n",
      "Steps : 392500, \t Total Gen Loss : 3626.57763671875, \t Total Dis Loss : 5.750749551225454e-06\n",
      "Steps : 392600, \t Total Gen Loss : 3146.02099609375, \t Total Dis Loss : 8.29520672596118e-07\n",
      "Steps : 392700, \t Total Gen Loss : 3179.3857421875, \t Total Dis Loss : 6.918251074239379e-07\n",
      "Steps : 392800, \t Total Gen Loss : 2609.894287109375, \t Total Dis Loss : 8.145005949700135e-07\n",
      "Steps : 392900, \t Total Gen Loss : 3113.12109375, \t Total Dis Loss : 4.413920578372199e-06\n",
      "Steps : 393000, \t Total Gen Loss : 3335.423095703125, \t Total Dis Loss : 5.7057233789237216e-05\n",
      "Steps : 393100, \t Total Gen Loss : 3616.42822265625, \t Total Dis Loss : 1.033213993650861e-05\n",
      "Steps : 393200, \t Total Gen Loss : 3653.306884765625, \t Total Dis Loss : 2.5746321625774726e-06\n",
      "Steps : 393300, \t Total Gen Loss : 3512.77880859375, \t Total Dis Loss : 1.6329765912814764e-06\n",
      "Steps : 393400, \t Total Gen Loss : 3375.619384765625, \t Total Dis Loss : 6.401768587238621e-06\n",
      "Steps : 393500, \t Total Gen Loss : 3856.80224609375, \t Total Dis Loss : 1.2791755580110475e-05\n",
      "Steps : 393600, \t Total Gen Loss : 3308.592041015625, \t Total Dis Loss : 5.2972441153542604e-06\n",
      "Steps : 393700, \t Total Gen Loss : 4074.822509765625, \t Total Dis Loss : 2.1747049686382525e-05\n",
      "Time for epoch 70 is 76.17437958717346 sec\n",
      "Steps : 393800, \t Total Gen Loss : 3319.006103515625, \t Total Dis Loss : 1.3871516785002314e-05\n",
      "Steps : 393900, \t Total Gen Loss : 3346.25927734375, \t Total Dis Loss : 4.635307959688362e-06\n",
      "Steps : 394000, \t Total Gen Loss : 3858.931640625, \t Total Dis Loss : 2.5141849619103596e-05\n",
      "Steps : 394100, \t Total Gen Loss : 3848.113037109375, \t Total Dis Loss : 4.5943306758999825e-05\n",
      "Steps : 394200, \t Total Gen Loss : 3957.0654296875, \t Total Dis Loss : 0.0024970825761556625\n",
      "Steps : 394300, \t Total Gen Loss : 3463.350341796875, \t Total Dis Loss : 0.00011153024388477206\n",
      "Steps : 394400, \t Total Gen Loss : 4523.23583984375, \t Total Dis Loss : 5.0653638936637435e-06\n",
      "Steps : 394500, \t Total Gen Loss : 3543.097900390625, \t Total Dis Loss : 7.151249155867845e-05\n",
      "Steps : 394600, \t Total Gen Loss : 3129.990478515625, \t Total Dis Loss : 3.798015313805081e-05\n",
      "Steps : 394700, \t Total Gen Loss : 3450.471923828125, \t Total Dis Loss : 2.287278402945958e-05\n",
      "Steps : 394800, \t Total Gen Loss : 3453.39453125, \t Total Dis Loss : 1.0445061889186036e-05\n",
      "Steps : 394900, \t Total Gen Loss : 3180.60888671875, \t Total Dis Loss : 1.9424593119765632e-05\n",
      "Steps : 395000, \t Total Gen Loss : 3880.987548828125, \t Total Dis Loss : 2.7487590159580577e-06\n",
      "Steps : 395100, \t Total Gen Loss : 3219.783935546875, \t Total Dis Loss : 1.969636468857061e-05\n",
      "Steps : 395200, \t Total Gen Loss : 4051.33154296875, \t Total Dis Loss : 0.00010605629358906299\n",
      "Steps : 395300, \t Total Gen Loss : 3081.6181640625, \t Total Dis Loss : 1.5505174815189093e-05\n",
      "Steps : 395400, \t Total Gen Loss : 3158.501220703125, \t Total Dis Loss : 1.1523721695994027e-05\n",
      "Steps : 395500, \t Total Gen Loss : 3516.369384765625, \t Total Dis Loss : 4.379186975711491e-06\n",
      "Steps : 395600, \t Total Gen Loss : 3139.960205078125, \t Total Dis Loss : 9.592501737643033e-05\n",
      "Steps : 395700, \t Total Gen Loss : 4150.6240234375, \t Total Dis Loss : 6.951702289370587e-06\n",
      "Steps : 395800, \t Total Gen Loss : 3741.572998046875, \t Total Dis Loss : 5.544510713662021e-06\n",
      "Steps : 395900, \t Total Gen Loss : 3051.727783203125, \t Total Dis Loss : 4.85428699903423e-07\n",
      "Steps : 396000, \t Total Gen Loss : 3587.035888671875, \t Total Dis Loss : 2.8865938475064468e-06\n",
      "Steps : 396100, \t Total Gen Loss : 3841.841552734375, \t Total Dis Loss : 9.811733434617054e-07\n",
      "Steps : 396200, \t Total Gen Loss : 3509.2392578125, \t Total Dis Loss : 1.6730078868931741e-06\n",
      "Steps : 396300, \t Total Gen Loss : 3545.282470703125, \t Total Dis Loss : 6.414351787498163e-07\n",
      "Steps : 396400, \t Total Gen Loss : 3251.77587890625, \t Total Dis Loss : 7.413193543470697e-07\n",
      "Steps : 396500, \t Total Gen Loss : 3706.036865234375, \t Total Dis Loss : 8.440764531769673e-07\n",
      "Steps : 396600, \t Total Gen Loss : 3382.36865234375, \t Total Dis Loss : 6.541031325468794e-07\n",
      "Steps : 396700, \t Total Gen Loss : 3475.396240234375, \t Total Dis Loss : 4.71318287509348e-07\n",
      "Steps : 396800, \t Total Gen Loss : 3196.024658203125, \t Total Dis Loss : 2.7812532152893255e-06\n",
      "Steps : 396900, \t Total Gen Loss : 3778.021484375, \t Total Dis Loss : 0.0009869597852230072\n",
      "Steps : 397000, \t Total Gen Loss : 3340.99169921875, \t Total Dis Loss : 7.483627996407449e-05\n",
      "Steps : 397100, \t Total Gen Loss : 3826.69482421875, \t Total Dis Loss : 4.5587825297843665e-05\n",
      "Steps : 397200, \t Total Gen Loss : 3771.85888671875, \t Total Dis Loss : 1.2542074728116859e-05\n",
      "Steps : 397300, \t Total Gen Loss : 3610.869140625, \t Total Dis Loss : 7.341425225604326e-05\n",
      "Steps : 397400, \t Total Gen Loss : 3254.151611328125, \t Total Dis Loss : 1.1560888196981978e-05\n",
      "Steps : 397500, \t Total Gen Loss : 3240.61474609375, \t Total Dis Loss : 9.682380550657399e-06\n",
      "Steps : 397600, \t Total Gen Loss : 3698.115966796875, \t Total Dis Loss : 0.00012753123883157969\n",
      "Steps : 397700, \t Total Gen Loss : 3810.091064453125, \t Total Dis Loss : 2.6549698759481544e-06\n",
      "Steps : 397800, \t Total Gen Loss : 3826.765869140625, \t Total Dis Loss : 4.996119241695851e-05\n",
      "Steps : 397900, \t Total Gen Loss : 3465.837646484375, \t Total Dis Loss : 2.2306321625364944e-06\n",
      "Steps : 398000, \t Total Gen Loss : 3096.6298828125, \t Total Dis Loss : 1.5639148841728456e-05\n",
      "Steps : 398100, \t Total Gen Loss : 3092.4111328125, \t Total Dis Loss : 4.052261829201598e-06\n",
      "Steps : 398200, \t Total Gen Loss : 3916.178955078125, \t Total Dis Loss : 3.8676121221215e-06\n",
      "Steps : 398300, \t Total Gen Loss : 3456.218505859375, \t Total Dis Loss : 7.914402431197232e-07\n",
      "Steps : 398400, \t Total Gen Loss : 3776.512451171875, \t Total Dis Loss : 2.338234253329574e-06\n",
      "Steps : 398500, \t Total Gen Loss : 3482.664306640625, \t Total Dis Loss : 7.025027116469573e-06\n",
      "Steps : 398600, \t Total Gen Loss : 4282.9990234375, \t Total Dis Loss : 4.085934506292688e-06\n",
      "Steps : 398700, \t Total Gen Loss : 3420.47314453125, \t Total Dis Loss : 1.0313775419490412e-05\n",
      "Steps : 398800, \t Total Gen Loss : 3823.22705078125, \t Total Dis Loss : 1.702310555629083e-06\n",
      "Steps : 398900, \t Total Gen Loss : 3532.46728515625, \t Total Dis Loss : 3.244928848289419e-06\n",
      "Steps : 399000, \t Total Gen Loss : 3279.130859375, \t Total Dis Loss : 9.651187428971753e-05\n",
      "Steps : 399100, \t Total Gen Loss : 3792.4248046875, \t Total Dis Loss : 5.753613368142396e-05\n",
      "Steps : 399200, \t Total Gen Loss : 3653.833984375, \t Total Dis Loss : 2.2466645532404073e-05\n",
      "Steps : 399300, \t Total Gen Loss : 3613.38427734375, \t Total Dis Loss : 2.2796541088609956e-05\n",
      "Time for epoch 71 is 75.02948188781738 sec\n",
      "Steps : 399400, \t Total Gen Loss : 3959.513671875, \t Total Dis Loss : 2.4415285224677064e-05\n",
      "Steps : 399500, \t Total Gen Loss : 3240.6923828125, \t Total Dis Loss : 0.00017649814253672957\n",
      "Steps : 399600, \t Total Gen Loss : 4072.6943359375, \t Total Dis Loss : 2.828094693541061e-06\n",
      "Steps : 399700, \t Total Gen Loss : 3323.731689453125, \t Total Dis Loss : 4.341203293733997e-06\n",
      "Steps : 399800, \t Total Gen Loss : 3653.030029296875, \t Total Dis Loss : 1.6600100934738293e-05\n",
      "Steps : 399900, \t Total Gen Loss : 3215.61328125, \t Total Dis Loss : 0.0036085143219679594\n",
      "Steps : 400000, \t Total Gen Loss : 3985.19970703125, \t Total Dis Loss : 3.7263591366354376e-05\n",
      "Steps : 400100, \t Total Gen Loss : 3154.618896484375, \t Total Dis Loss : 6.9851289481448475e-06\n",
      "Steps : 400200, \t Total Gen Loss : 3613.046142578125, \t Total Dis Loss : 0.003235700773075223\n",
      "Steps : 400300, \t Total Gen Loss : 3481.984375, \t Total Dis Loss : 8.526749297743663e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 400400, \t Total Gen Loss : 3062.201416015625, \t Total Dis Loss : 3.077680594287813e-05\n",
      "Steps : 400500, \t Total Gen Loss : 3491.5234375, \t Total Dis Loss : 1.1805623216787353e-05\n",
      "Steps : 400600, \t Total Gen Loss : 3229.481689453125, \t Total Dis Loss : 0.6056514382362366\n",
      "Steps : 400700, \t Total Gen Loss : 3154.7119140625, \t Total Dis Loss : 0.00019346534099895507\n",
      "Steps : 400800, \t Total Gen Loss : 3709.143310546875, \t Total Dis Loss : 2.3949214664753526e-05\n",
      "Steps : 400900, \t Total Gen Loss : 4318.74658203125, \t Total Dis Loss : 2.2730648197466508e-05\n",
      "Steps : 401000, \t Total Gen Loss : 3243.122314453125, \t Total Dis Loss : 1.468260325054871e-05\n",
      "Steps : 401100, \t Total Gen Loss : 2857.60400390625, \t Total Dis Loss : 1.479141883464763e-05\n",
      "Steps : 401200, \t Total Gen Loss : 3404.85498046875, \t Total Dis Loss : 1.2540935131255537e-05\n",
      "Steps : 401300, \t Total Gen Loss : 3513.489013671875, \t Total Dis Loss : 2.847157247742871e-06\n",
      "Steps : 401400, \t Total Gen Loss : 3611.209228515625, \t Total Dis Loss : 0.00011212932440685108\n",
      "Steps : 401500, \t Total Gen Loss : 3715.1591796875, \t Total Dis Loss : 2.4750621378188953e-05\n",
      "Steps : 401600, \t Total Gen Loss : 3320.080322265625, \t Total Dis Loss : 4.449569314601831e-05\n",
      "Steps : 401700, \t Total Gen Loss : 3353.677978515625, \t Total Dis Loss : 1.0689387636375614e-05\n",
      "Steps : 401800, \t Total Gen Loss : 3702.152099609375, \t Total Dis Loss : 2.767700789263472e-05\n",
      "Steps : 401900, \t Total Gen Loss : 2998.719482421875, \t Total Dis Loss : 3.327169906697236e-05\n",
      "Steps : 402000, \t Total Gen Loss : 3171.95458984375, \t Total Dis Loss : 5.599939322564751e-05\n",
      "Steps : 402100, \t Total Gen Loss : 3342.212890625, \t Total Dis Loss : 0.00014761621423531324\n",
      "Steps : 402200, \t Total Gen Loss : 3445.3203125, \t Total Dis Loss : 5.06566493641003e-06\n",
      "Steps : 402300, \t Total Gen Loss : 2861.60986328125, \t Total Dis Loss : 1.292725937673822e-05\n",
      "Steps : 402400, \t Total Gen Loss : 3749.332275390625, \t Total Dis Loss : 6.1559603636851534e-06\n",
      "Steps : 402500, \t Total Gen Loss : 3827.63134765625, \t Total Dis Loss : 1.8298858321941225e-06\n",
      "Steps : 402600, \t Total Gen Loss : 4286.46923828125, \t Total Dis Loss : 1.1701151834131451e-06\n",
      "Steps : 402700, \t Total Gen Loss : 3401.78173828125, \t Total Dis Loss : 1.5052681874294649e-06\n",
      "Steps : 402800, \t Total Gen Loss : 3870.016357421875, \t Total Dis Loss : 1.212046640830522e-06\n",
      "Steps : 402900, \t Total Gen Loss : 3341.13037109375, \t Total Dis Loss : 8.765770758145663e-07\n",
      "Steps : 403000, \t Total Gen Loss : 3285.490966796875, \t Total Dis Loss : 2.078895704471506e-06\n",
      "Steps : 403100, \t Total Gen Loss : 3657.558837890625, \t Total Dis Loss : 7.898939884398715e-07\n",
      "Steps : 403200, \t Total Gen Loss : 3166.193603515625, \t Total Dis Loss : 3.3330434234812856e-06\n",
      "Steps : 403300, \t Total Gen Loss : 3331.031982421875, \t Total Dis Loss : 5.08081848238362e-06\n",
      "Steps : 403400, \t Total Gen Loss : 2955.847412109375, \t Total Dis Loss : 7.080030286488181e-07\n",
      "Steps : 403500, \t Total Gen Loss : 4055.25341796875, \t Total Dis Loss : 2.612181106087519e-06\n",
      "Steps : 403600, \t Total Gen Loss : 4244.3681640625, \t Total Dis Loss : 4.632386207958916e-06\n",
      "Steps : 403700, \t Total Gen Loss : 3167.70703125, \t Total Dis Loss : 8.925656402425375e-06\n",
      "Steps : 403800, \t Total Gen Loss : 4253.18701171875, \t Total Dis Loss : 2.1815953914483543e-06\n",
      "Steps : 403900, \t Total Gen Loss : 4001.099853515625, \t Total Dis Loss : 1.686201176198665e-05\n",
      "Steps : 404000, \t Total Gen Loss : 3796.843017578125, \t Total Dis Loss : 3.97378562411177e-06\n",
      "Steps : 404100, \t Total Gen Loss : 3252.575439453125, \t Total Dis Loss : 5.513665200851392e-06\n",
      "Steps : 404200, \t Total Gen Loss : 3457.89111328125, \t Total Dis Loss : 3.801243053658254e-07\n",
      "Steps : 404300, \t Total Gen Loss : 3736.5361328125, \t Total Dis Loss : 8.934608217714413e-07\n",
      "Steps : 404400, \t Total Gen Loss : 3557.73583984375, \t Total Dis Loss : 1.0474436749063898e-05\n",
      "Steps : 404500, \t Total Gen Loss : 3111.48779296875, \t Total Dis Loss : 1.145940700553183e-06\n",
      "Steps : 404600, \t Total Gen Loss : 3624.18896484375, \t Total Dis Loss : 5.9305766626494005e-05\n",
      "Steps : 404700, \t Total Gen Loss : 3274.140625, \t Total Dis Loss : 7.036346687527839e-06\n",
      "Steps : 404800, \t Total Gen Loss : 3622.41357421875, \t Total Dis Loss : 2.343172809560201e-06\n",
      "Steps : 404900, \t Total Gen Loss : 3380.469970703125, \t Total Dis Loss : 1.7810541976359673e-05\n",
      "Steps : 405000, \t Total Gen Loss : 3160.9453125, \t Total Dis Loss : 1.085782400878088e-06\n",
      "Time for epoch 72 is 75.85194683074951 sec\n",
      "Steps : 405100, \t Total Gen Loss : 3586.996337890625, \t Total Dis Loss : 2.2256455849856138e-05\n",
      "Steps : 405200, \t Total Gen Loss : 3456.26953125, \t Total Dis Loss : 1.3889515457776724e-06\n",
      "Steps : 405300, \t Total Gen Loss : 3824.433837890625, \t Total Dis Loss : 3.0500286811729893e-06\n",
      "Steps : 405400, \t Total Gen Loss : 3519.04736328125, \t Total Dis Loss : 1.9741664800676517e-05\n",
      "Steps : 405500, \t Total Gen Loss : 3135.779541015625, \t Total Dis Loss : 2.1226687749731354e-05\n",
      "Steps : 405600, \t Total Gen Loss : 3253.140625, \t Total Dis Loss : 1.2839397641073447e-05\n",
      "Steps : 405700, \t Total Gen Loss : 3716.164306640625, \t Total Dis Loss : 6.8727435973414686e-06\n",
      "Steps : 405800, \t Total Gen Loss : 3858.63818359375, \t Total Dis Loss : 2.1657600882463157e-05\n",
      "Steps : 405900, \t Total Gen Loss : 3512.539794921875, \t Total Dis Loss : 2.3401131329592317e-05\n",
      "Steps : 406000, \t Total Gen Loss : 3310.9521484375, \t Total Dis Loss : 6.713827133353334e-06\n",
      "Steps : 406100, \t Total Gen Loss : 3342.180419921875, \t Total Dis Loss : 1.4236375136533752e-06\n",
      "Steps : 406200, \t Total Gen Loss : 3363.314208984375, \t Total Dis Loss : 2.863229383365251e-06\n",
      "Steps : 406300, \t Total Gen Loss : 3512.468017578125, \t Total Dis Loss : 7.280957106559072e-06\n",
      "Steps : 406400, \t Total Gen Loss : 3477.407470703125, \t Total Dis Loss : 1.4124620065558702e-05\n",
      "Steps : 406500, \t Total Gen Loss : 4415.0625, \t Total Dis Loss : 9.856677934294567e-05\n",
      "Steps : 406600, \t Total Gen Loss : 3392.255126953125, \t Total Dis Loss : 6.151220532046864e-06\n",
      "Steps : 406700, \t Total Gen Loss : 3151.913330078125, \t Total Dis Loss : 9.851620234258007e-06\n",
      "Steps : 406800, \t Total Gen Loss : 3913.03515625, \t Total Dis Loss : 4.986495696357451e-06\n",
      "Steps : 406900, \t Total Gen Loss : 3166.40380859375, \t Total Dis Loss : 7.752156307105906e-06\n",
      "Steps : 407000, \t Total Gen Loss : 3838.813720703125, \t Total Dis Loss : 1.0362169859945425e-06\n",
      "Steps : 407100, \t Total Gen Loss : 3531.650146484375, \t Total Dis Loss : 2.142392986570485e-06\n",
      "Steps : 407200, \t Total Gen Loss : 3619.169921875, \t Total Dis Loss : 5.0110058509744704e-05\n",
      "Steps : 407300, \t Total Gen Loss : 3945.02685546875, \t Total Dis Loss : 2.167712227674201e-05\n",
      "Steps : 407400, \t Total Gen Loss : 4316.8134765625, \t Total Dis Loss : 5.508213507710025e-05\n",
      "Steps : 407500, \t Total Gen Loss : 3081.09912109375, \t Total Dis Loss : 5.520503691514023e-06\n",
      "Steps : 407600, \t Total Gen Loss : 3852.38427734375, \t Total Dis Loss : 8.415681804763153e-06\n",
      "Steps : 407700, \t Total Gen Loss : 3871.456298828125, \t Total Dis Loss : 2.0376059183035977e-05\n",
      "Steps : 407800, \t Total Gen Loss : 4070.088623046875, \t Total Dis Loss : 3.506723032842274e-06\n",
      "Steps : 407900, \t Total Gen Loss : 3846.789306640625, \t Total Dis Loss : 3.5244488572061528e-06\n",
      "Steps : 408000, \t Total Gen Loss : 4110.04931640625, \t Total Dis Loss : 6.452294655900914e-06\n",
      "Steps : 408100, \t Total Gen Loss : 3620.083984375, \t Total Dis Loss : 1.4563486729457509e-05\n",
      "Steps : 408200, \t Total Gen Loss : 3555.76171875, \t Total Dis Loss : 1.3350339031603653e-05\n",
      "Steps : 408300, \t Total Gen Loss : 3513.76513671875, \t Total Dis Loss : 5.474796489579603e-05\n",
      "Steps : 408400, \t Total Gen Loss : 3862.028076171875, \t Total Dis Loss : 0.0011787755647674203\n",
      "Steps : 408500, \t Total Gen Loss : 3363.267333984375, \t Total Dis Loss : 7.848786481190473e-06\n",
      "Steps : 408600, \t Total Gen Loss : 4332.734375, \t Total Dis Loss : 3.1948889045452233e-06\n",
      "Steps : 408700, \t Total Gen Loss : 3507.386962890625, \t Total Dis Loss : 2.7155096177011728e-05\n",
      "Steps : 408800, \t Total Gen Loss : 3135.092529296875, \t Total Dis Loss : 1.4506757906929124e-05\n",
      "Steps : 408900, \t Total Gen Loss : 3478.375732421875, \t Total Dis Loss : 1.7608534108148888e-05\n",
      "Steps : 409000, \t Total Gen Loss : 3624.417724609375, \t Total Dis Loss : 5.056054760643747e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 409100, \t Total Gen Loss : 3019.254150390625, \t Total Dis Loss : 9.076626156456769e-06\n",
      "Steps : 409200, \t Total Gen Loss : 3338.72998046875, \t Total Dis Loss : 1.1228243238292634e-05\n",
      "Steps : 409300, \t Total Gen Loss : 3691.0634765625, \t Total Dis Loss : 3.472611297183903e-06\n",
      "Steps : 409400, \t Total Gen Loss : 3737.958984375, \t Total Dis Loss : 1.2014197636744939e-05\n",
      "Steps : 409500, \t Total Gen Loss : 3330.55517578125, \t Total Dis Loss : 6.319313979474828e-05\n",
      "Steps : 409600, \t Total Gen Loss : 3864.765625, \t Total Dis Loss : 5.7980374549515545e-05\n",
      "Steps : 409700, \t Total Gen Loss : 3609.75390625, \t Total Dis Loss : 4.20520063926233e-06\n",
      "Steps : 409800, \t Total Gen Loss : 3265.33447265625, \t Total Dis Loss : 1.838503885664977e-05\n",
      "Steps : 409900, \t Total Gen Loss : 3235.08642578125, \t Total Dis Loss : 4.396747681312263e-05\n",
      "Steps : 410000, \t Total Gen Loss : 3060.6904296875, \t Total Dis Loss : 4.3389740312704816e-05\n",
      "Steps : 410100, \t Total Gen Loss : 3725.415771484375, \t Total Dis Loss : 1.8399403415969573e-05\n",
      "Steps : 410200, \t Total Gen Loss : 3884.115234375, \t Total Dis Loss : 9.540178780298447e-07\n",
      "Steps : 410300, \t Total Gen Loss : 4061.185546875, \t Total Dis Loss : 1.3443679563351907e-05\n",
      "Steps : 410400, \t Total Gen Loss : 3292.67431640625, \t Total Dis Loss : 1.3730544196732808e-05\n",
      "Steps : 410500, \t Total Gen Loss : 4325.78564453125, \t Total Dis Loss : 7.617691608174937e-06\n",
      "Steps : 410600, \t Total Gen Loss : 3607.4091796875, \t Total Dis Loss : 1.1601750884437934e-05\n",
      "Time for epoch 73 is 75.18903589248657 sec\n",
      "Steps : 410700, \t Total Gen Loss : 3230.256103515625, \t Total Dis Loss : 1.5190419162536273e-06\n",
      "Steps : 410800, \t Total Gen Loss : 3637.251708984375, \t Total Dis Loss : 1.2232977496751118e-05\n",
      "Steps : 410900, \t Total Gen Loss : 4050.562744140625, \t Total Dis Loss : 1.1801437267422443e-06\n",
      "Steps : 411000, \t Total Gen Loss : 3431.1435546875, \t Total Dis Loss : 1.759604856488295e-06\n",
      "Steps : 411100, \t Total Gen Loss : 3603.719482421875, \t Total Dis Loss : 7.867305612307973e-06\n",
      "Steps : 411200, \t Total Gen Loss : 3708.50146484375, \t Total Dis Loss : 7.4480717557889875e-06\n",
      "Steps : 411300, \t Total Gen Loss : 3606.11962890625, \t Total Dis Loss : 1.4902887414791621e-05\n",
      "Steps : 411400, \t Total Gen Loss : 3411.90625, \t Total Dis Loss : 3.6441801967157517e-06\n",
      "Steps : 411500, \t Total Gen Loss : 3772.407958984375, \t Total Dis Loss : 9.141694135905709e-06\n",
      "Steps : 411600, \t Total Gen Loss : 3797.581787109375, \t Total Dis Loss : 1.6763642634032294e-06\n",
      "Steps : 411700, \t Total Gen Loss : 3248.23583984375, \t Total Dis Loss : 0.0029654225800186396\n",
      "Steps : 411800, \t Total Gen Loss : 3594.64892578125, \t Total Dis Loss : 6.160687462397618e-06\n",
      "Steps : 411900, \t Total Gen Loss : 4243.9658203125, \t Total Dis Loss : 9.715521628095303e-06\n",
      "Steps : 412000, \t Total Gen Loss : 3958.6044921875, \t Total Dis Loss : 6.151055458758492e-06\n",
      "Steps : 412100, \t Total Gen Loss : 3351.099365234375, \t Total Dis Loss : 1.57870144903427e-05\n",
      "Steps : 412200, \t Total Gen Loss : 3287.8955078125, \t Total Dis Loss : 1.9652039554785006e-05\n",
      "Steps : 412300, \t Total Gen Loss : 3623.76123046875, \t Total Dis Loss : 6.932612450327724e-06\n",
      "Steps : 412400, \t Total Gen Loss : 3401.27685546875, \t Total Dis Loss : 3.9852871850598603e-05\n",
      "Steps : 412500, \t Total Gen Loss : 3379.3720703125, \t Total Dis Loss : 2.6234683900838718e-06\n",
      "Steps : 412600, \t Total Gen Loss : 4255.91015625, \t Total Dis Loss : 0.32083362340927124\n",
      "Steps : 412700, \t Total Gen Loss : 3847.7333984375, \t Total Dis Loss : 4.8818263167049736e-05\n",
      "Steps : 412800, \t Total Gen Loss : 3701.07421875, \t Total Dis Loss : 5.522752690012567e-05\n",
      "Steps : 412900, \t Total Gen Loss : 3876.102294921875, \t Total Dis Loss : 3.0442042771028355e-05\n",
      "Steps : 413000, \t Total Gen Loss : 3419.0517578125, \t Total Dis Loss : 2.8562586521729827e-05\n",
      "Steps : 413100, \t Total Gen Loss : 2989.58203125, \t Total Dis Loss : 8.482801604259294e-06\n",
      "Steps : 413200, \t Total Gen Loss : 4119.95166015625, \t Total Dis Loss : 3.297214425401762e-05\n",
      "Steps : 413300, \t Total Gen Loss : 3132.618896484375, \t Total Dis Loss : 3.125677540083416e-05\n",
      "Steps : 413400, \t Total Gen Loss : 3079.501708984375, \t Total Dis Loss : 7.013761205598712e-05\n",
      "Steps : 413500, \t Total Gen Loss : 3454.94873046875, \t Total Dis Loss : 1.8567587176221423e-05\n",
      "Steps : 413600, \t Total Gen Loss : 3773.697265625, \t Total Dis Loss : 5.640399831463583e-05\n",
      "Steps : 413700, \t Total Gen Loss : 3477.9287109375, \t Total Dis Loss : 9.106239303946495e-06\n",
      "Steps : 413800, \t Total Gen Loss : 3538.8427734375, \t Total Dis Loss : 2.745433448581025e-06\n",
      "Steps : 413900, \t Total Gen Loss : 3570.659912109375, \t Total Dis Loss : 1.677817272138782e-05\n",
      "Steps : 414000, \t Total Gen Loss : 3495.352294921875, \t Total Dis Loss : 7.3287051236547995e-06\n",
      "Steps : 414100, \t Total Gen Loss : 3648.544921875, \t Total Dis Loss : 7.546365395683097e-06\n",
      "Steps : 414200, \t Total Gen Loss : 3906.85009765625, \t Total Dis Loss : 5.6268003390869126e-05\n",
      "Steps : 414300, \t Total Gen Loss : 3513.820556640625, \t Total Dis Loss : 7.734269456705078e-05\n",
      "Steps : 414400, \t Total Gen Loss : 3828.6015625, \t Total Dis Loss : 1.3593778021459002e-05\n",
      "Steps : 414500, \t Total Gen Loss : 3762.867431640625, \t Total Dis Loss : 7.58687292545801e-06\n",
      "Steps : 414600, \t Total Gen Loss : 3470.469970703125, \t Total Dis Loss : 1.3589978152594995e-05\n",
      "Steps : 414700, \t Total Gen Loss : 3497.65576171875, \t Total Dis Loss : 7.468296644219663e-06\n",
      "Steps : 414800, \t Total Gen Loss : 3611.346923828125, \t Total Dis Loss : 3.8285103073576465e-05\n",
      "Steps : 414900, \t Total Gen Loss : 3966.833251953125, \t Total Dis Loss : 0.00027017213869839907\n",
      "Steps : 415000, \t Total Gen Loss : 3296.517333984375, \t Total Dis Loss : 5.665934622811619e-06\n",
      "Steps : 415100, \t Total Gen Loss : 3108.505859375, \t Total Dis Loss : 1.0585933523543645e-05\n",
      "Steps : 415200, \t Total Gen Loss : 3516.681640625, \t Total Dis Loss : 7.482600267394446e-06\n",
      "Steps : 415300, \t Total Gen Loss : 3798.25439453125, \t Total Dis Loss : 5.904180579818785e-05\n",
      "Steps : 415400, \t Total Gen Loss : 3852.1494140625, \t Total Dis Loss : 5.050199615652673e-05\n",
      "Steps : 415500, \t Total Gen Loss : 3298.44287109375, \t Total Dis Loss : 1.0166937499889173e-05\n",
      "Steps : 415600, \t Total Gen Loss : 3562.5458984375, \t Total Dis Loss : 7.855078365537338e-06\n",
      "Steps : 415700, \t Total Gen Loss : 3824.99072265625, \t Total Dis Loss : 5.63815683563007e-06\n",
      "Steps : 415800, \t Total Gen Loss : 3443.794189453125, \t Total Dis Loss : 0.0004055495373904705\n",
      "Steps : 415900, \t Total Gen Loss : 3483.43603515625, \t Total Dis Loss : 8.14998202258721e-06\n",
      "Steps : 416000, \t Total Gen Loss : 3575.7119140625, \t Total Dis Loss : 1.6351061276509427e-05\n",
      "Steps : 416100, \t Total Gen Loss : 3575.5380859375, \t Total Dis Loss : 2.1832456695847213e-05\n",
      "Steps : 416200, \t Total Gen Loss : 3438.43408203125, \t Total Dis Loss : 8.532786523574032e-06\n",
      "Time for epoch 74 is 74.98841571807861 sec\n",
      "Steps : 416300, \t Total Gen Loss : 3254.958740234375, \t Total Dis Loss : 1.3993560969538521e-05\n",
      "Steps : 416400, \t Total Gen Loss : 3824.853759765625, \t Total Dis Loss : 0.0002443460398353636\n",
      "Steps : 416500, \t Total Gen Loss : 3458.741943359375, \t Total Dis Loss : 2.7707788831321523e-05\n",
      "Steps : 416600, \t Total Gen Loss : 3513.287353515625, \t Total Dis Loss : 1.713645178824663e-05\n",
      "Steps : 416700, \t Total Gen Loss : 3517.840576171875, \t Total Dis Loss : 2.4438022592221387e-06\n",
      "Steps : 416800, \t Total Gen Loss : 3484.639404296875, \t Total Dis Loss : 1.8635716969583882e-06\n",
      "Steps : 416900, \t Total Gen Loss : 3442.330810546875, \t Total Dis Loss : 1.2153584975749254e-05\n",
      "Steps : 417000, \t Total Gen Loss : 3795.7568359375, \t Total Dis Loss : 6.471523647633148e-06\n",
      "Steps : 417100, \t Total Gen Loss : 4068.06884765625, \t Total Dis Loss : 1.6915532796701882e-06\n",
      "Steps : 417200, \t Total Gen Loss : 3355.734130859375, \t Total Dis Loss : 3.2368500342272455e-06\n",
      "Steps : 417300, \t Total Gen Loss : 3163.345458984375, \t Total Dis Loss : 3.823522547463654e-06\n",
      "Steps : 417400, \t Total Gen Loss : 3022.625244140625, \t Total Dis Loss : 8.780553798715118e-06\n",
      "Steps : 417500, \t Total Gen Loss : 3717.224365234375, \t Total Dis Loss : 7.757269031571923e-07\n",
      "Steps : 417600, \t Total Gen Loss : 3636.080078125, \t Total Dis Loss : 3.108260671069729e-06\n",
      "Steps : 417700, \t Total Gen Loss : 3998.72119140625, \t Total Dis Loss : 3.0901826448825886e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 417800, \t Total Gen Loss : 3830.622314453125, \t Total Dis Loss : 1.7798749468056485e-05\n",
      "Steps : 417900, \t Total Gen Loss : 3538.044921875, \t Total Dis Loss : 5.363123818824533e-06\n",
      "Steps : 418000, \t Total Gen Loss : 3248.666748046875, \t Total Dis Loss : 0.0001629520411370322\n",
      "Steps : 418100, \t Total Gen Loss : 3442.806640625, \t Total Dis Loss : 0.0002355832839384675\n",
      "Steps : 418200, \t Total Gen Loss : 3429.6279296875, \t Total Dis Loss : 8.990654350782279e-06\n",
      "Steps : 418300, \t Total Gen Loss : 3220.269775390625, \t Total Dis Loss : 7.606574854435166e-06\n",
      "Steps : 418400, \t Total Gen Loss : 3712.333984375, \t Total Dis Loss : 2.200688641096349e-06\n",
      "Steps : 418500, \t Total Gen Loss : 3550.119140625, \t Total Dis Loss : 3.911078238161281e-06\n",
      "Steps : 418600, \t Total Gen Loss : 3712.460205078125, \t Total Dis Loss : 5.014813723391853e-06\n",
      "Steps : 418700, \t Total Gen Loss : 3543.345947265625, \t Total Dis Loss : 2.3412046630255645e-06\n",
      "Steps : 418800, \t Total Gen Loss : 3504.9013671875, \t Total Dis Loss : 5.515207158168778e-05\n",
      "Steps : 418900, \t Total Gen Loss : 3545.40625, \t Total Dis Loss : 8.64640787767712e-06\n",
      "Steps : 419000, \t Total Gen Loss : 3856.94970703125, \t Total Dis Loss : 4.6113840653561056e-05\n",
      "Steps : 419100, \t Total Gen Loss : 3770.681884765625, \t Total Dis Loss : 1.3994194887345657e-05\n",
      "Steps : 419200, \t Total Gen Loss : 3620.614501953125, \t Total Dis Loss : 4.3087675294373184e-06\n",
      "Steps : 419300, \t Total Gen Loss : 3439.25830078125, \t Total Dis Loss : 2.251375690320856e-06\n",
      "Steps : 419400, \t Total Gen Loss : 3779.09912109375, \t Total Dis Loss : 6.469458185165422e-06\n",
      "Steps : 419500, \t Total Gen Loss : 3833.526611328125, \t Total Dis Loss : 1.0772537279990502e-05\n",
      "Steps : 419600, \t Total Gen Loss : 3408.246337890625, \t Total Dis Loss : 2.813701712511829e-06\n",
      "Steps : 419700, \t Total Gen Loss : 3563.125244140625, \t Total Dis Loss : 3.352530029587797e-06\n",
      "Steps : 419800, \t Total Gen Loss : 3445.852294921875, \t Total Dis Loss : 3.5553744055505376e-06\n",
      "Steps : 419900, \t Total Gen Loss : 3542.60888671875, \t Total Dis Loss : 1.0511589607631322e-05\n",
      "Steps : 420000, \t Total Gen Loss : 3786.025146484375, \t Total Dis Loss : 2.842733238139772e-06\n",
      "Steps : 420100, \t Total Gen Loss : 3864.168212890625, \t Total Dis Loss : 5.0139356062572915e-06\n",
      "Steps : 420200, \t Total Gen Loss : 3680.39208984375, \t Total Dis Loss : 2.2018195977580035e-06\n",
      "Steps : 420300, \t Total Gen Loss : 3592.8408203125, \t Total Dis Loss : 7.117973837011959e-07\n",
      "Steps : 420400, \t Total Gen Loss : 3068.50927734375, \t Total Dis Loss : 4.1655417589936405e-05\n",
      "Steps : 420500, \t Total Gen Loss : 3975.437744140625, \t Total Dis Loss : 3.94408232295973e-07\n",
      "Steps : 420600, \t Total Gen Loss : 3664.545654296875, \t Total Dis Loss : 3.996396117145196e-06\n",
      "Steps : 420700, \t Total Gen Loss : 3898.057861328125, \t Total Dis Loss : 2.9861275834264234e-06\n",
      "Steps : 420800, \t Total Gen Loss : 3205.554443359375, \t Total Dis Loss : 1.3225828297436237e-05\n",
      "Steps : 420900, \t Total Gen Loss : 3568.057373046875, \t Total Dis Loss : 3.633266896940768e-05\n",
      "Steps : 421000, \t Total Gen Loss : 3152.811767578125, \t Total Dis Loss : 2.7450073503132444e-06\n",
      "Steps : 421100, \t Total Gen Loss : 3128.0263671875, \t Total Dis Loss : 1.048316880769562e-05\n",
      "Steps : 421200, \t Total Gen Loss : 3520.899169921875, \t Total Dis Loss : 4.430055014381651e-06\n",
      "Steps : 421300, \t Total Gen Loss : 3645.707275390625, \t Total Dis Loss : 1.5089451608218951e-06\n",
      "Steps : 421400, \t Total Gen Loss : 3530.196533203125, \t Total Dis Loss : 4.081530732946703e-06\n",
      "Steps : 421500, \t Total Gen Loss : 3325.977294921875, \t Total Dis Loss : 6.050076081010047e-06\n",
      "Steps : 421600, \t Total Gen Loss : 3358.717041015625, \t Total Dis Loss : 2.7336816401657416e-06\n",
      "Steps : 421700, \t Total Gen Loss : 3625.012451171875, \t Total Dis Loss : 1.2704330174528877e-06\n",
      "Steps : 421800, \t Total Gen Loss : 3850.47900390625, \t Total Dis Loss : 9.237575682163879e-07\n",
      "Time for epoch 75 is 76.94007563591003 sec\n",
      "Steps : 421900, \t Total Gen Loss : 3153.122314453125, \t Total Dis Loss : 1.7579052382643567e-06\n",
      "Steps : 422000, \t Total Gen Loss : 3540.011962890625, \t Total Dis Loss : 1.4803672456764616e-05\n",
      "Steps : 422100, \t Total Gen Loss : 4174.55419921875, \t Total Dis Loss : 6.458959887822857e-06\n",
      "Steps : 422200, \t Total Gen Loss : 3577.8984375, \t Total Dis Loss : 1.585262339176552e-06\n",
      "Steps : 422300, \t Total Gen Loss : 4131.78857421875, \t Total Dis Loss : 1.549935041111894e-05\n",
      "Steps : 422400, \t Total Gen Loss : 3651.68603515625, \t Total Dis Loss : 6.52047401672462e-06\n",
      "Steps : 422500, \t Total Gen Loss : 3088.80712890625, \t Total Dis Loss : 7.27350779925473e-05\n",
      "Steps : 422600, \t Total Gen Loss : 3183.60986328125, \t Total Dis Loss : 5.406301170296501e-06\n",
      "Steps : 422700, \t Total Gen Loss : 3144.462890625, \t Total Dis Loss : 2.322349018868408e-06\n",
      "Steps : 422800, \t Total Gen Loss : 3285.837158203125, \t Total Dis Loss : 4.0603531488159206e-06\n",
      "Steps : 422900, \t Total Gen Loss : 3758.294189453125, \t Total Dis Loss : 8.834740583552048e-06\n",
      "Steps : 423000, \t Total Gen Loss : 4254.6865234375, \t Total Dis Loss : 2.2157482817419805e-05\n",
      "Steps : 423100, \t Total Gen Loss : 3887.17138671875, \t Total Dis Loss : 8.197880561056081e-07\n",
      "Steps : 423200, \t Total Gen Loss : 3472.318115234375, \t Total Dis Loss : 0.0006304295966401696\n",
      "Steps : 423300, \t Total Gen Loss : 4216.30078125, \t Total Dis Loss : 3.455662590567954e-05\n",
      "Steps : 423400, \t Total Gen Loss : 3709.2587890625, \t Total Dis Loss : 0.00010658844257704914\n",
      "Steps : 423500, \t Total Gen Loss : 3107.30615234375, \t Total Dis Loss : 6.838576518930495e-05\n",
      "Steps : 423600, \t Total Gen Loss : 3238.33837890625, \t Total Dis Loss : 5.083464566268958e-05\n",
      "Steps : 423700, \t Total Gen Loss : 3094.7294921875, \t Total Dis Loss : 4.9239479267271236e-06\n",
      "Steps : 423800, \t Total Gen Loss : 4101.0458984375, \t Total Dis Loss : 3.657603156170808e-05\n",
      "Steps : 423900, \t Total Gen Loss : 3261.159423828125, \t Total Dis Loss : 1.6513154150743503e-06\n",
      "Steps : 424000, \t Total Gen Loss : 3724.7197265625, \t Total Dis Loss : 3.237642522435635e-05\n",
      "Steps : 424100, \t Total Gen Loss : 3499.125732421875, \t Total Dis Loss : 8.668078407936264e-06\n",
      "Steps : 424200, \t Total Gen Loss : 3097.5341796875, \t Total Dis Loss : 1.6984698731903336e-06\n",
      "Steps : 424300, \t Total Gen Loss : 3867.966064453125, \t Total Dis Loss : 3.5698915326065617e-06\n",
      "Steps : 424400, \t Total Gen Loss : 2887.285400390625, \t Total Dis Loss : 8.408479516219813e-06\n",
      "Steps : 424500, \t Total Gen Loss : 3506.9765625, \t Total Dis Loss : 7.482280580006773e-06\n",
      "Steps : 424600, \t Total Gen Loss : 3918.326416015625, \t Total Dis Loss : 4.185670604783809e-06\n",
      "Steps : 424700, \t Total Gen Loss : 3385.387939453125, \t Total Dis Loss : 4.880812412011437e-06\n",
      "Steps : 424800, \t Total Gen Loss : 3530.80712890625, \t Total Dis Loss : 2.191821749875089e-06\n",
      "Steps : 424900, \t Total Gen Loss : 3657.266357421875, \t Total Dis Loss : 3.2668122003087774e-05\n",
      "Steps : 425000, \t Total Gen Loss : 4427.92724609375, \t Total Dis Loss : 8.23659502202645e-06\n",
      "Steps : 425100, \t Total Gen Loss : 3569.41552734375, \t Total Dis Loss : 2.1776804715045728e-05\n",
      "Steps : 425200, \t Total Gen Loss : 3249.857666015625, \t Total Dis Loss : 2.376908014412038e-06\n",
      "Steps : 425300, \t Total Gen Loss : 3507.591064453125, \t Total Dis Loss : 4.553321559797041e-07\n",
      "Steps : 425400, \t Total Gen Loss : 3288.567626953125, \t Total Dis Loss : 3.349407961650286e-06\n",
      "Steps : 425500, \t Total Gen Loss : 3423.445068359375, \t Total Dis Loss : 1.3704727734875632e-06\n",
      "Steps : 425600, \t Total Gen Loss : 3729.379638671875, \t Total Dis Loss : 4.8756020987639204e-05\n",
      "Steps : 425700, \t Total Gen Loss : 3668.456787109375, \t Total Dis Loss : 3.838732482108753e-06\n",
      "Steps : 425800, \t Total Gen Loss : 3608.376953125, \t Total Dis Loss : 2.346828296140302e-06\n",
      "Steps : 425900, \t Total Gen Loss : 3515.42724609375, \t Total Dis Loss : 1.8239524024465936e-06\n",
      "Steps : 426000, \t Total Gen Loss : 3895.7275390625, \t Total Dis Loss : 1.2261309620953398e-06\n",
      "Steps : 426100, \t Total Gen Loss : 4413.6259765625, \t Total Dis Loss : 8.81364258020767e-07\n",
      "Steps : 426200, \t Total Gen Loss : 3406.309814453125, \t Total Dis Loss : 1.1558595360838808e-06\n",
      "Steps : 426300, \t Total Gen Loss : 3784.61474609375, \t Total Dis Loss : 2.4131648387992755e-05\n",
      "Steps : 426400, \t Total Gen Loss : 3522.589599609375, \t Total Dis Loss : 1.1530999017850263e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 426500, \t Total Gen Loss : 3891.61767578125, \t Total Dis Loss : 5.1978076953673735e-06\n",
      "Steps : 426600, \t Total Gen Loss : 4223.1689453125, \t Total Dis Loss : 2.0761431187565904e-06\n",
      "Steps : 426700, \t Total Gen Loss : 3529.77880859375, \t Total Dis Loss : 9.141962436842732e-06\n",
      "Steps : 426800, \t Total Gen Loss : 3364.248046875, \t Total Dis Loss : 1.1468296179373283e-06\n",
      "Steps : 426900, \t Total Gen Loss : 3390.552490234375, \t Total Dis Loss : 7.832011306163622e-07\n",
      "Steps : 427000, \t Total Gen Loss : 3780.705078125, \t Total Dis Loss : 5.616950147668831e-06\n",
      "Steps : 427100, \t Total Gen Loss : 3552.676025390625, \t Total Dis Loss : 2.9056086532364134e-06\n",
      "Steps : 427200, \t Total Gen Loss : 3636.06298828125, \t Total Dis Loss : 8.278358291136101e-05\n",
      "Steps : 427300, \t Total Gen Loss : 3131.102783203125, \t Total Dis Loss : 0.00048357879859395325\n",
      "Steps : 427400, \t Total Gen Loss : 3276.530029296875, \t Total Dis Loss : 2.5617550818424206e-06\n",
      "Steps : 427500, \t Total Gen Loss : 3264.80810546875, \t Total Dis Loss : 8.630475349491462e-05\n",
      "Time for epoch 76 is 77.51531291007996 sec\n",
      "Steps : 427600, \t Total Gen Loss : 3367.631103515625, \t Total Dis Loss : 7.091551196936052e-06\n",
      "Steps : 427700, \t Total Gen Loss : 3056.3525390625, \t Total Dis Loss : 1.4031553291715682e-05\n",
      "Steps : 427800, \t Total Gen Loss : 2733.31982421875, \t Total Dis Loss : 1.1420712326071225e-05\n",
      "Steps : 427900, \t Total Gen Loss : 3253.254150390625, \t Total Dis Loss : 1.9822182366624475e-06\n",
      "Steps : 428000, \t Total Gen Loss : 3129.237060546875, \t Total Dis Loss : 1.8818600437953137e-06\n",
      "Steps : 428100, \t Total Gen Loss : 3981.527587890625, \t Total Dis Loss : 3.851537712762365e-06\n",
      "Steps : 428200, \t Total Gen Loss : 3967.2666015625, \t Total Dis Loss : 1.4094289326749276e-05\n",
      "Steps : 428300, \t Total Gen Loss : 3809.836669921875, \t Total Dis Loss : 2.852551006071735e-06\n",
      "Steps : 428400, \t Total Gen Loss : 4330.8828125, \t Total Dis Loss : 6.8607719185820315e-06\n",
      "Steps : 428500, \t Total Gen Loss : 3680.2353515625, \t Total Dis Loss : 9.761729415913578e-06\n",
      "Steps : 428600, \t Total Gen Loss : 3496.334228515625, \t Total Dis Loss : 1.8381417703494662e-06\n",
      "Steps : 428700, \t Total Gen Loss : 3401.75830078125, \t Total Dis Loss : 2.1887105958739994e-06\n",
      "Steps : 428800, \t Total Gen Loss : 3132.1845703125, \t Total Dis Loss : 2.7972159841738176e-06\n",
      "Steps : 428900, \t Total Gen Loss : 3812.44140625, \t Total Dis Loss : 4.975234332960099e-05\n",
      "Steps : 429000, \t Total Gen Loss : 4107.53271484375, \t Total Dis Loss : 2.7857167879119515e-06\n",
      "Steps : 429100, \t Total Gen Loss : 3919.04931640625, \t Total Dis Loss : 1.9588311261031777e-05\n",
      "Steps : 429200, \t Total Gen Loss : 3662.959228515625, \t Total Dis Loss : 0.0005762680666521192\n",
      "Steps : 429300, \t Total Gen Loss : 3656.65673828125, \t Total Dis Loss : 3.4092358873749617e-06\n",
      "Steps : 429400, \t Total Gen Loss : 3429.183837890625, \t Total Dis Loss : 2.9345653729251353e-06\n",
      "Steps : 429500, \t Total Gen Loss : 3827.225341796875, \t Total Dis Loss : 2.224796662630979e-05\n",
      "Steps : 429600, \t Total Gen Loss : 4020.57275390625, \t Total Dis Loss : 2.7467263862490654e-05\n",
      "Steps : 429700, \t Total Gen Loss : 3350.710693359375, \t Total Dis Loss : 3.1693498385720886e-06\n",
      "Steps : 429800, \t Total Gen Loss : 3702.11572265625, \t Total Dis Loss : 1.086343308998039e-05\n",
      "Steps : 429900, \t Total Gen Loss : 3389.09423828125, \t Total Dis Loss : 6.1925607042212505e-06\n",
      "Steps : 430000, \t Total Gen Loss : 3636.17138671875, \t Total Dis Loss : 4.774280455421831e-07\n",
      "Steps : 430100, \t Total Gen Loss : 2614.7353515625, \t Total Dis Loss : 6.558387326549564e-07\n",
      "Steps : 430200, \t Total Gen Loss : 3840.08544921875, \t Total Dis Loss : 3.6457652186072664e-06\n",
      "Steps : 430300, \t Total Gen Loss : 3540.2666015625, \t Total Dis Loss : 1.9395241906750016e-05\n",
      "Steps : 430400, \t Total Gen Loss : 3754.605224609375, \t Total Dis Loss : 5.289144610287622e-06\n",
      "Steps : 430500, \t Total Gen Loss : 3638.880615234375, \t Total Dis Loss : 3.62833307008259e-05\n",
      "Steps : 430600, \t Total Gen Loss : 3600.203369140625, \t Total Dis Loss : 0.00013853583368472755\n",
      "Steps : 430700, \t Total Gen Loss : 3650.5009765625, \t Total Dis Loss : 1.7270534954150207e-05\n",
      "Steps : 430800, \t Total Gen Loss : 3566.77783203125, \t Total Dis Loss : 2.946504991996335e-06\n",
      "Steps : 430900, \t Total Gen Loss : 3269.619140625, \t Total Dis Loss : 7.174399797804654e-06\n",
      "Steps : 431000, \t Total Gen Loss : 3872.17333984375, \t Total Dis Loss : 7.769027433823794e-06\n",
      "Steps : 431100, \t Total Gen Loss : 3667.233642578125, \t Total Dis Loss : 8.137070608427166e-07\n",
      "Steps : 431200, \t Total Gen Loss : 3789.273193359375, \t Total Dis Loss : 0.020173652097582817\n",
      "Steps : 431300, \t Total Gen Loss : 3622.630615234375, \t Total Dis Loss : 3.3787543998187175e-06\n",
      "Steps : 431400, \t Total Gen Loss : 3658.226806640625, \t Total Dis Loss : 8.164020255208015e-05\n",
      "Steps : 431500, \t Total Gen Loss : 3780.149169921875, \t Total Dis Loss : 2.9641689707204932e-06\n",
      "Steps : 431600, \t Total Gen Loss : 3459.6884765625, \t Total Dis Loss : 1.886314976218273e-06\n",
      "Steps : 431700, \t Total Gen Loss : 3330.751953125, \t Total Dis Loss : 6.774195935577154e-05\n",
      "Steps : 431800, \t Total Gen Loss : 3570.228759765625, \t Total Dis Loss : 1.5446439647348598e-05\n",
      "Steps : 431900, \t Total Gen Loss : 3175.76708984375, \t Total Dis Loss : 1.8216205717180856e-05\n",
      "Steps : 432000, \t Total Gen Loss : 3400.039794921875, \t Total Dis Loss : 2.9557308153016493e-05\n",
      "Steps : 432100, \t Total Gen Loss : 4138.60546875, \t Total Dis Loss : 7.836845270503545e-07\n",
      "Steps : 432200, \t Total Gen Loss : 4083.631103515625, \t Total Dis Loss : 2.541950834711315e-06\n",
      "Steps : 432300, \t Total Gen Loss : 3851.943115234375, \t Total Dis Loss : 3.288725565653294e-05\n",
      "Steps : 432400, \t Total Gen Loss : 3447.906982421875, \t Total Dis Loss : 0.00063511379994452\n",
      "Steps : 432500, \t Total Gen Loss : 3502.427001953125, \t Total Dis Loss : 0.00015419475676026195\n",
      "Steps : 432600, \t Total Gen Loss : 2976.88623046875, \t Total Dis Loss : 5.680541562469443e-06\n",
      "Steps : 432700, \t Total Gen Loss : 3643.67333984375, \t Total Dis Loss : 3.0581395549234e-05\n",
      "Steps : 432800, \t Total Gen Loss : 3542.513671875, \t Total Dis Loss : 1.0010598998633213e-05\n",
      "Steps : 432900, \t Total Gen Loss : 3434.319580078125, \t Total Dis Loss : 9.362158743897453e-05\n",
      "Steps : 433000, \t Total Gen Loss : 3549.19287109375, \t Total Dis Loss : 7.157750223996118e-05\n",
      "Steps : 433100, \t Total Gen Loss : 3390.699462890625, \t Total Dis Loss : 2.123618514815462e-06\n",
      "Time for epoch 77 is 75.21371936798096 sec\n",
      "Steps : 433200, \t Total Gen Loss : 3271.601806640625, \t Total Dis Loss : 5.0245298552908935e-06\n",
      "Steps : 433300, \t Total Gen Loss : 3662.408203125, \t Total Dis Loss : 0.0005796495825052261\n",
      "Steps : 433400, \t Total Gen Loss : 3705.49755859375, \t Total Dis Loss : 5.1096067181788385e-05\n",
      "Steps : 433500, \t Total Gen Loss : 4261.087890625, \t Total Dis Loss : 3.8032576412661e-06\n",
      "Steps : 433600, \t Total Gen Loss : 3371.69677734375, \t Total Dis Loss : 7.816409379302058e-06\n",
      "Steps : 433700, \t Total Gen Loss : 4023.031494140625, \t Total Dis Loss : 7.167741387092974e-06\n",
      "Steps : 433800, \t Total Gen Loss : 3255.436279296875, \t Total Dis Loss : 2.7560192847886356e-06\n",
      "Steps : 433900, \t Total Gen Loss : 3037.759033203125, \t Total Dis Loss : 2.3715865609119646e-05\n",
      "Steps : 434000, \t Total Gen Loss : 3490.62158203125, \t Total Dis Loss : 6.670990842394531e-06\n",
      "Steps : 434100, \t Total Gen Loss : 3498.842529296875, \t Total Dis Loss : 1.8579070456326008e-05\n",
      "Steps : 434200, \t Total Gen Loss : 3672.673583984375, \t Total Dis Loss : 1.0848962119780481e-05\n",
      "Steps : 434300, \t Total Gen Loss : 4456.0654296875, \t Total Dis Loss : 6.282445610850118e-06\n",
      "Steps : 434400, \t Total Gen Loss : 3627.772705078125, \t Total Dis Loss : 6.0037859839212615e-06\n",
      "Steps : 434500, \t Total Gen Loss : 4056.921142578125, \t Total Dis Loss : 1.8949345758301206e-05\n",
      "Steps : 434600, \t Total Gen Loss : 3671.085205078125, \t Total Dis Loss : 1.3025293810642324e-05\n",
      "Steps : 434700, \t Total Gen Loss : 3711.47998046875, \t Total Dis Loss : 1.180392337118974e-05\n",
      "Steps : 434800, \t Total Gen Loss : 3636.30029296875, \t Total Dis Loss : 1.2253410204721149e-05\n",
      "Steps : 434900, \t Total Gen Loss : 3464.660888671875, \t Total Dis Loss : 3.1596187909599394e-05\n",
      "Steps : 435000, \t Total Gen Loss : 3174.412353515625, \t Total Dis Loss : 4.027097929792944e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 435100, \t Total Gen Loss : 3658.46630859375, \t Total Dis Loss : 3.538381861289963e-06\n",
      "Steps : 435200, \t Total Gen Loss : 3904.189208984375, \t Total Dis Loss : 2.3673403575230623e-06\n",
      "Steps : 435300, \t Total Gen Loss : 3703.188232421875, \t Total Dis Loss : 3.903817855643865e-07\n",
      "Steps : 435400, \t Total Gen Loss : 2856.394775390625, \t Total Dis Loss : 4.306385108066024e-06\n",
      "Steps : 435500, \t Total Gen Loss : 3959.970458984375, \t Total Dis Loss : 1.3935159586253576e-06\n",
      "Steps : 435600, \t Total Gen Loss : 3791.777099609375, \t Total Dis Loss : 5.500364181898476e-07\n",
      "Steps : 435700, \t Total Gen Loss : 4322.6953125, \t Total Dis Loss : 2.589489668025635e-06\n",
      "Steps : 435800, \t Total Gen Loss : 3487.4912109375, \t Total Dis Loss : 8.552823373975116e-07\n",
      "Steps : 435900, \t Total Gen Loss : 3447.073486328125, \t Total Dis Loss : 2.1827992895850912e-05\n",
      "Steps : 436000, \t Total Gen Loss : 3387.785888671875, \t Total Dis Loss : 0.00021259894128888845\n",
      "Steps : 436100, \t Total Gen Loss : 3048.5537109375, \t Total Dis Loss : 1.5276291378540918e-06\n",
      "Steps : 436200, \t Total Gen Loss : 3638.06298828125, \t Total Dis Loss : 2.4296673473145347e-06\n",
      "Steps : 436300, \t Total Gen Loss : 3345.423828125, \t Total Dis Loss : 1.948939825524576e-06\n",
      "Steps : 436400, \t Total Gen Loss : 2753.783203125, \t Total Dis Loss : 1.916924702527467e-06\n",
      "Steps : 436500, \t Total Gen Loss : 4024.55029296875, \t Total Dis Loss : 5.272232215247641e-07\n",
      "Steps : 436600, \t Total Gen Loss : 3944.806884765625, \t Total Dis Loss : 3.37069025135861e-07\n",
      "Steps : 436700, \t Total Gen Loss : 3408.645263671875, \t Total Dis Loss : 1.7280967767874245e-06\n",
      "Steps : 436800, \t Total Gen Loss : 3849.76806640625, \t Total Dis Loss : 5.69955886930984e-07\n",
      "Steps : 436900, \t Total Gen Loss : 3742.969482421875, \t Total Dis Loss : 4.209210828776122e-07\n",
      "Steps : 437000, \t Total Gen Loss : 3587.315185546875, \t Total Dis Loss : 1.3197508224038756e-06\n",
      "Steps : 437100, \t Total Gen Loss : 4227.79248046875, \t Total Dis Loss : 8.573431841796264e-06\n",
      "Steps : 437200, \t Total Gen Loss : 3668.810546875, \t Total Dis Loss : 1.1409498256398365e-05\n",
      "Steps : 437300, \t Total Gen Loss : 3445.651123046875, \t Total Dis Loss : 1.0479888260306325e-05\n",
      "Steps : 437400, \t Total Gen Loss : 3457.491943359375, \t Total Dis Loss : 6.4043742895592e-06\n",
      "Steps : 437500, \t Total Gen Loss : 3876.333251953125, \t Total Dis Loss : 7.101036317180842e-06\n",
      "Steps : 437600, \t Total Gen Loss : 3891.5625, \t Total Dis Loss : 3.078916961385403e-06\n",
      "Steps : 437700, \t Total Gen Loss : 3426.475341796875, \t Total Dis Loss : 4.739968517242232e-06\n",
      "Steps : 437800, \t Total Gen Loss : 3970.162353515625, \t Total Dis Loss : 2.615363700897433e-05\n",
      "Steps : 437900, \t Total Gen Loss : 3151.854736328125, \t Total Dis Loss : 5.084345957584446e-06\n",
      "Steps : 438000, \t Total Gen Loss : 3697.140625, \t Total Dis Loss : 1.586311191204004e-05\n",
      "Steps : 438100, \t Total Gen Loss : 3827.05126953125, \t Total Dis Loss : 8.336832979694009e-05\n",
      "Steps : 438200, \t Total Gen Loss : 3852.880859375, \t Total Dis Loss : 6.016674433340086e-06\n",
      "Steps : 438300, \t Total Gen Loss : 3609.69140625, \t Total Dis Loss : 5.676545697497204e-06\n",
      "Steps : 438400, \t Total Gen Loss : 3408.845458984375, \t Total Dis Loss : 3.6166529753245413e-06\n",
      "Steps : 438500, \t Total Gen Loss : 3523.221435546875, \t Total Dis Loss : 5.139580707691493e-07\n",
      "Steps : 438600, \t Total Gen Loss : 3031.65234375, \t Total Dis Loss : 6.12161272783851e-07\n",
      "Steps : 438700, \t Total Gen Loss : 3628.50927734375, \t Total Dis Loss : 6.522208764181414e-07\n",
      "Time for epoch 78 is 75.50586032867432 sec\n",
      "Steps : 438800, \t Total Gen Loss : 3768.236083984375, \t Total Dis Loss : 1.8758570377030992e-06\n",
      "Steps : 438900, \t Total Gen Loss : 3092.425048828125, \t Total Dis Loss : 1.872930624813307e-06\n",
      "Steps : 439000, \t Total Gen Loss : 3334.4921875, \t Total Dis Loss : 3.5350967664271593e-07\n",
      "Steps : 439100, \t Total Gen Loss : 3682.33984375, \t Total Dis Loss : 1.4166422488415265e-06\n",
      "Steps : 439200, \t Total Gen Loss : 3756.01953125, \t Total Dis Loss : 3.232435119571164e-05\n",
      "Steps : 439300, \t Total Gen Loss : 4003.9521484375, \t Total Dis Loss : 8.422750852332683e-07\n",
      "Steps : 439400, \t Total Gen Loss : 4055.426513671875, \t Total Dis Loss : 6.095126536820317e-07\n",
      "Steps : 439500, \t Total Gen Loss : 3403.3203125, \t Total Dis Loss : 8.653192935526022e-07\n",
      "Steps : 439600, \t Total Gen Loss : 3503.206298828125, \t Total Dis Loss : 3.126556975985295e-06\n",
      "Steps : 439700, \t Total Gen Loss : 3467.703857421875, \t Total Dis Loss : 7.338356340369501e-07\n",
      "Steps : 439800, \t Total Gen Loss : 3487.69775390625, \t Total Dis Loss : 9.194282029056922e-07\n",
      "Steps : 439900, \t Total Gen Loss : 3394.22119140625, \t Total Dis Loss : 8.702407967575709e-07\n",
      "Steps : 440000, \t Total Gen Loss : 3130.5869140625, \t Total Dis Loss : 1.147717966887285e-06\n",
      "Steps : 440100, \t Total Gen Loss : 3841.172119140625, \t Total Dis Loss : 1.3166060170988203e-06\n",
      "Steps : 440200, \t Total Gen Loss : 4026.0615234375, \t Total Dis Loss : 1.6191609120141948e-06\n",
      "Steps : 440300, \t Total Gen Loss : 3947.19091796875, \t Total Dis Loss : 2.557762627475313e-06\n",
      "Steps : 440400, \t Total Gen Loss : 4084.93994140625, \t Total Dis Loss : 2.385931566095678e-06\n",
      "Steps : 440500, \t Total Gen Loss : 3690.31982421875, \t Total Dis Loss : 5.28576310898643e-06\n",
      "Steps : 440600, \t Total Gen Loss : 3799.185791015625, \t Total Dis Loss : 1.3438998394121882e-06\n",
      "Steps : 440700, \t Total Gen Loss : 3447.5751953125, \t Total Dis Loss : 6.393330295395572e-06\n",
      "Steps : 440800, \t Total Gen Loss : 3371.28515625, \t Total Dis Loss : 7.437014346578508e-07\n",
      "Steps : 440900, \t Total Gen Loss : 3069.0224609375, \t Total Dis Loss : 0.00025274144718423486\n",
      "Steps : 441000, \t Total Gen Loss : 3515.956298828125, \t Total Dis Loss : 1.3569127759183175e-06\n",
      "Steps : 441100, \t Total Gen Loss : 4787.55517578125, \t Total Dis Loss : 1.4525156075251289e-05\n",
      "Steps : 441200, \t Total Gen Loss : 3519.877197265625, \t Total Dis Loss : 2.3661307295697043e-06\n",
      "Steps : 441300, \t Total Gen Loss : 3299.82666015625, \t Total Dis Loss : 6.787300890209735e-07\n",
      "Steps : 441400, \t Total Gen Loss : 3594.84326171875, \t Total Dis Loss : 0.0006033001700416207\n",
      "Steps : 441500, \t Total Gen Loss : 3741.848876953125, \t Total Dis Loss : 1.225115374836605e-05\n",
      "Steps : 441600, \t Total Gen Loss : 3603.462890625, \t Total Dis Loss : 1.3613624787467415e-06\n",
      "Steps : 441700, \t Total Gen Loss : 3971.488037109375, \t Total Dis Loss : 1.537694060971262e-06\n",
      "Steps : 441800, \t Total Gen Loss : 3109.609619140625, \t Total Dis Loss : 2.416549932604539e-06\n",
      "Steps : 441900, \t Total Gen Loss : 3925.080810546875, \t Total Dis Loss : 1.3752114682574756e-06\n",
      "Steps : 442000, \t Total Gen Loss : 3786.9140625, \t Total Dis Loss : 6.770919753762428e-06\n",
      "Steps : 442100, \t Total Gen Loss : 3983.346923828125, \t Total Dis Loss : 1.3601027148979483e-06\n",
      "Steps : 442200, \t Total Gen Loss : 3826.852783203125, \t Total Dis Loss : 1.3463522918755189e-06\n",
      "Steps : 442300, \t Total Gen Loss : 3847.200927734375, \t Total Dis Loss : 3.286223000031896e-05\n",
      "Steps : 442400, \t Total Gen Loss : 3894.841064453125, \t Total Dis Loss : 6.094599598327477e-07\n",
      "Steps : 442500, \t Total Gen Loss : 3320.930419921875, \t Total Dis Loss : 1.7365423445880879e-06\n",
      "Steps : 442600, \t Total Gen Loss : 3898.320556640625, \t Total Dis Loss : 8.868458962751902e-07\n",
      "Steps : 442700, \t Total Gen Loss : 3058.209228515625, \t Total Dis Loss : 2.6391804567538202e-05\n",
      "Steps : 442800, \t Total Gen Loss : 4057.724853515625, \t Total Dis Loss : 9.556797522236593e-07\n",
      "Steps : 442900, \t Total Gen Loss : 3626.749267578125, \t Total Dis Loss : 6.022307843522867e-07\n",
      "Steps : 443000, \t Total Gen Loss : 3334.44091796875, \t Total Dis Loss : 3.4949323435284896e-07\n",
      "Steps : 443100, \t Total Gen Loss : 3168.951171875, \t Total Dis Loss : 1.35842606141523e-06\n",
      "Steps : 443200, \t Total Gen Loss : 3843.666015625, \t Total Dis Loss : 1.7709552366795833e-06\n",
      "Steps : 443300, \t Total Gen Loss : 3352.871826171875, \t Total Dis Loss : 9.798316114029149e-07\n",
      "Steps : 443400, \t Total Gen Loss : 3939.65234375, \t Total Dis Loss : 1.3957821920485003e-06\n",
      "Steps : 443500, \t Total Gen Loss : 3045.272216796875, \t Total Dis Loss : 1.9767417143157218e-06\n",
      "Steps : 443600, \t Total Gen Loss : 2765.4482421875, \t Total Dis Loss : 1.4597713970943005e-06\n",
      "Steps : 443700, \t Total Gen Loss : 3731.859619140625, \t Total Dis Loss : 8.712046110304072e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 443800, \t Total Gen Loss : 4253.30224609375, \t Total Dis Loss : 0.000418212846852839\n",
      "Steps : 443900, \t Total Gen Loss : 3758.361572265625, \t Total Dis Loss : 1.434979094483424e-05\n",
      "Steps : 444000, \t Total Gen Loss : 3409.879638671875, \t Total Dis Loss : 1.8035508446700987e-06\n",
      "Steps : 444100, \t Total Gen Loss : 3369.88671875, \t Total Dis Loss : 8.467838597425725e-06\n",
      "Steps : 444200, \t Total Gen Loss : 4124.88037109375, \t Total Dis Loss : 2.883405841203057e-06\n",
      "Steps : 444300, \t Total Gen Loss : 3785.39892578125, \t Total Dis Loss : 7.316895334952278e-06\n",
      "Time for epoch 79 is 76.80820655822754 sec\n",
      "Steps : 444400, \t Total Gen Loss : 3872.8359375, \t Total Dis Loss : 4.539792826108169e-06\n",
      "Steps : 444500, \t Total Gen Loss : 3762.688720703125, \t Total Dis Loss : 3.521380131132901e-05\n",
      "Steps : 444600, \t Total Gen Loss : 3815.68603515625, \t Total Dis Loss : 4.204626748105511e-06\n",
      "Steps : 444700, \t Total Gen Loss : 4055.0771484375, \t Total Dis Loss : 1.705348040559329e-05\n",
      "Steps : 444800, \t Total Gen Loss : 3247.63427734375, \t Total Dis Loss : 7.771970558678731e-05\n",
      "Steps : 444900, \t Total Gen Loss : 4252.208984375, \t Total Dis Loss : 3.501603487165994e-06\n",
      "Steps : 445000, \t Total Gen Loss : 3573.01416015625, \t Total Dis Loss : 4.858646661887178e-06\n",
      "Steps : 445100, \t Total Gen Loss : 3148.965087890625, \t Total Dis Loss : 8.239523594966158e-06\n",
      "Steps : 445200, \t Total Gen Loss : 3763.812744140625, \t Total Dis Loss : 1.88030890058144e-06\n",
      "Steps : 445300, \t Total Gen Loss : 3085.01318359375, \t Total Dis Loss : 3.509852376737399e-06\n",
      "Steps : 445400, \t Total Gen Loss : 3501.768310546875, \t Total Dis Loss : 4.0057411752059124e-06\n",
      "Steps : 445500, \t Total Gen Loss : 3351.288818359375, \t Total Dis Loss : 1.392327021676465e-06\n",
      "Steps : 445600, \t Total Gen Loss : 3364.56298828125, \t Total Dis Loss : 1.3075273272988852e-05\n",
      "Steps : 445700, \t Total Gen Loss : 4113.09765625, \t Total Dis Loss : 1.9052570223720977e-06\n",
      "Steps : 445800, \t Total Gen Loss : 3512.83447265625, \t Total Dis Loss : 1.0954930985462852e-05\n",
      "Steps : 445900, \t Total Gen Loss : 4105.55859375, \t Total Dis Loss : 2.7711976144928485e-06\n",
      "Steps : 446000, \t Total Gen Loss : 3484.83935546875, \t Total Dis Loss : 7.687156539759599e-06\n",
      "Steps : 446100, \t Total Gen Loss : 3223.572265625, \t Total Dis Loss : 1.5545310816378333e-06\n",
      "Steps : 446200, \t Total Gen Loss : 3625.64794921875, \t Total Dis Loss : 0.000598985468968749\n",
      "Steps : 446300, \t Total Gen Loss : 3492.038818359375, \t Total Dis Loss : 5.986565156490542e-05\n",
      "Steps : 446400, \t Total Gen Loss : 3606.13134765625, \t Total Dis Loss : 2.8169280994916335e-05\n",
      "Steps : 446500, \t Total Gen Loss : 3290.4951171875, \t Total Dis Loss : 5.556515679927543e-05\n",
      "Steps : 446600, \t Total Gen Loss : 4006.80029296875, \t Total Dis Loss : 1.6960893844952807e-05\n",
      "Steps : 446700, \t Total Gen Loss : 3784.430419921875, \t Total Dis Loss : 5.731217243010178e-06\n",
      "Steps : 446800, \t Total Gen Loss : 3292.793701171875, \t Total Dis Loss : 2.7176074581802823e-06\n",
      "Steps : 446900, \t Total Gen Loss : 4182.23779296875, \t Total Dis Loss : 2.3179268282547127e-06\n",
      "Steps : 447000, \t Total Gen Loss : 3764.6357421875, \t Total Dis Loss : 2.990803295688238e-05\n",
      "Steps : 447100, \t Total Gen Loss : 3740.98388671875, \t Total Dis Loss : 7.637931412318721e-05\n",
      "Steps : 447200, \t Total Gen Loss : 3455.86474609375, \t Total Dis Loss : 7.206720624708396e-07\n",
      "Steps : 447300, \t Total Gen Loss : 4032.768310546875, \t Total Dis Loss : 5.210499125496426e-07\n",
      "Steps : 447400, \t Total Gen Loss : 3604.748779296875, \t Total Dis Loss : 1.3489182038028957e-06\n",
      "Steps : 447500, \t Total Gen Loss : 3248.03125, \t Total Dis Loss : 2.4270357243949547e-05\n",
      "Steps : 447600, \t Total Gen Loss : 3695.105712890625, \t Total Dis Loss : 0.0016514998860657215\n",
      "Steps : 447700, \t Total Gen Loss : 3421.917236328125, \t Total Dis Loss : 2.095850504701957e-05\n",
      "Steps : 447800, \t Total Gen Loss : 4028.05029296875, \t Total Dis Loss : 9.029777174873743e-06\n",
      "Steps : 447900, \t Total Gen Loss : 4363.4541015625, \t Total Dis Loss : 7.70002316130558e-06\n",
      "Steps : 448000, \t Total Gen Loss : 4030.73486328125, \t Total Dis Loss : 5.983127721265191e-06\n",
      "Steps : 448100, \t Total Gen Loss : 3756.65087890625, \t Total Dis Loss : 2.5592532892915187e-06\n",
      "Steps : 448200, \t Total Gen Loss : 3560.579833984375, \t Total Dis Loss : 2.0802353901672177e-06\n",
      "Steps : 448300, \t Total Gen Loss : 3640.542724609375, \t Total Dis Loss : 1.556888378217991e-06\n",
      "Steps : 448400, \t Total Gen Loss : 4257.97802734375, \t Total Dis Loss : 7.81968310548109e-07\n",
      "Steps : 448500, \t Total Gen Loss : 3236.782470703125, \t Total Dis Loss : 4.526455995801371e-06\n",
      "Steps : 448600, \t Total Gen Loss : 3053.321044921875, \t Total Dis Loss : 4.122297013964271e-06\n",
      "Steps : 448700, \t Total Gen Loss : 3795.0546875, \t Total Dis Loss : 6.059834163352207e-07\n",
      "Steps : 448800, \t Total Gen Loss : 4017.797607421875, \t Total Dis Loss : 6.631444762206229e-07\n",
      "Steps : 448900, \t Total Gen Loss : 3462.671875, \t Total Dis Loss : 1.1264884960837662e-05\n",
      "Steps : 449000, \t Total Gen Loss : 3851.783935546875, \t Total Dis Loss : 6.851453804301855e-07\n",
      "Steps : 449100, \t Total Gen Loss : 3760.982421875, \t Total Dis Loss : 2.0144339032412972e-06\n",
      "Steps : 449200, \t Total Gen Loss : 3460.591064453125, \t Total Dis Loss : 9.158607099379878e-07\n",
      "Steps : 449300, \t Total Gen Loss : 3122.607421875, \t Total Dis Loss : 0.0001243670703843236\n",
      "Steps : 449400, \t Total Gen Loss : 3524.04345703125, \t Total Dis Loss : 3.5132143239025027e-06\n",
      "Steps : 449500, \t Total Gen Loss : 4293.611328125, \t Total Dis Loss : 9.587000704414095e-07\n",
      "Steps : 449600, \t Total Gen Loss : 3483.81689453125, \t Total Dis Loss : 6.053614015399944e-06\n",
      "Steps : 449700, \t Total Gen Loss : 3774.841552734375, \t Total Dis Loss : 2.4223550099122804e-06\n",
      "Steps : 449800, \t Total Gen Loss : 3323.966796875, \t Total Dis Loss : 0.0001727408089209348\n",
      "Steps : 449900, \t Total Gen Loss : 4073.7412109375, \t Total Dis Loss : 2.2258404897002038e-06\n",
      "Steps : 450000, \t Total Gen Loss : 3396.769775390625, \t Total Dis Loss : 1.8629507394507527e-06\n",
      "Time for epoch 80 is 76.69383478164673 sec\n",
      "Steps : 450100, \t Total Gen Loss : 3877.7109375, \t Total Dis Loss : 1.3832420791004552e-06\n",
      "Steps : 450200, \t Total Gen Loss : 3793.38916015625, \t Total Dis Loss : 6.929593041604676e-07\n",
      "Steps : 450300, \t Total Gen Loss : 3778.80322265625, \t Total Dis Loss : 4.884283953288104e-06\n",
      "Steps : 450400, \t Total Gen Loss : 3277.43115234375, \t Total Dis Loss : 8.501005140715279e-06\n",
      "Steps : 450500, \t Total Gen Loss : 3998.547607421875, \t Total Dis Loss : 1.5135785815800773e-06\n",
      "Steps : 450600, \t Total Gen Loss : 3494.6845703125, \t Total Dis Loss : 7.262599410751136e-06\n",
      "Steps : 450700, \t Total Gen Loss : 3420.285400390625, \t Total Dis Loss : 1.1157071639900096e-06\n",
      "Steps : 450800, \t Total Gen Loss : 3132.83251953125, \t Total Dis Loss : 1.1264459317317232e-05\n",
      "Steps : 450900, \t Total Gen Loss : 3350.9130859375, \t Total Dis Loss : 3.28568639815785e-05\n",
      "Steps : 451000, \t Total Gen Loss : 3496.74658203125, \t Total Dis Loss : 1.5361490568466252e-06\n",
      "Steps : 451100, \t Total Gen Loss : 3506.162109375, \t Total Dis Loss : 2.475770997989457e-06\n",
      "Steps : 451200, \t Total Gen Loss : 3572.18310546875, \t Total Dis Loss : 1.4705909734402667e-06\n",
      "Steps : 451300, \t Total Gen Loss : 3811.0546875, \t Total Dis Loss : 7.580938472528942e-07\n",
      "Steps : 451400, \t Total Gen Loss : 3080.58251953125, \t Total Dis Loss : 1.2738697478198446e-05\n",
      "Steps : 451500, \t Total Gen Loss : 3272.65576171875, \t Total Dis Loss : 9.608248774384265e-07\n",
      "Steps : 451600, \t Total Gen Loss : 3491.095458984375, \t Total Dis Loss : 1.1567885849217419e-05\n",
      "Steps : 451700, \t Total Gen Loss : 3202.8857421875, \t Total Dis Loss : 1.6255769423878519e-06\n",
      "Steps : 451800, \t Total Gen Loss : 3667.41455078125, \t Total Dis Loss : 6.96322956628137e-07\n",
      "Steps : 451900, \t Total Gen Loss : 3420.453857421875, \t Total Dis Loss : 1.3561369769377052e-06\n",
      "Steps : 452000, \t Total Gen Loss : 2961.835205078125, \t Total Dis Loss : 6.355567165883258e-06\n",
      "Steps : 452100, \t Total Gen Loss : 3115.030029296875, \t Total Dis Loss : 5.89985347687616e-07\n",
      "Steps : 452200, \t Total Gen Loss : 3593.473876953125, \t Total Dis Loss : 3.5853756799042458e-06\n",
      "Steps : 452300, \t Total Gen Loss : 3759.56787109375, \t Total Dis Loss : 1.7804997014536639e-06\n",
      "Steps : 452400, \t Total Gen Loss : 3504.92578125, \t Total Dis Loss : 4.4234475353732705e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 452500, \t Total Gen Loss : 3300.41357421875, \t Total Dis Loss : 3.0898054319550283e-06\n",
      "Steps : 452600, \t Total Gen Loss : 3783.546630859375, \t Total Dis Loss : 3.9640653994865716e-06\n",
      "Steps : 452700, \t Total Gen Loss : 3043.4423828125, \t Total Dis Loss : 9.764708011061884e-07\n",
      "Steps : 452800, \t Total Gen Loss : 3680.220947265625, \t Total Dis Loss : 0.015012437477707863\n",
      "Steps : 452900, \t Total Gen Loss : 3314.413818359375, \t Total Dis Loss : 2.6579837140161544e-06\n",
      "Steps : 453000, \t Total Gen Loss : 3406.2392578125, \t Total Dis Loss : 2.50417679126258e-06\n",
      "Steps : 453100, \t Total Gen Loss : 3250.828857421875, \t Total Dis Loss : 8.856919180288969e-07\n",
      "Steps : 453200, \t Total Gen Loss : 3590.416748046875, \t Total Dis Loss : 3.2942043048933556e-07\n",
      "Steps : 453300, \t Total Gen Loss : 3374.67041015625, \t Total Dis Loss : 1.0875559155465453e-06\n",
      "Steps : 453400, \t Total Gen Loss : 3949.11669921875, \t Total Dis Loss : 0.000780912465415895\n",
      "Steps : 453500, \t Total Gen Loss : 3418.191650390625, \t Total Dis Loss : 4.045388413942419e-05\n",
      "Steps : 453600, \t Total Gen Loss : 3742.890869140625, \t Total Dis Loss : 3.061652932956349e-06\n",
      "Steps : 453700, \t Total Gen Loss : 3484.169677734375, \t Total Dis Loss : 1.3625546557705093e-07\n",
      "Steps : 453800, \t Total Gen Loss : 3319.5673828125, \t Total Dis Loss : 2.6791244067680964e-07\n",
      "Steps : 453900, \t Total Gen Loss : 3743.019775390625, \t Total Dis Loss : 6.387497251125751e-07\n",
      "Steps : 454000, \t Total Gen Loss : 3742.1337890625, \t Total Dis Loss : 1.3981723441247595e-06\n",
      "Steps : 454100, \t Total Gen Loss : 4253.6396484375, \t Total Dis Loss : 3.921521965821739e-06\n",
      "Steps : 454200, \t Total Gen Loss : 3920.848876953125, \t Total Dis Loss : 1.671898417043849e-06\n",
      "Steps : 454300, \t Total Gen Loss : 3477.456298828125, \t Total Dis Loss : 2.657512141013285e-06\n",
      "Steps : 454400, \t Total Gen Loss : 3732.092529296875, \t Total Dis Loss : 3.1967563245416386e-06\n",
      "Steps : 454500, \t Total Gen Loss : 3339.126708984375, \t Total Dis Loss : 0.00020900522940792143\n",
      "Steps : 454600, \t Total Gen Loss : 3656.678466796875, \t Total Dis Loss : 7.2573102443129756e-06\n",
      "Steps : 454700, \t Total Gen Loss : 2842.56298828125, \t Total Dis Loss : 2.978093561978312e-06\n",
      "Steps : 454800, \t Total Gen Loss : 3256.981201171875, \t Total Dis Loss : 2.1816176740685478e-06\n",
      "Steps : 454900, \t Total Gen Loss : 3951.6591796875, \t Total Dis Loss : 4.331991931394441e-06\n",
      "Steps : 455000, \t Total Gen Loss : 3760.63330078125, \t Total Dis Loss : 9.212056397700508e-07\n",
      "Steps : 455100, \t Total Gen Loss : 3596.7197265625, \t Total Dis Loss : 0.00018021915457211435\n",
      "Steps : 455200, \t Total Gen Loss : 3670.52587890625, \t Total Dis Loss : 6.704966381221311e-06\n",
      "Steps : 455300, \t Total Gen Loss : 3376.560791015625, \t Total Dis Loss : 3.731566266651498e-06\n",
      "Steps : 455400, \t Total Gen Loss : 3268.0625, \t Total Dis Loss : 1.1148857993248384e-05\n",
      "Steps : 455500, \t Total Gen Loss : 4115.810546875, \t Total Dis Loss : 4.651844210457057e-05\n",
      "Steps : 455600, \t Total Gen Loss : 3860.828125, \t Total Dis Loss : 4.1795278775680345e-06\n",
      "Time for epoch 81 is 75.37433004379272 sec\n",
      "Steps : 455700, \t Total Gen Loss : 3636.163330078125, \t Total Dis Loss : 2.9845126846339554e-05\n",
      "Steps : 455800, \t Total Gen Loss : 3967.1845703125, \t Total Dis Loss : 3.527509988998645e-06\n",
      "Steps : 455900, \t Total Gen Loss : 3621.32861328125, \t Total Dis Loss : 8.917077138903551e-07\n",
      "Steps : 456000, \t Total Gen Loss : 3581.2373046875, \t Total Dis Loss : 2.800326910801232e-06\n",
      "Steps : 456100, \t Total Gen Loss : 3936.413818359375, \t Total Dis Loss : 0.00037056676228530705\n",
      "Steps : 456200, \t Total Gen Loss : 3692.239990234375, \t Total Dis Loss : 5.640076778945513e-06\n",
      "Steps : 456300, \t Total Gen Loss : 4272.2666015625, \t Total Dis Loss : 1.275941940548364e-05\n",
      "Steps : 456400, \t Total Gen Loss : 3708.09765625, \t Total Dis Loss : 3.6221917980583385e-06\n",
      "Steps : 456500, \t Total Gen Loss : 3374.74609375, \t Total Dis Loss : 1.272519739359268e-06\n",
      "Steps : 456600, \t Total Gen Loss : 3832.4775390625, \t Total Dis Loss : 1.4905021998856682e-06\n",
      "Steps : 456700, \t Total Gen Loss : 3551.696533203125, \t Total Dis Loss : 4.1634152125880064e-07\n",
      "Steps : 456800, \t Total Gen Loss : 3487.749755859375, \t Total Dis Loss : 1.9666374555527e-06\n",
      "Steps : 456900, \t Total Gen Loss : 3823.140380859375, \t Total Dis Loss : 9.466638402955141e-06\n",
      "Steps : 457000, \t Total Gen Loss : 3637.54638671875, \t Total Dis Loss : 6.389305440279713e-07\n",
      "Steps : 457100, \t Total Gen Loss : 2992.226806640625, \t Total Dis Loss : 1.7454382259529666e-06\n",
      "Steps : 457200, \t Total Gen Loss : 3685.197021484375, \t Total Dis Loss : 2.1258533706713933e-06\n",
      "Steps : 457300, \t Total Gen Loss : 3693.00634765625, \t Total Dis Loss : 2.34690173783747e-06\n",
      "Steps : 457400, \t Total Gen Loss : 2880.884765625, \t Total Dis Loss : 0.005543383304029703\n",
      "Steps : 457500, \t Total Gen Loss : 3563.3662109375, \t Total Dis Loss : 2.4936186946433736e-06\n",
      "Steps : 457600, \t Total Gen Loss : 3639.392333984375, \t Total Dis Loss : 1.0805027159221936e-05\n",
      "Steps : 457700, \t Total Gen Loss : 3757.001220703125, \t Total Dis Loss : 5.992864316795021e-06\n",
      "Steps : 457800, \t Total Gen Loss : 3919.304931640625, \t Total Dis Loss : 3.537622319527145e-07\n",
      "Steps : 457900, \t Total Gen Loss : 3630.15771484375, \t Total Dis Loss : 8.158977834682446e-06\n",
      "Steps : 458000, \t Total Gen Loss : 3518.447509765625, \t Total Dis Loss : 3.4408108149364125e-06\n",
      "Steps : 458100, \t Total Gen Loss : 3692.1337890625, \t Total Dis Loss : 6.6688489823718555e-06\n",
      "Steps : 458200, \t Total Gen Loss : 3021.913330078125, \t Total Dis Loss : 0.000918387551791966\n",
      "Steps : 458300, \t Total Gen Loss : 3038.828369140625, \t Total Dis Loss : 1.4042938346392475e-05\n",
      "Steps : 458400, \t Total Gen Loss : 3512.843994140625, \t Total Dis Loss : 1.7409744032192975e-05\n",
      "Steps : 458500, \t Total Gen Loss : 3660.5341796875, \t Total Dis Loss : 1.64929406309966e-05\n",
      "Steps : 458600, \t Total Gen Loss : 3278.646484375, \t Total Dis Loss : 5.473380588227883e-05\n",
      "Steps : 458700, \t Total Gen Loss : 4303.32861328125, \t Total Dis Loss : 3.198181730112992e-05\n",
      "Steps : 458800, \t Total Gen Loss : 3459.638916015625, \t Total Dis Loss : 5.338517075870186e-05\n",
      "Steps : 458900, \t Total Gen Loss : 3647.69140625, \t Total Dis Loss : 7.195808393589687e-06\n",
      "Steps : 459000, \t Total Gen Loss : 3713.3349609375, \t Total Dis Loss : 1.1565962267923169e-05\n",
      "Steps : 459100, \t Total Gen Loss : 3513.44677734375, \t Total Dis Loss : 1.7774289062799653e-06\n",
      "Steps : 459200, \t Total Gen Loss : 3285.047607421875, \t Total Dis Loss : 1.291400394620723e-06\n",
      "Steps : 459300, \t Total Gen Loss : 3606.712646484375, \t Total Dis Loss : 8.951221275310672e-07\n",
      "Steps : 459400, \t Total Gen Loss : 2887.566650390625, \t Total Dis Loss : 7.677715075260494e-06\n",
      "Steps : 459500, \t Total Gen Loss : 3492.475830078125, \t Total Dis Loss : 3.671274316729978e-05\n",
      "Steps : 459600, \t Total Gen Loss : 3312.61962890625, \t Total Dis Loss : 3.431828008615412e-05\n",
      "Steps : 459700, \t Total Gen Loss : 3650.776611328125, \t Total Dis Loss : 1.354097207695304e-07\n",
      "Steps : 459800, \t Total Gen Loss : 3932.955810546875, \t Total Dis Loss : 7.088118536557886e-07\n",
      "Steps : 459900, \t Total Gen Loss : 3440.92578125, \t Total Dis Loss : 9.914764814311638e-05\n",
      "Steps : 460000, \t Total Gen Loss : 3343.482421875, \t Total Dis Loss : 9.138629684457555e-05\n",
      "Steps : 460100, \t Total Gen Loss : 3761.856689453125, \t Total Dis Loss : 5.4282962082652375e-05\n",
      "Steps : 460200, \t Total Gen Loss : 3111.31884765625, \t Total Dis Loss : 4.520793481788132e-06\n",
      "Steps : 460300, \t Total Gen Loss : 3833.500244140625, \t Total Dis Loss : 4.285642262402689e-06\n",
      "Steps : 460400, \t Total Gen Loss : 3480.00146484375, \t Total Dis Loss : 0.0015638932818546891\n",
      "Steps : 460500, \t Total Gen Loss : 4019.912841796875, \t Total Dis Loss : 0.00010368865332566202\n",
      "Steps : 460600, \t Total Gen Loss : 3392.05224609375, \t Total Dis Loss : 5.1867424190277234e-05\n",
      "Steps : 460700, \t Total Gen Loss : 3705.34033203125, \t Total Dis Loss : 0.00013218905951362103\n",
      "Steps : 460800, \t Total Gen Loss : 3196.009033203125, \t Total Dis Loss : 3.9337686757789925e-05\n",
      "Steps : 460900, \t Total Gen Loss : 3063.837158203125, \t Total Dis Loss : 7.739355169178452e-06\n",
      "Steps : 461000, \t Total Gen Loss : 3430.02001953125, \t Total Dis Loss : 5.216586032474879e-06\n",
      "Steps : 461100, \t Total Gen Loss : 3033.93115234375, \t Total Dis Loss : 1.0808496426761849e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 461200, \t Total Gen Loss : 3859.712890625, \t Total Dis Loss : 3.0193780276022153e-06\n",
      "Time for epoch 82 is 75.37408518791199 sec\n",
      "Steps : 461300, \t Total Gen Loss : 3412.50439453125, \t Total Dis Loss : 1.946210250025615e-05\n",
      "Steps : 461400, \t Total Gen Loss : 3163.47900390625, \t Total Dis Loss : 1.1014816436727415e-06\n",
      "Steps : 461500, \t Total Gen Loss : 3880.163330078125, \t Total Dis Loss : 1.6945160723480512e-06\n",
      "Steps : 461600, \t Total Gen Loss : 3616.597412109375, \t Total Dis Loss : 2.2070407794672064e-06\n",
      "Steps : 461700, \t Total Gen Loss : 3870.885986328125, \t Total Dis Loss : 2.6199861622444587e-06\n",
      "Steps : 461800, \t Total Gen Loss : 3944.900390625, \t Total Dis Loss : 4.335533958510496e-06\n",
      "Steps : 461900, \t Total Gen Loss : 3755.04541015625, \t Total Dis Loss : 6.831174914623261e-07\n",
      "Steps : 462000, \t Total Gen Loss : 3278.45361328125, \t Total Dis Loss : 1.8273807654622942e-05\n",
      "Steps : 462100, \t Total Gen Loss : 3709.531494140625, \t Total Dis Loss : 6.502038559119683e-06\n",
      "Steps : 462200, \t Total Gen Loss : 3780.448974609375, \t Total Dis Loss : 1.9818930923065636e-06\n",
      "Steps : 462300, \t Total Gen Loss : 3553.2841796875, \t Total Dis Loss : 8.078370228759013e-06\n",
      "Steps : 462400, \t Total Gen Loss : 3153.013671875, \t Total Dis Loss : 8.547347533749416e-06\n",
      "Steps : 462500, \t Total Gen Loss : 4084.797119140625, \t Total Dis Loss : 9.499440238869283e-06\n",
      "Steps : 462600, \t Total Gen Loss : 3507.50146484375, \t Total Dis Loss : 7.447571533703012e-06\n",
      "Steps : 462700, \t Total Gen Loss : 3669.145263671875, \t Total Dis Loss : 2.5249992177123204e-05\n",
      "Steps : 462800, \t Total Gen Loss : 3000.15673828125, \t Total Dis Loss : 9.4927031568659e-07\n",
      "Steps : 462900, \t Total Gen Loss : 3536.286865234375, \t Total Dis Loss : 3.469051080173813e-06\n",
      "Steps : 463000, \t Total Gen Loss : 4216.65771484375, \t Total Dis Loss : 1.150988282461185e-05\n",
      "Steps : 463100, \t Total Gen Loss : 3705.978759765625, \t Total Dis Loss : 3.8640271668555215e-05\n",
      "Steps : 463200, \t Total Gen Loss : 3103.907470703125, \t Total Dis Loss : 5.92532342125196e-05\n",
      "Steps : 463300, \t Total Gen Loss : 3789.3115234375, \t Total Dis Loss : 1.304653960687574e-05\n",
      "Steps : 463400, \t Total Gen Loss : 3456.314208984375, \t Total Dis Loss : 3.8487723941216245e-06\n",
      "Steps : 463500, \t Total Gen Loss : 3292.04248046875, \t Total Dis Loss : 0.00011855950288008898\n",
      "Steps : 463600, \t Total Gen Loss : 2850.8466796875, \t Total Dis Loss : 9.61840123636648e-06\n",
      "Steps : 463700, \t Total Gen Loss : 2901.564453125, \t Total Dis Loss : 5.079706170363352e-05\n",
      "Steps : 463800, \t Total Gen Loss : 3532.73779296875, \t Total Dis Loss : 2.6308182441425743e-06\n",
      "Steps : 463900, \t Total Gen Loss : 3499.089599609375, \t Total Dis Loss : 2.815843345160829e-06\n",
      "Steps : 464000, \t Total Gen Loss : 4045.5615234375, \t Total Dis Loss : 3.349865664858953e-07\n",
      "Steps : 464100, \t Total Gen Loss : 3537.508056640625, \t Total Dis Loss : 0.0005538736586458981\n",
      "Steps : 464200, \t Total Gen Loss : 3986.13818359375, \t Total Dis Loss : 3.605771325965179e-06\n",
      "Steps : 464300, \t Total Gen Loss : 3628.10400390625, \t Total Dis Loss : 1.1450550800873316e-06\n",
      "Steps : 464400, \t Total Gen Loss : 4084.388916015625, \t Total Dis Loss : 0.0001500848011346534\n",
      "Steps : 464500, \t Total Gen Loss : 3376.095458984375, \t Total Dis Loss : 1.6802316622488433e-06\n",
      "Steps : 464600, \t Total Gen Loss : 3418.04931640625, \t Total Dis Loss : 1.5030493614176521e-06\n",
      "Steps : 464700, \t Total Gen Loss : 3567.435791015625, \t Total Dis Loss : 2.2546209947904572e-05\n",
      "Steps : 464800, \t Total Gen Loss : 4103.130859375, \t Total Dis Loss : 1.2258886272320524e-05\n",
      "Steps : 464900, \t Total Gen Loss : 2948.741455078125, \t Total Dis Loss : 5.138972937857034e-06\n",
      "Steps : 465000, \t Total Gen Loss : 3325.109619140625, \t Total Dis Loss : 9.521496053821465e-07\n",
      "Steps : 465100, \t Total Gen Loss : 3653.05224609375, \t Total Dis Loss : 2.2704807633999735e-05\n",
      "Steps : 465200, \t Total Gen Loss : 3544.36083984375, \t Total Dis Loss : 6.102688985265559e-06\n",
      "Steps : 465300, \t Total Gen Loss : 3329.337890625, \t Total Dis Loss : 6.533190116897458e-06\n",
      "Steps : 465400, \t Total Gen Loss : 3781.96826171875, \t Total Dis Loss : 6.537031822517747e-06\n",
      "Steps : 465500, \t Total Gen Loss : 3305.40673828125, \t Total Dis Loss : 3.210039358236827e-05\n",
      "Steps : 465600, \t Total Gen Loss : 3486.517333984375, \t Total Dis Loss : 6.2233057178673334e-06\n",
      "Steps : 465700, \t Total Gen Loss : 3231.969970703125, \t Total Dis Loss : 5.116657575854333e-06\n",
      "Steps : 465800, \t Total Gen Loss : 3408.593017578125, \t Total Dis Loss : 8.637905921204947e-06\n",
      "Steps : 465900, \t Total Gen Loss : 3064.91162109375, \t Total Dis Loss : 1.0496323739062063e-05\n",
      "Steps : 466000, \t Total Gen Loss : 3825.85009765625, \t Total Dis Loss : 1.2240502655913588e-06\n",
      "Steps : 466100, \t Total Gen Loss : 3410.355224609375, \t Total Dis Loss : 1.6370174762414536e-06\n",
      "Steps : 466200, \t Total Gen Loss : 3627.252197265625, \t Total Dis Loss : 2.3352035896095913e-06\n",
      "Steps : 466300, \t Total Gen Loss : 3743.20751953125, \t Total Dis Loss : 0.0005321382777765393\n",
      "Steps : 466400, \t Total Gen Loss : 3744.927490234375, \t Total Dis Loss : 1.0438272965984652e-06\n",
      "Steps : 466500, \t Total Gen Loss : 3428.232177734375, \t Total Dis Loss : 1.5410985270136734e-06\n",
      "Steps : 466600, \t Total Gen Loss : 2927.121826171875, \t Total Dis Loss : 2.6737463940662565e-06\n",
      "Steps : 466700, \t Total Gen Loss : 3511.3330078125, \t Total Dis Loss : 2.749789700828842e-06\n",
      "Steps : 466800, \t Total Gen Loss : 3656.224365234375, \t Total Dis Loss : 5.2879795475746505e-06\n",
      "Time for epoch 83 is 75.0000433921814 sec\n",
      "Steps : 466900, \t Total Gen Loss : 3322.056396484375, \t Total Dis Loss : 3.948948233301053e-06\n",
      "Steps : 467000, \t Total Gen Loss : 3700.447265625, \t Total Dis Loss : 4.641811301553389e-06\n",
      "Steps : 467100, \t Total Gen Loss : 3619.866943359375, \t Total Dis Loss : 3.5992538869322743e-06\n",
      "Steps : 467200, \t Total Gen Loss : 3575.61767578125, \t Total Dis Loss : 8.992225957626943e-06\n",
      "Steps : 467300, \t Total Gen Loss : 3074.2783203125, \t Total Dis Loss : 6.973788231334765e-07\n",
      "Steps : 467400, \t Total Gen Loss : 3118.7353515625, \t Total Dis Loss : 7.61409182814532e-07\n",
      "Steps : 467500, \t Total Gen Loss : 3633.2509765625, \t Total Dis Loss : 6.647377404078725e-07\n",
      "Steps : 467600, \t Total Gen Loss : 3578.751708984375, \t Total Dis Loss : 1.0776859198813327e-05\n",
      "Steps : 467700, \t Total Gen Loss : 4052.9931640625, \t Total Dis Loss : 1.1934913345612586e-05\n",
      "Steps : 467800, \t Total Gen Loss : 3252.380859375, \t Total Dis Loss : 6.223858690646011e-06\n",
      "Steps : 467900, \t Total Gen Loss : 3848.468994140625, \t Total Dis Loss : 1.1183414017068571e-06\n",
      "Steps : 468000, \t Total Gen Loss : 3601.47021484375, \t Total Dis Loss : 4.68706275569275e-06\n",
      "Steps : 468100, \t Total Gen Loss : 3386.1708984375, \t Total Dis Loss : 5.9799222071887925e-06\n",
      "Steps : 468200, \t Total Gen Loss : 3841.57373046875, \t Total Dis Loss : 4.7340081437141635e-06\n",
      "Steps : 468300, \t Total Gen Loss : 3820.355224609375, \t Total Dis Loss : 2.360613871132955e-05\n",
      "Steps : 468400, \t Total Gen Loss : 3450.562744140625, \t Total Dis Loss : 0.0008767173858359456\n",
      "Steps : 468500, \t Total Gen Loss : 3183.4326171875, \t Total Dis Loss : 9.701852832222357e-05\n",
      "Steps : 468600, \t Total Gen Loss : 4046.307861328125, \t Total Dis Loss : 3.1132316507864743e-05\n",
      "Steps : 468700, \t Total Gen Loss : 3178.075927734375, \t Total Dis Loss : 8.901845285436139e-05\n",
      "Steps : 468800, \t Total Gen Loss : 4091.532958984375, \t Total Dis Loss : 7.466012903023511e-05\n",
      "Steps : 468900, \t Total Gen Loss : 3261.97314453125, \t Total Dis Loss : 2.2368101781466976e-05\n",
      "Steps : 469000, \t Total Gen Loss : 3092.633056640625, \t Total Dis Loss : 9.594979019311722e-06\n",
      "Steps : 469100, \t Total Gen Loss : 3334.67333984375, \t Total Dis Loss : 1.2044215509376954e-05\n",
      "Steps : 469200, \t Total Gen Loss : 3751.622314453125, \t Total Dis Loss : 1.0460075827722903e-05\n",
      "Steps : 469300, \t Total Gen Loss : 3577.03369140625, \t Total Dis Loss : 2.287252755195368e-05\n",
      "Steps : 469400, \t Total Gen Loss : 3051.329833984375, \t Total Dis Loss : 5.246390628599329e-06\n",
      "Steps : 469500, \t Total Gen Loss : 3102.283203125, \t Total Dis Loss : 1.948588396771811e-05\n",
      "Steps : 469600, \t Total Gen Loss : 3013.776611328125, \t Total Dis Loss : 1.4401897715288214e-05\n",
      "Steps : 469700, \t Total Gen Loss : 3790.24169921875, \t Total Dis Loss : 1.578528645040933e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 469800, \t Total Gen Loss : 4039.16796875, \t Total Dis Loss : 1.9581277229008265e-05\n",
      "Steps : 469900, \t Total Gen Loss : 3416.071533203125, \t Total Dis Loss : 5.8732541219796985e-05\n",
      "Steps : 470000, \t Total Gen Loss : 3773.864013671875, \t Total Dis Loss : 6.467990715464111e-06\n",
      "Steps : 470100, \t Total Gen Loss : 4247.07373046875, \t Total Dis Loss : 9.82490973910899e-07\n",
      "Steps : 470200, \t Total Gen Loss : 3307.4580078125, \t Total Dis Loss : 1.890586645458825e-05\n",
      "Steps : 470300, \t Total Gen Loss : 3483.349365234375, \t Total Dis Loss : 1.487060399085749e-05\n",
      "Steps : 470400, \t Total Gen Loss : 3582.55615234375, \t Total Dis Loss : 2.3322961624216987e-06\n",
      "Steps : 470500, \t Total Gen Loss : 3432.86865234375, \t Total Dis Loss : 2.200412200181745e-05\n",
      "Steps : 470600, \t Total Gen Loss : 3437.662109375, \t Total Dis Loss : 3.3534101930854376e-06\n",
      "Steps : 470700, \t Total Gen Loss : 3610.9267578125, \t Total Dis Loss : 3.1098828912945464e-05\n",
      "Steps : 470800, \t Total Gen Loss : 3873.907470703125, \t Total Dis Loss : 3.7920567592664156e-06\n",
      "Steps : 470900, \t Total Gen Loss : 3821.0224609375, \t Total Dis Loss : 0.0007959752110764384\n",
      "Steps : 471000, \t Total Gen Loss : 3574.45458984375, \t Total Dis Loss : 1.0082992503157584e-06\n",
      "Steps : 471100, \t Total Gen Loss : 3350.435302734375, \t Total Dis Loss : 1.43893350923463e-06\n",
      "Steps : 471200, \t Total Gen Loss : 3711.05712890625, \t Total Dis Loss : 2.8016902433591895e-06\n",
      "Steps : 471300, \t Total Gen Loss : 3786.619140625, \t Total Dis Loss : 6.36224240224692e-06\n",
      "Steps : 471400, \t Total Gen Loss : 3370.335693359375, \t Total Dis Loss : 1.2769160093739629e-05\n",
      "Steps : 471500, \t Total Gen Loss : 3954.071044921875, \t Total Dis Loss : 8.656767022330314e-05\n",
      "Steps : 471600, \t Total Gen Loss : 3876.12158203125, \t Total Dis Loss : 1.7124129954027012e-06\n",
      "Steps : 471700, \t Total Gen Loss : 3529.46826171875, \t Total Dis Loss : 1.0076440958073363e-05\n",
      "Steps : 471800, \t Total Gen Loss : 3534.857177734375, \t Total Dis Loss : 7.952006853884086e-06\n",
      "Steps : 471900, \t Total Gen Loss : 3879.343017578125, \t Total Dis Loss : 6.46761054667877e-06\n",
      "Steps : 472000, \t Total Gen Loss : 3491.85546875, \t Total Dis Loss : 1.8102178728440776e-05\n",
      "Steps : 472100, \t Total Gen Loss : 4199.49755859375, \t Total Dis Loss : 2.9638504202011973e-06\n",
      "Steps : 472200, \t Total Gen Loss : 3815.505615234375, \t Total Dis Loss : 2.5754525267984718e-05\n",
      "Steps : 472300, \t Total Gen Loss : 3473.080810546875, \t Total Dis Loss : 3.3771082144085085e-06\n",
      "Steps : 472400, \t Total Gen Loss : 3116.71533203125, \t Total Dis Loss : 2.4410437617916614e-05\n",
      "Steps : 472500, \t Total Gen Loss : 3706.434814453125, \t Total Dis Loss : 0.0004285501199774444\n",
      "Time for epoch 84 is 74.99544548988342 sec\n",
      "Steps : 472600, \t Total Gen Loss : 3751.796142578125, \t Total Dis Loss : 2.5901572371367365e-05\n",
      "Steps : 472700, \t Total Gen Loss : 3801.59814453125, \t Total Dis Loss : 9.813134056457784e-06\n",
      "Steps : 472800, \t Total Gen Loss : 3506.94482421875, \t Total Dis Loss : 8.915279067878146e-06\n",
      "Steps : 472900, \t Total Gen Loss : 3753.6875, \t Total Dis Loss : 6.062224656488979e-06\n",
      "Steps : 473000, \t Total Gen Loss : 3747.6787109375, \t Total Dis Loss : 5.136826075613499e-06\n",
      "Steps : 473100, \t Total Gen Loss : 3403.697021484375, \t Total Dis Loss : 8.196342605515383e-06\n",
      "Steps : 473200, \t Total Gen Loss : 3665.365478515625, \t Total Dis Loss : 5.876041541341692e-06\n",
      "Steps : 473300, \t Total Gen Loss : 3794.57763671875, \t Total Dis Loss : 1.0081920663651545e-05\n",
      "Steps : 473400, \t Total Gen Loss : 3608.699951171875, \t Total Dis Loss : 1.883097297650238e-06\n",
      "Steps : 473500, \t Total Gen Loss : 3562.03466796875, \t Total Dis Loss : 1.4050773643248249e-05\n",
      "Steps : 473600, \t Total Gen Loss : 3105.46240234375, \t Total Dis Loss : 7.39134156901855e-06\n",
      "Steps : 473700, \t Total Gen Loss : 3493.017333984375, \t Total Dis Loss : 2.75885076916893e-06\n",
      "Steps : 473800, \t Total Gen Loss : 3600.739501953125, \t Total Dis Loss : 4.8259360482916236e-05\n",
      "Steps : 473900, \t Total Gen Loss : 3561.3720703125, \t Total Dis Loss : 4.132538379053585e-05\n",
      "Steps : 474000, \t Total Gen Loss : 3793.31787109375, \t Total Dis Loss : 1.3338150893105194e-05\n",
      "Steps : 474100, \t Total Gen Loss : 4059.076171875, \t Total Dis Loss : 3.1900228350423276e-05\n",
      "Steps : 474200, \t Total Gen Loss : 3433.680419921875, \t Total Dis Loss : 3.192582880728878e-06\n",
      "Steps : 474300, \t Total Gen Loss : 3702.99365234375, \t Total Dis Loss : 6.627973903050588e-07\n",
      "Steps : 474400, \t Total Gen Loss : 3696.088623046875, \t Total Dis Loss : 1.220601006934885e-05\n",
      "Steps : 474500, \t Total Gen Loss : 3978.486328125, \t Total Dis Loss : 3.773443040699931e-06\n",
      "Steps : 474600, \t Total Gen Loss : 3776.28271484375, \t Total Dis Loss : 1.1250518809902132e-06\n",
      "Steps : 474700, \t Total Gen Loss : 3511.837890625, \t Total Dis Loss : 3.422938561925548e-06\n",
      "Steps : 474800, \t Total Gen Loss : 4061.4560546875, \t Total Dis Loss : 2.1383606508607045e-05\n",
      "Steps : 474900, \t Total Gen Loss : 3128.818603515625, \t Total Dis Loss : 3.482427564449608e-05\n",
      "Steps : 475000, \t Total Gen Loss : 2904.17041015625, \t Total Dis Loss : 9.587912791175768e-05\n",
      "Steps : 475100, \t Total Gen Loss : 3718.453369140625, \t Total Dis Loss : 1.8698148096518707e-06\n",
      "Steps : 475200, \t Total Gen Loss : 3015.93017578125, \t Total Dis Loss : 9.777472769201268e-07\n",
      "Steps : 475300, \t Total Gen Loss : 3712.378173828125, \t Total Dis Loss : 7.616628181494889e-07\n",
      "Steps : 475400, \t Total Gen Loss : 2842.68505859375, \t Total Dis Loss : 5.577566298597958e-06\n",
      "Steps : 475500, \t Total Gen Loss : 3901.25048828125, \t Total Dis Loss : 0.00012634744052775204\n",
      "Steps : 475600, \t Total Gen Loss : 3836.415771484375, \t Total Dis Loss : 1.5422456272062846e-05\n",
      "Steps : 475700, \t Total Gen Loss : 3938.586669921875, \t Total Dis Loss : 1.2665409485634882e-05\n",
      "Steps : 475800, \t Total Gen Loss : 3486.240478515625, \t Total Dis Loss : 1.1470096069388092e-05\n",
      "Steps : 475900, \t Total Gen Loss : 3632.819580078125, \t Total Dis Loss : 2.300866981386207e-05\n",
      "Steps : 476000, \t Total Gen Loss : 4019.922607421875, \t Total Dis Loss : 4.5303044316824526e-05\n",
      "Steps : 476100, \t Total Gen Loss : 4368.41259765625, \t Total Dis Loss : 1.2554914064821787e-05\n",
      "Steps : 476200, \t Total Gen Loss : 3498.061279296875, \t Total Dis Loss : 1.1789848031185102e-06\n",
      "Steps : 476300, \t Total Gen Loss : 4005.050048828125, \t Total Dis Loss : 0.00038627610774710774\n",
      "Steps : 476400, \t Total Gen Loss : 3912.584228515625, \t Total Dis Loss : 6.683429819531739e-05\n",
      "Steps : 476500, \t Total Gen Loss : 3186.1787109375, \t Total Dis Loss : 7.350210944423452e-05\n",
      "Steps : 476600, \t Total Gen Loss : 3326.17919921875, \t Total Dis Loss : 3.2568463211646304e-05\n",
      "Steps : 476700, \t Total Gen Loss : 3715.226318359375, \t Total Dis Loss : 1.3321057849680074e-05\n",
      "Steps : 476800, \t Total Gen Loss : 3773.076171875, \t Total Dis Loss : 2.486359562681173e-06\n",
      "Steps : 476900, \t Total Gen Loss : 3648.169921875, \t Total Dis Loss : 3.4636527743714396e-06\n",
      "Steps : 477000, \t Total Gen Loss : 3357.0341796875, \t Total Dis Loss : 1.0458228643983603e-05\n",
      "Steps : 477100, \t Total Gen Loss : 4334.962890625, \t Total Dis Loss : 6.23916548647685e-06\n",
      "Steps : 477200, \t Total Gen Loss : 3508.07470703125, \t Total Dis Loss : 1.9184310531272786e-06\n",
      "Steps : 477300, \t Total Gen Loss : 3425.211181640625, \t Total Dis Loss : 1.1965725207119249e-05\n",
      "Steps : 477400, \t Total Gen Loss : 3228.818359375, \t Total Dis Loss : 5.083054020360578e-06\n",
      "Steps : 477500, \t Total Gen Loss : 3624.79443359375, \t Total Dis Loss : 4.983609869668726e-06\n",
      "Steps : 477600, \t Total Gen Loss : 3109.575927734375, \t Total Dis Loss : 4.389137393445708e-05\n",
      "Steps : 477700, \t Total Gen Loss : 3656.73193359375, \t Total Dis Loss : 3.7848067222512327e-06\n",
      "Steps : 477800, \t Total Gen Loss : 3715.982421875, \t Total Dis Loss : 2.3833104023651686e-06\n",
      "Steps : 477900, \t Total Gen Loss : 3988.709716796875, \t Total Dis Loss : 1.8187027990279603e-06\n",
      "Steps : 478000, \t Total Gen Loss : 3812.466796875, \t Total Dis Loss : 2.041965444732341e-06\n",
      "Steps : 478100, \t Total Gen Loss : 3777.669677734375, \t Total Dis Loss : 4.226379132887814e-06\n",
      "Time for epoch 85 is 75.10893893241882 sec\n",
      "Steps : 478200, \t Total Gen Loss : 3407.17333984375, \t Total Dis Loss : 1.0352831623094971e-06\n",
      "Steps : 478300, \t Total Gen Loss : 4131.625, \t Total Dis Loss : 1.0364464060330647e-06\n",
      "Steps : 478400, \t Total Gen Loss : 4042.018310546875, \t Total Dis Loss : 4.239271947881207e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 478500, \t Total Gen Loss : 3613.372314453125, \t Total Dis Loss : 0.00037386323674581945\n",
      "Steps : 478600, \t Total Gen Loss : 3509.947265625, \t Total Dis Loss : 1.3909118933952413e-05\n",
      "Steps : 478700, \t Total Gen Loss : 3672.30859375, \t Total Dis Loss : 6.108456545916852e-06\n",
      "Steps : 478800, \t Total Gen Loss : 3263.439697265625, \t Total Dis Loss : 2.084533298329916e-05\n",
      "Steps : 478900, \t Total Gen Loss : 3202.0029296875, \t Total Dis Loss : 0.0009846907341852784\n",
      "Steps : 479000, \t Total Gen Loss : 3698.811279296875, \t Total Dis Loss : 5.42918996870867e-07\n",
      "Steps : 479100, \t Total Gen Loss : 3439.69287109375, \t Total Dis Loss : 1.3554924862546613e-06\n",
      "Steps : 479200, \t Total Gen Loss : 3864.411865234375, \t Total Dis Loss : 4.4244447963137645e-06\n",
      "Steps : 479300, \t Total Gen Loss : 3193.83984375, \t Total Dis Loss : 1.8357215594733134e-05\n",
      "Steps : 479400, \t Total Gen Loss : 3469.075439453125, \t Total Dis Loss : 1.2944378795509692e-05\n",
      "Steps : 479500, \t Total Gen Loss : 3426.980712890625, \t Total Dis Loss : 1.5496752894250676e-05\n",
      "Steps : 479600, \t Total Gen Loss : 3031.8544921875, \t Total Dis Loss : 1.966486706805881e-06\n",
      "Steps : 479700, \t Total Gen Loss : 3796.02197265625, \t Total Dis Loss : 2.612899379528244e-06\n",
      "Steps : 479800, \t Total Gen Loss : 3329.627197265625, \t Total Dis Loss : 5.130479621584527e-06\n",
      "Steps : 479900, \t Total Gen Loss : 3733.0546875, \t Total Dis Loss : 6.897154435137054e-06\n",
      "Steps : 480000, \t Total Gen Loss : 3589.65966796875, \t Total Dis Loss : 4.652621555578662e-06\n",
      "Steps : 480100, \t Total Gen Loss : 3846.987060546875, \t Total Dis Loss : 2.6463969788892427e-06\n",
      "Steps : 480200, \t Total Gen Loss : 3348.4970703125, \t Total Dis Loss : 5.141160272614798e-06\n",
      "Steps : 480300, \t Total Gen Loss : 3506.224853515625, \t Total Dis Loss : 6.0448323893069755e-06\n",
      "Steps : 480400, \t Total Gen Loss : 3631.4580078125, \t Total Dis Loss : 1.8533178263169248e-06\n",
      "Steps : 480500, \t Total Gen Loss : 3083.7529296875, \t Total Dis Loss : 1.57549789037148e-06\n",
      "Steps : 480600, \t Total Gen Loss : 4087.4091796875, \t Total Dis Loss : 4.818924026039895e-06\n",
      "Steps : 480700, \t Total Gen Loss : 3544.966552734375, \t Total Dis Loss : 1.1121037459815852e-05\n",
      "Steps : 480800, \t Total Gen Loss : 3297.0126953125, \t Total Dis Loss : 1.028927954394021e-06\n",
      "Steps : 480900, \t Total Gen Loss : 3811.023681640625, \t Total Dis Loss : 1.2438606518117012e-06\n",
      "Steps : 481000, \t Total Gen Loss : 3666.38623046875, \t Total Dis Loss : 2.7609962671704125e-06\n",
      "Steps : 481100, \t Total Gen Loss : 3343.448974609375, \t Total Dis Loss : 3.947390723624267e-06\n",
      "Steps : 481200, \t Total Gen Loss : 3708.322509765625, \t Total Dis Loss : 4.4298553802946117e-07\n",
      "Steps : 481300, \t Total Gen Loss : 3787.72216796875, \t Total Dis Loss : 1.267118932446465e-06\n",
      "Steps : 481400, \t Total Gen Loss : 3573.380859375, \t Total Dis Loss : 1.0822391232068185e-06\n",
      "Steps : 481500, \t Total Gen Loss : 3076.078857421875, \t Total Dis Loss : 1.3155383840057766e-06\n",
      "Steps : 481600, \t Total Gen Loss : 3019.716064453125, \t Total Dis Loss : 1.8347953982811305e-06\n",
      "Steps : 481700, \t Total Gen Loss : 4060.63671875, \t Total Dis Loss : 7.835922588128597e-06\n",
      "Steps : 481800, \t Total Gen Loss : 3614.828369140625, \t Total Dis Loss : 1.181334255306865e-06\n",
      "Steps : 481900, \t Total Gen Loss : 3414.455810546875, \t Total Dis Loss : 1.5971893390087644e-06\n",
      "Steps : 482000, \t Total Gen Loss : 3651.9853515625, \t Total Dis Loss : 5.985061761748511e-07\n",
      "Steps : 482100, \t Total Gen Loss : 3560.567626953125, \t Total Dis Loss : 9.19235105811822e-07\n",
      "Steps : 482200, \t Total Gen Loss : 3428.59765625, \t Total Dis Loss : 6.392363502527587e-06\n",
      "Steps : 482300, \t Total Gen Loss : 3432.917724609375, \t Total Dis Loss : 2.8703923362627393e-06\n",
      "Steps : 482400, \t Total Gen Loss : 3812.289306640625, \t Total Dis Loss : 4.733504738396732e-06\n",
      "Steps : 482500, \t Total Gen Loss : 3269.921630859375, \t Total Dis Loss : 2.3490001694881357e-06\n",
      "Steps : 482600, \t Total Gen Loss : 3578.067138671875, \t Total Dis Loss : 2.021607542701531e-06\n",
      "Steps : 482700, \t Total Gen Loss : 3665.858642578125, \t Total Dis Loss : 1.6608687474217732e-06\n",
      "Steps : 482800, \t Total Gen Loss : 3534.89013671875, \t Total Dis Loss : 2.5034705686266534e-06\n",
      "Steps : 482900, \t Total Gen Loss : 3688.899169921875, \t Total Dis Loss : 1.8321965171708143e-06\n",
      "Steps : 483000, \t Total Gen Loss : 3620.741455078125, \t Total Dis Loss : 2.267962827318115e-06\n",
      "Steps : 483100, \t Total Gen Loss : 3393.241455078125, \t Total Dis Loss : 7.633618224645033e-06\n",
      "Steps : 483200, \t Total Gen Loss : 3483.0771484375, \t Total Dis Loss : 1.0099886367243016e-06\n",
      "Steps : 483300, \t Total Gen Loss : 3408.18798828125, \t Total Dis Loss : 1.0308508535672445e-06\n",
      "Steps : 483400, \t Total Gen Loss : 3805.663330078125, \t Total Dis Loss : 2.5675210508779855e-06\n",
      "Steps : 483500, \t Total Gen Loss : 3763.9619140625, \t Total Dis Loss : 4.7590410758857615e-06\n",
      "Steps : 483600, \t Total Gen Loss : 3605.4853515625, \t Total Dis Loss : 1.93941309589718e-06\n",
      "Steps : 483700, \t Total Gen Loss : 3346.9951171875, \t Total Dis Loss : 2.3872448764450382e-06\n",
      "Time for epoch 86 is 74.98890781402588 sec\n",
      "Steps : 483800, \t Total Gen Loss : 3780.696533203125, \t Total Dis Loss : 1.732137093313213e-06\n",
      "Steps : 483900, \t Total Gen Loss : 3387.924560546875, \t Total Dis Loss : 1.166204583569197e-06\n",
      "Steps : 484000, \t Total Gen Loss : 2845.886474609375, \t Total Dis Loss : 2.0772835341631435e-05\n",
      "Steps : 484100, \t Total Gen Loss : 3437.943115234375, \t Total Dis Loss : 7.564693191852712e-07\n",
      "Steps : 484200, \t Total Gen Loss : 3392.274658203125, \t Total Dis Loss : 8.012752346076013e-07\n",
      "Steps : 484300, \t Total Gen Loss : 3918.697021484375, \t Total Dis Loss : 1.202742851091898e-06\n",
      "Steps : 484400, \t Total Gen Loss : 3553.483154296875, \t Total Dis Loss : 1.2958963679920998e-06\n",
      "Steps : 484500, \t Total Gen Loss : 3416.8720703125, \t Total Dis Loss : 4.437872121343389e-06\n",
      "Steps : 484600, \t Total Gen Loss : 3702.5244140625, \t Total Dis Loss : 2.3598795451107435e-06\n",
      "Steps : 484700, \t Total Gen Loss : 3483.822021484375, \t Total Dis Loss : 7.903549885668326e-07\n",
      "Steps : 484800, \t Total Gen Loss : 3750.09765625, \t Total Dis Loss : 2.757093625405105e-06\n",
      "Steps : 484900, \t Total Gen Loss : 3878.383056640625, \t Total Dis Loss : 1.7449781353207072e-06\n",
      "Steps : 485000, \t Total Gen Loss : 3980.005615234375, \t Total Dis Loss : 2.3592269826622214e-06\n",
      "Steps : 485100, \t Total Gen Loss : 3762.179443359375, \t Total Dis Loss : 1.96883047465235e-06\n",
      "Steps : 485200, \t Total Gen Loss : 3656.089599609375, \t Total Dis Loss : 1.8620178252604092e-06\n",
      "Steps : 485300, \t Total Gen Loss : 3591.75537109375, \t Total Dis Loss : 4.615448290223867e-07\n",
      "Steps : 485400, \t Total Gen Loss : 3157.608154296875, \t Total Dis Loss : 2.470320850989083e-06\n",
      "Steps : 485500, \t Total Gen Loss : 4221.43603515625, \t Total Dis Loss : 4.036884035940602e-07\n",
      "Steps : 485600, \t Total Gen Loss : 3843.35595703125, \t Total Dis Loss : 1.7768967381925904e-06\n",
      "Steps : 485700, \t Total Gen Loss : 4283.16943359375, \t Total Dis Loss : 2.456169568176847e-06\n",
      "Steps : 485800, \t Total Gen Loss : 3021.882568359375, \t Total Dis Loss : 0.00010467493848409504\n",
      "Steps : 485900, \t Total Gen Loss : 4611.8916015625, \t Total Dis Loss : 7.989758046278439e-07\n",
      "Steps : 486000, \t Total Gen Loss : 3741.42578125, \t Total Dis Loss : 4.2909345211228356e-05\n",
      "Steps : 486100, \t Total Gen Loss : 3126.3076171875, \t Total Dis Loss : 8.92724892764818e-06\n",
      "Steps : 486200, \t Total Gen Loss : 4021.552001953125, \t Total Dis Loss : 5.817311375722056e-06\n",
      "Steps : 486300, \t Total Gen Loss : 3515.361572265625, \t Total Dis Loss : 3.707650466822088e-05\n",
      "Steps : 486400, \t Total Gen Loss : 3409.06396484375, \t Total Dis Loss : 3.696405838127248e-05\n",
      "Steps : 486500, \t Total Gen Loss : 3668.944580078125, \t Total Dis Loss : 6.47810593363829e-05\n",
      "Steps : 486600, \t Total Gen Loss : 4230.76025390625, \t Total Dis Loss : 1.7926700820680708e-05\n",
      "Steps : 486700, \t Total Gen Loss : 3728.75927734375, \t Total Dis Loss : 4.874369551544078e-05\n",
      "Steps : 486800, \t Total Gen Loss : 3392.097412109375, \t Total Dis Loss : 4.674702722695656e-06\n",
      "Steps : 486900, \t Total Gen Loss : 4294.10498046875, \t Total Dis Loss : 8.118969162751455e-06\n",
      "Steps : 487000, \t Total Gen Loss : 4191.4111328125, \t Total Dis Loss : 1.1415382914492511e-06\n",
      "Steps : 487100, \t Total Gen Loss : 3639.095703125, \t Total Dis Loss : 6.597491051252291e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 487200, \t Total Gen Loss : 3303.884521484375, \t Total Dis Loss : 9.735193771120976e-07\n",
      "Steps : 487300, \t Total Gen Loss : 3715.117431640625, \t Total Dis Loss : 8.044061132750358e-07\n",
      "Steps : 487400, \t Total Gen Loss : 3483.392578125, \t Total Dis Loss : 4.631281171896262e-06\n",
      "Steps : 487500, \t Total Gen Loss : 3032.138427734375, \t Total Dis Loss : 1.2398989383655135e-05\n",
      "Steps : 487600, \t Total Gen Loss : 4217.5966796875, \t Total Dis Loss : 2.658785888343118e-06\n",
      "Steps : 487700, \t Total Gen Loss : 3579.691650390625, \t Total Dis Loss : 1.3979195045976667e-06\n",
      "Steps : 487800, \t Total Gen Loss : 3801.39990234375, \t Total Dis Loss : 1.6918326082304702e-06\n",
      "Steps : 487900, \t Total Gen Loss : 3677.43798828125, \t Total Dis Loss : 1.884161974885501e-06\n",
      "Steps : 488000, \t Total Gen Loss : 3760.813232421875, \t Total Dis Loss : 1.1243419066886418e-06\n",
      "Steps : 488100, \t Total Gen Loss : 4364.28466796875, \t Total Dis Loss : 1.217902422467887e-06\n",
      "Steps : 488200, \t Total Gen Loss : 4518.27197265625, \t Total Dis Loss : 3.0762413416596246e-07\n",
      "Steps : 488300, \t Total Gen Loss : 3595.145751953125, \t Total Dis Loss : 2.7772776434176194e-07\n",
      "Steps : 488400, \t Total Gen Loss : 3811.718017578125, \t Total Dis Loss : 1.4766482081540744e-06\n",
      "Steps : 488500, \t Total Gen Loss : 3601.550048828125, \t Total Dis Loss : 0.0025915566366165876\n",
      "Steps : 488600, \t Total Gen Loss : 3624.263427734375, \t Total Dis Loss : 2.4141041649272665e-05\n",
      "Steps : 488700, \t Total Gen Loss : 3396.510498046875, \t Total Dis Loss : 1.0421896149637178e-05\n",
      "Steps : 488800, \t Total Gen Loss : 3484.666748046875, \t Total Dis Loss : 1.4148838545224862e-06\n",
      "Steps : 488900, \t Total Gen Loss : 3630.7177734375, \t Total Dis Loss : 0.00031854669214226305\n",
      "Steps : 489000, \t Total Gen Loss : 3658.07470703125, \t Total Dis Loss : 7.209549221443012e-05\n",
      "Steps : 489100, \t Total Gen Loss : 3562.227294921875, \t Total Dis Loss : 6.902710447320715e-05\n",
      "Steps : 489200, \t Total Gen Loss : 4067.365966796875, \t Total Dis Loss : 2.5305473172920756e-05\n",
      "Steps : 489300, \t Total Gen Loss : 3462.91015625, \t Total Dis Loss : 1.633924330235459e-05\n",
      "Time for epoch 87 is 74.97118186950684 sec\n",
      "Steps : 489400, \t Total Gen Loss : 3012.114501953125, \t Total Dis Loss : 3.0347995561896823e-05\n",
      "Steps : 489500, \t Total Gen Loss : 2770.610107421875, \t Total Dis Loss : 5.847770080436021e-05\n",
      "Steps : 489600, \t Total Gen Loss : 3912.28857421875, \t Total Dis Loss : 2.0221266822773032e-05\n",
      "Steps : 489700, \t Total Gen Loss : 3732.340576171875, \t Total Dis Loss : 3.767319503822364e-05\n",
      "Steps : 489800, \t Total Gen Loss : 3286.44775390625, \t Total Dis Loss : 1.904420423670672e-05\n",
      "Steps : 489900, \t Total Gen Loss : 3837.2880859375, \t Total Dis Loss : 2.1564662802120438e-06\n",
      "Steps : 490000, \t Total Gen Loss : 3514.423828125, \t Total Dis Loss : 6.07956565090717e-07\n",
      "Steps : 490100, \t Total Gen Loss : 3451.20361328125, \t Total Dis Loss : 1.9316169073135825e-06\n",
      "Steps : 490200, \t Total Gen Loss : 3622.005859375, \t Total Dis Loss : 4.6913996811781544e-06\n",
      "Steps : 490300, \t Total Gen Loss : 3300.68603515625, \t Total Dis Loss : 4.983747317055531e-07\n",
      "Steps : 490400, \t Total Gen Loss : 3079.835693359375, \t Total Dis Loss : 9.774987574928673e-07\n",
      "Steps : 490500, \t Total Gen Loss : 3513.88134765625, \t Total Dis Loss : 1.7053298506652936e-05\n",
      "Steps : 490600, \t Total Gen Loss : 3897.580810546875, \t Total Dis Loss : 4.4599801185540855e-06\n",
      "Steps : 490700, \t Total Gen Loss : 3330.883544921875, \t Total Dis Loss : 1.05131175587303e-06\n",
      "Steps : 490800, \t Total Gen Loss : 2983.863525390625, \t Total Dis Loss : 4.844022896577371e-06\n",
      "Steps : 490900, \t Total Gen Loss : 4094.10205078125, \t Total Dis Loss : 8.879472261469346e-06\n",
      "Steps : 491000, \t Total Gen Loss : 3848.080322265625, \t Total Dis Loss : 6.663903604930965e-06\n",
      "Steps : 491100, \t Total Gen Loss : 3535.505615234375, \t Total Dis Loss : 4.097504643141292e-06\n",
      "Steps : 491200, \t Total Gen Loss : 3440.690185546875, \t Total Dis Loss : 4.806821380043402e-06\n",
      "Steps : 491300, \t Total Gen Loss : 4063.959228515625, \t Total Dis Loss : 9.915548616845626e-06\n",
      "Steps : 491400, \t Total Gen Loss : 3566.13134765625, \t Total Dis Loss : 1.0997639492416056e-06\n",
      "Steps : 491500, \t Total Gen Loss : 3537.062255859375, \t Total Dis Loss : 1.415767655998934e-06\n",
      "Steps : 491600, \t Total Gen Loss : 3635.86328125, \t Total Dis Loss : 2.1674702566087944e-06\n",
      "Steps : 491700, \t Total Gen Loss : 4130.78271484375, \t Total Dis Loss : 7.652558906556806e-07\n",
      "Steps : 491800, \t Total Gen Loss : 3272.181396484375, \t Total Dis Loss : 0.352666974067688\n",
      "Steps : 491900, \t Total Gen Loss : 3728.578369140625, \t Total Dis Loss : 3.856055627693422e-06\n",
      "Steps : 492000, \t Total Gen Loss : 3009.751953125, \t Total Dis Loss : 1.9406697902013548e-06\n",
      "Steps : 492100, \t Total Gen Loss : 3553.880615234375, \t Total Dis Loss : 2.187915742979385e-05\n",
      "Steps : 492200, \t Total Gen Loss : 4204.80078125, \t Total Dis Loss : 1.1823100066976622e-05\n",
      "Steps : 492300, \t Total Gen Loss : 3596.0888671875, \t Total Dis Loss : 8.282344424515031e-06\n",
      "Steps : 492400, \t Total Gen Loss : 3579.767333984375, \t Total Dis Loss : 1.9762641386478208e-05\n",
      "Steps : 492500, \t Total Gen Loss : 3944.95556640625, \t Total Dis Loss : 9.955021596397273e-06\n",
      "Steps : 492600, \t Total Gen Loss : 3565.126708984375, \t Total Dis Loss : 2.3275637431652285e-05\n",
      "Steps : 492700, \t Total Gen Loss : 3189.332763671875, \t Total Dis Loss : 1.025951041810913e-05\n",
      "Steps : 492800, \t Total Gen Loss : 3403.928466796875, \t Total Dis Loss : 4.915580575470813e-06\n",
      "Steps : 492900, \t Total Gen Loss : 3673.093017578125, \t Total Dis Loss : 0.0002762383664958179\n",
      "Steps : 493000, \t Total Gen Loss : 3548.925537109375, \t Total Dis Loss : 2.3181835786090232e-05\n",
      "Steps : 493100, \t Total Gen Loss : 3898.468505859375, \t Total Dis Loss : 2.3673703253734857e-05\n",
      "Steps : 493200, \t Total Gen Loss : 3259.693603515625, \t Total Dis Loss : 1.7728270904626697e-06\n",
      "Steps : 493300, \t Total Gen Loss : 3578.02685546875, \t Total Dis Loss : 1.174992917185591e-06\n",
      "Steps : 493400, \t Total Gen Loss : 3484.0771484375, \t Total Dis Loss : 2.982978230647859e-06\n",
      "Steps : 493500, \t Total Gen Loss : 3610.898681640625, \t Total Dis Loss : 3.1292461244447622e-06\n",
      "Steps : 493600, \t Total Gen Loss : 2977.833251953125, \t Total Dis Loss : 3.0831608910375508e-06\n",
      "Steps : 493700, \t Total Gen Loss : 3826.793701171875, \t Total Dis Loss : 3.522910446918104e-06\n",
      "Steps : 493800, \t Total Gen Loss : 3791.043701171875, \t Total Dis Loss : 4.052597887493903e-06\n",
      "Steps : 493900, \t Total Gen Loss : 3716.568603515625, \t Total Dis Loss : 2.1367611680034315e-06\n",
      "Steps : 494000, \t Total Gen Loss : 3510.748046875, \t Total Dis Loss : 3.380651605766616e-06\n",
      "Steps : 494100, \t Total Gen Loss : 3160.330810546875, \t Total Dis Loss : 1.5538007573923096e-05\n",
      "Steps : 494200, \t Total Gen Loss : 3909.79150390625, \t Total Dis Loss : 1.5176909073488787e-05\n",
      "Steps : 494300, \t Total Gen Loss : 3789.627685546875, \t Total Dis Loss : 5.0363414629828185e-06\n",
      "Steps : 494400, \t Total Gen Loss : 3449.887939453125, \t Total Dis Loss : 8.464387065032497e-06\n",
      "Steps : 494500, \t Total Gen Loss : 3392.584228515625, \t Total Dis Loss : 8.239412636612542e-06\n",
      "Steps : 494600, \t Total Gen Loss : 3298.564208984375, \t Total Dis Loss : 6.426500931411283e-06\n",
      "Steps : 494700, \t Total Gen Loss : 3617.4765625, \t Total Dis Loss : 1.9086892280029133e-06\n",
      "Steps : 494800, \t Total Gen Loss : 3887.27734375, \t Total Dis Loss : 1.0299238510924624e-06\n",
      "Steps : 494900, \t Total Gen Loss : 4010.64501953125, \t Total Dis Loss : 1.2853190128225833e-05\n",
      "Steps : 495000, \t Total Gen Loss : 3538.04296875, \t Total Dis Loss : 5.68144787393976e-06\n",
      "Time for epoch 88 is 74.96131420135498 sec\n",
      "Steps : 495100, \t Total Gen Loss : 3598.530029296875, \t Total Dis Loss : 3.0916153264115565e-06\n",
      "Steps : 495200, \t Total Gen Loss : 3799.70947265625, \t Total Dis Loss : 5.27242036696407e-06\n",
      "Steps : 495300, \t Total Gen Loss : 3796.057861328125, \t Total Dis Loss : 1.515400526841404e-06\n",
      "Steps : 495400, \t Total Gen Loss : 3528.56884765625, \t Total Dis Loss : 6.335137732094154e-05\n",
      "Steps : 495500, \t Total Gen Loss : 3378.11279296875, \t Total Dis Loss : 3.248497887398116e-05\n",
      "Steps : 495600, \t Total Gen Loss : 4019.1240234375, \t Total Dis Loss : 6.179818683449412e-06\n",
      "Steps : 495700, \t Total Gen Loss : 3675.076416015625, \t Total Dis Loss : 1.3572315765486564e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 495800, \t Total Gen Loss : 3306.17919921875, \t Total Dis Loss : 2.3063039407134056e-06\n",
      "Steps : 495900, \t Total Gen Loss : 3137.69287109375, \t Total Dis Loss : 2.1590121832559817e-06\n",
      "Steps : 496000, \t Total Gen Loss : 3408.76025390625, \t Total Dis Loss : 1.3806137758365367e-05\n",
      "Steps : 496100, \t Total Gen Loss : 3913.908935546875, \t Total Dis Loss : 0.00011206846102140844\n",
      "Steps : 496200, \t Total Gen Loss : 3497.0849609375, \t Total Dis Loss : 1.3295674762048293e-05\n",
      "Steps : 496300, \t Total Gen Loss : 3458.727294921875, \t Total Dis Loss : 1.0737518323367112e-06\n",
      "Steps : 496400, \t Total Gen Loss : 3981.13037109375, \t Total Dis Loss : 8.370599971385673e-06\n",
      "Steps : 496500, \t Total Gen Loss : 3169.704345703125, \t Total Dis Loss : 1.805693500500638e-06\n",
      "Steps : 496600, \t Total Gen Loss : 3175.54931640625, \t Total Dis Loss : 1.2829716524720425e-06\n",
      "Steps : 496700, \t Total Gen Loss : 3037.678955078125, \t Total Dis Loss : 7.478174666175619e-07\n",
      "Steps : 496800, \t Total Gen Loss : 3743.058837890625, \t Total Dis Loss : 9.180497499983176e-07\n",
      "Steps : 496900, \t Total Gen Loss : 3332.453369140625, \t Total Dis Loss : 1.848212195909582e-05\n",
      "Steps : 497000, \t Total Gen Loss : 3584.28173828125, \t Total Dis Loss : 7.88375564297894e-06\n",
      "Steps : 497100, \t Total Gen Loss : 3841.7412109375, \t Total Dis Loss : 6.359244434861466e-05\n",
      "Steps : 497200, \t Total Gen Loss : 2785.922119140625, \t Total Dis Loss : 4.528847966867033e-06\n",
      "Steps : 497300, \t Total Gen Loss : 3113.176025390625, \t Total Dis Loss : 8.228698789025657e-07\n",
      "Steps : 497400, \t Total Gen Loss : 3035.557861328125, \t Total Dis Loss : 3.9166748138086405e-06\n",
      "Steps : 497500, \t Total Gen Loss : 3295.41064453125, \t Total Dis Loss : 1.5198201026578317e-06\n",
      "Steps : 497600, \t Total Gen Loss : 4015.868408203125, \t Total Dis Loss : 2.7413014322519302e-05\n",
      "Steps : 497700, \t Total Gen Loss : 3221.116455078125, \t Total Dis Loss : 1.3820535968989134e-05\n",
      "Steps : 497800, \t Total Gen Loss : 3369.40673828125, \t Total Dis Loss : 4.999906650482444e-06\n",
      "Steps : 497900, \t Total Gen Loss : 3345.99365234375, \t Total Dis Loss : 1.9283697838545777e-05\n",
      "Steps : 498000, \t Total Gen Loss : 3126.211181640625, \t Total Dis Loss : 2.536960892030038e-05\n",
      "Steps : 498100, \t Total Gen Loss : 3762.363037109375, \t Total Dis Loss : 3.3382225410605315e-06\n",
      "Steps : 498200, \t Total Gen Loss : 3554.712158203125, \t Total Dis Loss : 1.276264538319083e-05\n",
      "Steps : 498300, \t Total Gen Loss : 3268.350830078125, \t Total Dis Loss : 1.430697193427477e-05\n",
      "Steps : 498400, \t Total Gen Loss : 3936.789306640625, \t Total Dis Loss : 6.5044946495618206e-06\n",
      "Steps : 498500, \t Total Gen Loss : 3663.846435546875, \t Total Dis Loss : 9.538814629195258e-05\n",
      "Steps : 498600, \t Total Gen Loss : 3391.886962890625, \t Total Dis Loss : 2.1411303805507487e-06\n",
      "Steps : 498700, \t Total Gen Loss : 3605.9013671875, \t Total Dis Loss : 6.490600412689673e-07\n",
      "Steps : 498800, \t Total Gen Loss : 3817.367919921875, \t Total Dis Loss : 7.2695429480518214e-06\n",
      "Steps : 498900, \t Total Gen Loss : 3171.019287109375, \t Total Dis Loss : 1.0417061275802553e-06\n",
      "Steps : 499000, \t Total Gen Loss : 4120.29248046875, \t Total Dis Loss : 6.094060154282488e-06\n",
      "Steps : 499100, \t Total Gen Loss : 3602.974609375, \t Total Dis Loss : 4.529777470452245e-06\n",
      "Steps : 499200, \t Total Gen Loss : 2962.413818359375, \t Total Dis Loss : 1.1660841664706822e-05\n",
      "Steps : 499300, \t Total Gen Loss : 3556.40478515625, \t Total Dis Loss : 6.522091098304372e-07\n",
      "Steps : 499400, \t Total Gen Loss : 4205.29931640625, \t Total Dis Loss : 1.4042552720638923e-06\n",
      "Steps : 499500, \t Total Gen Loss : 3419.552001953125, \t Total Dis Loss : 7.519191058236174e-06\n",
      "Steps : 499600, \t Total Gen Loss : 3529.216064453125, \t Total Dis Loss : 9.235355946657364e-07\n",
      "Steps : 499700, \t Total Gen Loss : 3917.771728515625, \t Total Dis Loss : 8.269889235634764e-07\n",
      "Steps : 499800, \t Total Gen Loss : 3352.8935546875, \t Total Dis Loss : 2.942482751677744e-06\n",
      "Steps : 499900, \t Total Gen Loss : 3923.743408203125, \t Total Dis Loss : 3.831146386801265e-07\n",
      "Steps : 500000, \t Total Gen Loss : 3548.272705078125, \t Total Dis Loss : 1.1077214367105626e-05\n",
      "Steps : 500100, \t Total Gen Loss : 3902.121337890625, \t Total Dis Loss : 2.0795848740817746e-06\n",
      "Steps : 500200, \t Total Gen Loss : 3793.264404296875, \t Total Dis Loss : 2.4282712729473133e-06\n",
      "Steps : 500300, \t Total Gen Loss : 3602.267578125, \t Total Dis Loss : 6.972483333811397e-06\n",
      "Steps : 500400, \t Total Gen Loss : 3658.126708984375, \t Total Dis Loss : 3.9629771890759e-06\n",
      "Steps : 500500, \t Total Gen Loss : 3375.705810546875, \t Total Dis Loss : 8.24271046440117e-05\n",
      "Steps : 500600, \t Total Gen Loss : 3131.532470703125, \t Total Dis Loss : 1.7892623418447329e-06\n",
      "Time for epoch 89 is 74.97324109077454 sec\n",
      "Steps : 500700, \t Total Gen Loss : 4068.333740234375, \t Total Dis Loss : 7.1491190283268224e-06\n",
      "Steps : 500800, \t Total Gen Loss : 3635.89013671875, \t Total Dis Loss : 6.134225259302184e-06\n",
      "Steps : 500900, \t Total Gen Loss : 4272.47412109375, \t Total Dis Loss : 1.8809662378771463e-06\n",
      "Steps : 501000, \t Total Gen Loss : 3846.66796875, \t Total Dis Loss : 2.2020985852577724e-06\n",
      "Steps : 501100, \t Total Gen Loss : 3495.61865234375, \t Total Dis Loss : 2.6976649678545073e-06\n",
      "Steps : 501200, \t Total Gen Loss : 3262.805419921875, \t Total Dis Loss : 5.072728299637674e-07\n",
      "Steps : 501300, \t Total Gen Loss : 4016.733154296875, \t Total Dis Loss : 5.278744765746524e-07\n",
      "Steps : 501400, \t Total Gen Loss : 2958.87646484375, \t Total Dis Loss : 4.205350876418379e-07\n",
      "Steps : 501500, \t Total Gen Loss : 3952.3984375, \t Total Dis Loss : 1.669624907663092e-06\n",
      "Steps : 501600, \t Total Gen Loss : 3181.275390625, \t Total Dis Loss : 9.881081496132538e-05\n",
      "Steps : 501700, \t Total Gen Loss : 3713.126708984375, \t Total Dis Loss : 4.597483348334208e-05\n",
      "Steps : 501800, \t Total Gen Loss : 3292.775146484375, \t Total Dis Loss : 5.6863835197873414e-05\n",
      "Steps : 501900, \t Total Gen Loss : 3418.946044921875, \t Total Dis Loss : 3.302759432699531e-05\n",
      "Steps : 502000, \t Total Gen Loss : 3543.13232421875, \t Total Dis Loss : 1.3979065442981664e-05\n",
      "Steps : 502100, \t Total Gen Loss : 3704.556640625, \t Total Dis Loss : 2.8466225558076985e-05\n",
      "Steps : 502200, \t Total Gen Loss : 3512.58056640625, \t Total Dis Loss : 1.2790134860551916e-05\n",
      "Steps : 502300, \t Total Gen Loss : 3639.9580078125, \t Total Dis Loss : 9.284708539780695e-06\n",
      "Steps : 502400, \t Total Gen Loss : 3720.90234375, \t Total Dis Loss : 4.460745458345627e-06\n",
      "Steps : 502500, \t Total Gen Loss : 3493.154541015625, \t Total Dis Loss : 8.2129718066426e-06\n",
      "Steps : 502600, \t Total Gen Loss : 3427.69970703125, \t Total Dis Loss : 6.915382982697338e-06\n",
      "Steps : 502700, \t Total Gen Loss : 3260.59521484375, \t Total Dis Loss : 6.383874278981239e-06\n",
      "Steps : 502800, \t Total Gen Loss : 3719.73974609375, \t Total Dis Loss : 4.8936840357782785e-06\n",
      "Steps : 502900, \t Total Gen Loss : 3985.253662109375, \t Total Dis Loss : 4.860889021074399e-06\n",
      "Steps : 503000, \t Total Gen Loss : 3364.741455078125, \t Total Dis Loss : 1.2599409728863975e-06\n",
      "Steps : 503100, \t Total Gen Loss : 3550.572021484375, \t Total Dis Loss : 2.7176151888852473e-06\n",
      "Steps : 503200, \t Total Gen Loss : 3754.50146484375, \t Total Dis Loss : 7.089990958775161e-06\n",
      "Steps : 503300, \t Total Gen Loss : 3754.73583984375, \t Total Dis Loss : 1.1415604603826068e-05\n",
      "Steps : 503400, \t Total Gen Loss : 3513.304931640625, \t Total Dis Loss : 1.0788051440613344e-05\n",
      "Steps : 503500, \t Total Gen Loss : 3063.30615234375, \t Total Dis Loss : 6.0529191614477895e-06\n",
      "Steps : 503600, \t Total Gen Loss : 3960.014892578125, \t Total Dis Loss : 4.607438313541934e-06\n",
      "Steps : 503700, \t Total Gen Loss : 3122.56787109375, \t Total Dis Loss : 2.898695856856648e-05\n",
      "Steps : 503800, \t Total Gen Loss : 3429.342041015625, \t Total Dis Loss : 2.391654925304465e-05\n",
      "Steps : 503900, \t Total Gen Loss : 3694.452880859375, \t Total Dis Loss : 8.68382539920276e-06\n",
      "Steps : 504000, \t Total Gen Loss : 3463.28759765625, \t Total Dis Loss : 5.7380304497200996e-05\n",
      "Steps : 504100, \t Total Gen Loss : 3676.0166015625, \t Total Dis Loss : 2.82115852314746e-05\n",
      "Steps : 504200, \t Total Gen Loss : 3734.160400390625, \t Total Dis Loss : 1.9021939579033642e-06\n",
      "Steps : 504300, \t Total Gen Loss : 3548.800537109375, \t Total Dis Loss : 2.409323315077927e-06\n",
      "Steps : 504400, \t Total Gen Loss : 3598.218994140625, \t Total Dis Loss : 3.0024602892808616e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 504500, \t Total Gen Loss : 3250.9111328125, \t Total Dis Loss : 3.209350325050764e-05\n",
      "Steps : 504600, \t Total Gen Loss : 3343.186767578125, \t Total Dis Loss : 1.68828646565089e-05\n",
      "Steps : 504700, \t Total Gen Loss : 3160.815185546875, \t Total Dis Loss : 5.156352358426375e-07\n",
      "Steps : 504800, \t Total Gen Loss : 3926.20654296875, \t Total Dis Loss : 4.834141122955771e-07\n",
      "Steps : 504900, \t Total Gen Loss : 3390.71923828125, \t Total Dis Loss : 4.589245236275019e-06\n",
      "Steps : 505000, \t Total Gen Loss : 3374.55517578125, \t Total Dis Loss : 3.322415432194248e-05\n",
      "Steps : 505100, \t Total Gen Loss : 3744.914794921875, \t Total Dis Loss : 2.1675250536645763e-06\n",
      "Steps : 505200, \t Total Gen Loss : 3567.65380859375, \t Total Dis Loss : 2.768362492133747e-06\n",
      "Steps : 505300, \t Total Gen Loss : 3473.32275390625, \t Total Dis Loss : 3.968534292653203e-05\n",
      "Steps : 505400, \t Total Gen Loss : 3566.883056640625, \t Total Dis Loss : 3.825047770078527e-06\n",
      "Steps : 505500, \t Total Gen Loss : 3762.06103515625, \t Total Dis Loss : 2.3611399058154348e-07\n",
      "Steps : 505600, \t Total Gen Loss : 3812.11083984375, \t Total Dis Loss : 2.6348313895141473e-06\n",
      "Steps : 505700, \t Total Gen Loss : 3140.029296875, \t Total Dis Loss : 8.315145123560796e-07\n",
      "Steps : 505800, \t Total Gen Loss : 3550.675537109375, \t Total Dis Loss : 6.666066383331781e-07\n",
      "Steps : 505900, \t Total Gen Loss : 4257.009765625, \t Total Dis Loss : 5.473808073475084e-07\n",
      "Steps : 506000, \t Total Gen Loss : 3951.19775390625, \t Total Dis Loss : 3.528170782374218e-05\n",
      "Steps : 506100, \t Total Gen Loss : 4154.07177734375, \t Total Dis Loss : 7.763444955344312e-06\n",
      "Steps : 506200, \t Total Gen Loss : 3101.783935546875, \t Total Dis Loss : 6.788464361306978e-06\n",
      "Time for epoch 90 is 76.71505784988403 sec\n",
      "Steps : 506300, \t Total Gen Loss : 3706.235107421875, \t Total Dis Loss : 2.9551342777267564e-06\n",
      "Steps : 506400, \t Total Gen Loss : 4396.08154296875, \t Total Dis Loss : 4.3665474436238583e-07\n",
      "Steps : 506500, \t Total Gen Loss : 3982.3505859375, \t Total Dis Loss : 1.7515397985334857e-06\n",
      "Steps : 506600, \t Total Gen Loss : 3771.20654296875, \t Total Dis Loss : 4.031611524624168e-07\n",
      "Steps : 506700, \t Total Gen Loss : 3141.595458984375, \t Total Dis Loss : 1.0079917274197214e-06\n",
      "Steps : 506800, \t Total Gen Loss : 4061.3095703125, \t Total Dis Loss : 1.0831677172973286e-06\n",
      "Steps : 506900, \t Total Gen Loss : 3141.0859375, \t Total Dis Loss : 1.5086302482814062e-06\n",
      "Steps : 507000, \t Total Gen Loss : 4050.06103515625, \t Total Dis Loss : 1.5266703030647477e-06\n",
      "Steps : 507100, \t Total Gen Loss : 3206.79150390625, \t Total Dis Loss : 1.2091670669178711e-06\n",
      "Steps : 507200, \t Total Gen Loss : 3726.0068359375, \t Total Dis Loss : 9.224900168192107e-07\n",
      "Steps : 507300, \t Total Gen Loss : 4036.7626953125, \t Total Dis Loss : 2.331034465896664e-06\n",
      "Steps : 507400, \t Total Gen Loss : 3707.573974609375, \t Total Dis Loss : 3.0716532251062745e-07\n",
      "Steps : 507500, \t Total Gen Loss : 3841.799072265625, \t Total Dis Loss : 1.873843757493887e-06\n",
      "Steps : 507600, \t Total Gen Loss : 3262.76708984375, \t Total Dis Loss : 5.66292874282226e-05\n",
      "Steps : 507700, \t Total Gen Loss : 3465.279052734375, \t Total Dis Loss : 4.2466367631277535e-06\n",
      "Steps : 507800, \t Total Gen Loss : 3483.974853515625, \t Total Dis Loss : 3.855811883113347e-06\n",
      "Steps : 507900, \t Total Gen Loss : 3950.0771484375, \t Total Dis Loss : 6.893410500197206e-06\n",
      "Steps : 508000, \t Total Gen Loss : 3761.5888671875, \t Total Dis Loss : 6.215148005139781e-06\n",
      "Steps : 508100, \t Total Gen Loss : 4270.3359375, \t Total Dis Loss : 5.095938377053244e-06\n",
      "Steps : 508200, \t Total Gen Loss : 3446.653076171875, \t Total Dis Loss : 0.00036439349059946835\n",
      "Steps : 508300, \t Total Gen Loss : 3205.206298828125, \t Total Dis Loss : 3.406604491829057e-07\n",
      "Steps : 508400, \t Total Gen Loss : 3467.698486328125, \t Total Dis Loss : 4.131072273594327e-06\n",
      "Steps : 508500, \t Total Gen Loss : 3430.763671875, \t Total Dis Loss : 5.455994482872484e-07\n",
      "Steps : 508600, \t Total Gen Loss : 3661.83447265625, \t Total Dis Loss : 7.316274604818318e-06\n",
      "Steps : 508700, \t Total Gen Loss : 3899.8251953125, \t Total Dis Loss : 5.008583343624196e-07\n",
      "Steps : 508800, \t Total Gen Loss : 3494.20458984375, \t Total Dis Loss : 1.3189660421630833e-06\n",
      "Steps : 508900, \t Total Gen Loss : 3338.185546875, \t Total Dis Loss : 3.4304309792787535e-07\n",
      "Steps : 509000, \t Total Gen Loss : 3760.740234375, \t Total Dis Loss : 5.662897706315562e-07\n",
      "Steps : 509100, \t Total Gen Loss : 3140.986572265625, \t Total Dis Loss : 1.249057390850794e-07\n",
      "Steps : 509200, \t Total Gen Loss : 3718.62353515625, \t Total Dis Loss : 1.1157871995237656e-06\n",
      "Steps : 509300, \t Total Gen Loss : 3584.343017578125, \t Total Dis Loss : 1.6186827451747376e-06\n",
      "Steps : 509400, \t Total Gen Loss : 3506.364013671875, \t Total Dis Loss : 6.13626980339177e-08\n",
      "Steps : 509500, \t Total Gen Loss : 3259.016845703125, \t Total Dis Loss : 4.727552004624158e-05\n",
      "Steps : 509600, \t Total Gen Loss : 3388.0908203125, \t Total Dis Loss : 9.90438479675504e-07\n",
      "Steps : 509700, \t Total Gen Loss : 3282.798095703125, \t Total Dis Loss : 1.1479835393402027e-06\n",
      "Steps : 509800, \t Total Gen Loss : 3483.118896484375, \t Total Dis Loss : 2.2652702682535164e-06\n",
      "Steps : 509900, \t Total Gen Loss : 3461.3486328125, \t Total Dis Loss : 1.9879082628904143e-06\n",
      "Steps : 510000, \t Total Gen Loss : 3903.618408203125, \t Total Dis Loss : 5.831686394230928e-06\n",
      "Steps : 510100, \t Total Gen Loss : 2922.116943359375, \t Total Dis Loss : 1.3141915360392886e-06\n",
      "Steps : 510200, \t Total Gen Loss : 3102.277099609375, \t Total Dis Loss : 9.879136086965445e-07\n",
      "Steps : 510300, \t Total Gen Loss : 3723.52099609375, \t Total Dis Loss : 1.58145621753647e-06\n",
      "Steps : 510400, \t Total Gen Loss : 3683.18408203125, \t Total Dis Loss : 6.370289611368207e-07\n",
      "Steps : 510500, \t Total Gen Loss : 3568.697021484375, \t Total Dis Loss : 3.0925884857424535e-06\n",
      "Steps : 510600, \t Total Gen Loss : 3554.419677734375, \t Total Dis Loss : 1.9738430978577526e-07\n",
      "Steps : 510700, \t Total Gen Loss : 2970.88818359375, \t Total Dis Loss : 6.553204912052024e-07\n",
      "Steps : 510800, \t Total Gen Loss : 3900.932373046875, \t Total Dis Loss : 2.4363936972804368e-05\n",
      "Steps : 510900, \t Total Gen Loss : 3972.486572265625, \t Total Dis Loss : 1.3645426406583283e-06\n",
      "Steps : 511000, \t Total Gen Loss : 3608.3056640625, \t Total Dis Loss : 1.2485232900871779e-06\n",
      "Steps : 511100, \t Total Gen Loss : 3465.929443359375, \t Total Dis Loss : 1.9161334421369247e-06\n",
      "Steps : 511200, \t Total Gen Loss : 3559.4296875, \t Total Dis Loss : 2.280295575474156e-06\n",
      "Steps : 511300, \t Total Gen Loss : 3198.474853515625, \t Total Dis Loss : 1.6351549447790603e-06\n",
      "Steps : 511400, \t Total Gen Loss : 3533.21240234375, \t Total Dis Loss : 6.545804467350536e-07\n",
      "Steps : 511500, \t Total Gen Loss : 2994.33349609375, \t Total Dis Loss : 2.8555566586874193e-06\n",
      "Steps : 511600, \t Total Gen Loss : 3708.803466796875, \t Total Dis Loss : 1.1282206742180279e-06\n",
      "Steps : 511700, \t Total Gen Loss : 3914.405029296875, \t Total Dis Loss : 3.0659580261271913e-06\n",
      "Steps : 511800, \t Total Gen Loss : 3663.450927734375, \t Total Dis Loss : 3.1272475098376162e-06\n",
      "Time for epoch 91 is 76.65181732177734 sec\n",
      "Steps : 511900, \t Total Gen Loss : 3445.386474609375, \t Total Dis Loss : 6.204791134223342e-06\n",
      "Steps : 512000, \t Total Gen Loss : 3567.7314453125, \t Total Dis Loss : 1.0775413556984859e-06\n",
      "Steps : 512100, \t Total Gen Loss : 3566.9951171875, \t Total Dis Loss : 2.539269416956813e-06\n",
      "Steps : 512200, \t Total Gen Loss : 3354.553466796875, \t Total Dis Loss : 0.00013152054452802986\n",
      "Steps : 512300, \t Total Gen Loss : 4089.739013671875, \t Total Dis Loss : 0.00012419687118381262\n",
      "Steps : 512400, \t Total Gen Loss : 3647.324462890625, \t Total Dis Loss : 4.5952951040817425e-05\n",
      "Steps : 512500, \t Total Gen Loss : 3577.64404296875, \t Total Dis Loss : 9.666407095210161e-06\n",
      "Steps : 512600, \t Total Gen Loss : 3436.79736328125, \t Total Dis Loss : 3.5785440104518784e-06\n",
      "Steps : 512700, \t Total Gen Loss : 3551.560546875, \t Total Dis Loss : 5.649987360811792e-05\n",
      "Steps : 512800, \t Total Gen Loss : 3544.825439453125, \t Total Dis Loss : 1.0157898941542953e-05\n",
      "Steps : 512900, \t Total Gen Loss : 3673.596435546875, \t Total Dis Loss : 2.125732635249733e-06\n",
      "Steps : 513000, \t Total Gen Loss : 3984.4404296875, \t Total Dis Loss : 1.9299845007481053e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 513100, \t Total Gen Loss : 3751.6162109375, \t Total Dis Loss : 1.510524089098908e-05\n",
      "Steps : 513200, \t Total Gen Loss : 3665.13330078125, \t Total Dis Loss : 1.5088915461092256e-05\n",
      "Steps : 513300, \t Total Gen Loss : 3125.92724609375, \t Total Dis Loss : 9.766806033439934e-05\n",
      "Steps : 513400, \t Total Gen Loss : 3762.016845703125, \t Total Dis Loss : 7.259558060468407e-06\n",
      "Steps : 513500, \t Total Gen Loss : 3434.06689453125, \t Total Dis Loss : 4.668872009006009e-07\n",
      "Steps : 513600, \t Total Gen Loss : 3106.14794921875, \t Total Dis Loss : 4.84728479932528e-05\n",
      "Steps : 513700, \t Total Gen Loss : 3017.123291015625, \t Total Dis Loss : 2.7195370421395637e-05\n",
      "Steps : 513800, \t Total Gen Loss : 3762.109375, \t Total Dis Loss : 7.3861292548826896e-06\n",
      "Steps : 513900, \t Total Gen Loss : 3322.587890625, \t Total Dis Loss : 6.775149358873023e-06\n",
      "Steps : 514000, \t Total Gen Loss : 3516.567626953125, \t Total Dis Loss : 1.3354601833270863e-05\n",
      "Steps : 514100, \t Total Gen Loss : 3660.260986328125, \t Total Dis Loss : 2.9802881726936903e-06\n",
      "Steps : 514200, \t Total Gen Loss : 4043.804931640625, \t Total Dis Loss : 4.785786586580798e-06\n",
      "Steps : 514300, \t Total Gen Loss : 3593.462158203125, \t Total Dis Loss : 1.1288684618193656e-05\n",
      "Steps : 514400, \t Total Gen Loss : 3802.152099609375, \t Total Dis Loss : 3.589454991015373e-06\n",
      "Steps : 514500, \t Total Gen Loss : 3951.59521484375, \t Total Dis Loss : 3.9965725591173396e-06\n",
      "Steps : 514600, \t Total Gen Loss : 3783.392333984375, \t Total Dis Loss : 4.016447201138362e-05\n",
      "Steps : 514700, \t Total Gen Loss : 3634.705322265625, \t Total Dis Loss : 2.876706503229798e-06\n",
      "Steps : 514800, \t Total Gen Loss : 3952.451416015625, \t Total Dis Loss : 1.6691162727511255e-06\n",
      "Steps : 514900, \t Total Gen Loss : 3461.803955078125, \t Total Dis Loss : 5.5630971473874524e-06\n",
      "Steps : 515000, \t Total Gen Loss : 3732.7529296875, \t Total Dis Loss : 1.3562940694100689e-05\n",
      "Steps : 515100, \t Total Gen Loss : 4049.59375, \t Total Dis Loss : 4.225134944135789e-06\n",
      "Steps : 515200, \t Total Gen Loss : 3505.911376953125, \t Total Dis Loss : 1.1540830655576428e-06\n",
      "Steps : 515300, \t Total Gen Loss : 3918.77197265625, \t Total Dis Loss : 2.563467660365859e-06\n",
      "Steps : 515400, \t Total Gen Loss : 3912.668701171875, \t Total Dis Loss : 2.0807262899324996e-06\n",
      "Steps : 515500, \t Total Gen Loss : 4338.89501953125, \t Total Dis Loss : 6.6607581175048836e-06\n",
      "Steps : 515600, \t Total Gen Loss : 2721.939697265625, \t Total Dis Loss : 5.244139629212441e-06\n",
      "Steps : 515700, \t Total Gen Loss : 3774.7578125, \t Total Dis Loss : 1.0247459613310639e-05\n",
      "Steps : 515800, \t Total Gen Loss : 3790.155517578125, \t Total Dis Loss : 1.0751343779702438e-06\n",
      "Steps : 515900, \t Total Gen Loss : 3442.7197265625, \t Total Dis Loss : 2.217608653154457e-06\n",
      "Steps : 516000, \t Total Gen Loss : 3210.522216796875, \t Total Dis Loss : 1.2271672176211723e-06\n",
      "Steps : 516100, \t Total Gen Loss : 3935.762451171875, \t Total Dis Loss : 5.690007469638658e-07\n",
      "Steps : 516200, \t Total Gen Loss : 4011.41259765625, \t Total Dis Loss : 1.3864708989785868e-06\n",
      "Steps : 516300, \t Total Gen Loss : 3224.2294921875, \t Total Dis Loss : 1.3536216556531144e-06\n",
      "Steps : 516400, \t Total Gen Loss : 3727.78857421875, \t Total Dis Loss : 2.43345283479357e-07\n",
      "Steps : 516500, \t Total Gen Loss : 3215.01123046875, \t Total Dis Loss : 2.424017338853446e-06\n",
      "Steps : 516600, \t Total Gen Loss : 3824.12548828125, \t Total Dis Loss : 2.8329945962468628e-06\n",
      "Steps : 516700, \t Total Gen Loss : 3358.897705078125, \t Total Dis Loss : 1.8892485968535766e-05\n",
      "Steps : 516800, \t Total Gen Loss : 3686.064453125, \t Total Dis Loss : 4.28788962381077e-06\n",
      "Steps : 516900, \t Total Gen Loss : 3726.80615234375, \t Total Dis Loss : 2.925672015408054e-05\n",
      "Steps : 517000, \t Total Gen Loss : 4179.70703125, \t Total Dis Loss : 2.799476897052955e-05\n",
      "Steps : 517100, \t Total Gen Loss : 3486.41357421875, \t Total Dis Loss : 4.994146820536116e-06\n",
      "Steps : 517200, \t Total Gen Loss : 3185.3203125, \t Total Dis Loss : 2.115016286552418e-06\n",
      "Steps : 517300, \t Total Gen Loss : 3634.623046875, \t Total Dis Loss : 2.839766693796264e-06\n",
      "Steps : 517400, \t Total Gen Loss : 4044.764404296875, \t Total Dis Loss : 2.86979502561735e-06\n",
      "Steps : 517500, \t Total Gen Loss : 3871.45947265625, \t Total Dis Loss : 8.481417171424255e-05\n",
      "Time for epoch 92 is 75.52925634384155 sec\n",
      "Steps : 517600, \t Total Gen Loss : 3449.3984375, \t Total Dis Loss : 1.9576249542296864e-05\n",
      "Steps : 517700, \t Total Gen Loss : 3712.153076171875, \t Total Dis Loss : 1.9905430235667154e-05\n",
      "Steps : 517800, \t Total Gen Loss : 4180.55615234375, \t Total Dis Loss : 1.4342237591336016e-05\n",
      "Steps : 517900, \t Total Gen Loss : 3468.896240234375, \t Total Dis Loss : 3.447515337029472e-05\n",
      "Steps : 518000, \t Total Gen Loss : 3470.752685546875, \t Total Dis Loss : 7.178228315751767e-06\n",
      "Steps : 518100, \t Total Gen Loss : 3742.615966796875, \t Total Dis Loss : 2.114224116667174e-06\n",
      "Steps : 518200, \t Total Gen Loss : 3155.443359375, \t Total Dis Loss : 0.00034986285027116537\n",
      "Steps : 518300, \t Total Gen Loss : 3186.685302734375, \t Total Dis Loss : 2.2640078896074556e-06\n",
      "Steps : 518400, \t Total Gen Loss : 3275.22998046875, \t Total Dis Loss : 1.2436006727511995e-05\n",
      "Steps : 518500, \t Total Gen Loss : 3833.323486328125, \t Total Dis Loss : 8.710390488886333e-07\n",
      "Steps : 518600, \t Total Gen Loss : 3373.279296875, \t Total Dis Loss : 1.4095555798121495e-06\n",
      "Steps : 518700, \t Total Gen Loss : 3216.1298828125, \t Total Dis Loss : 2.5356146124977386e-06\n",
      "Steps : 518800, \t Total Gen Loss : 3214.48779296875, \t Total Dis Loss : 1.0721187209128402e-05\n",
      "Steps : 518900, \t Total Gen Loss : 4118.44580078125, \t Total Dis Loss : 5.753629011451267e-05\n",
      "Steps : 519000, \t Total Gen Loss : 3780.137939453125, \t Total Dis Loss : 1.188334499602206e-05\n",
      "Steps : 519100, \t Total Gen Loss : 3333.44287109375, \t Total Dis Loss : 8.454458111373242e-07\n",
      "Steps : 519200, \t Total Gen Loss : 3992.022705078125, \t Total Dis Loss : 2.633873350532667e-07\n",
      "Steps : 519300, \t Total Gen Loss : 3731.48291015625, \t Total Dis Loss : 2.373863935645204e-06\n",
      "Steps : 519400, \t Total Gen Loss : 3764.41796875, \t Total Dis Loss : 0.00039983412716537714\n",
      "Steps : 519500, \t Total Gen Loss : 3351.091796875, \t Total Dis Loss : 6.553593152602843e-07\n",
      "Steps : 519600, \t Total Gen Loss : 3355.720458984375, \t Total Dis Loss : 2.258667791465996e-06\n",
      "Steps : 519700, \t Total Gen Loss : 3194.766357421875, \t Total Dis Loss : 1.7429140370950336e-06\n",
      "Steps : 519800, \t Total Gen Loss : 3086.691162109375, \t Total Dis Loss : 1.7429834997528815e-06\n",
      "Steps : 519900, \t Total Gen Loss : 3679.38818359375, \t Total Dis Loss : 1.0472427902641357e-06\n",
      "Steps : 520000, \t Total Gen Loss : 3420.260498046875, \t Total Dis Loss : 1.6780458054199698e-06\n",
      "Steps : 520100, \t Total Gen Loss : 3607.666015625, \t Total Dis Loss : 2.4418322936980985e-06\n",
      "Steps : 520200, \t Total Gen Loss : 3467.704833984375, \t Total Dis Loss : 3.870925411320059e-06\n",
      "Steps : 520300, \t Total Gen Loss : 3604.20361328125, \t Total Dis Loss : 1.144081352322246e-06\n",
      "Steps : 520400, \t Total Gen Loss : 3691.242431640625, \t Total Dis Loss : 1.3945173122920096e-06\n",
      "Steps : 520500, \t Total Gen Loss : 3496.638916015625, \t Total Dis Loss : 2.6361674372310517e-06\n",
      "Steps : 520600, \t Total Gen Loss : 3657.08154296875, \t Total Dis Loss : 2.804739779094234e-05\n",
      "Steps : 520700, \t Total Gen Loss : 3882.11669921875, \t Total Dis Loss : 1.9262755813542753e-05\n",
      "Steps : 520800, \t Total Gen Loss : 4240.20751953125, \t Total Dis Loss : 1.9863433408318087e-05\n",
      "Steps : 520900, \t Total Gen Loss : 3701.864990234375, \t Total Dis Loss : 4.0029644878814e-06\n",
      "Steps : 521000, \t Total Gen Loss : 4336.0546875, \t Total Dis Loss : 1.606854857527651e-05\n",
      "Steps : 521100, \t Total Gen Loss : 3334.261474609375, \t Total Dis Loss : 4.353965778136626e-06\n",
      "Steps : 521200, \t Total Gen Loss : 3325.98291015625, \t Total Dis Loss : 5.405394858826185e-06\n",
      "Steps : 521300, \t Total Gen Loss : 3335.857177734375, \t Total Dis Loss : 1.5120667740120552e-06\n",
      "Steps : 521400, \t Total Gen Loss : 3604.607177734375, \t Total Dis Loss : 2.5630261006881483e-06\n",
      "Steps : 521500, \t Total Gen Loss : 3643.748291015625, \t Total Dis Loss : 9.148996014118893e-07\n",
      "Steps : 521600, \t Total Gen Loss : 3773.83642578125, \t Total Dis Loss : 2.681507794477511e-06\n",
      "Steps : 521700, \t Total Gen Loss : 3811.271484375, \t Total Dis Loss : 7.400522008538246e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 521800, \t Total Gen Loss : 3409.091552734375, \t Total Dis Loss : 7.21656579116825e-06\n",
      "Steps : 521900, \t Total Gen Loss : 3123.13623046875, \t Total Dis Loss : 1.576246359036304e-05\n",
      "Steps : 522000, \t Total Gen Loss : 3619.100341796875, \t Total Dis Loss : 1.6578911527176388e-05\n",
      "Steps : 522100, \t Total Gen Loss : 3654.435302734375, \t Total Dis Loss : 3.4911263355752453e-06\n",
      "Steps : 522200, \t Total Gen Loss : 3481.637939453125, \t Total Dis Loss : 5.353824235498905e-06\n",
      "Steps : 522300, \t Total Gen Loss : 3478.97412109375, \t Total Dis Loss : 2.681046453290037e-06\n",
      "Steps : 522400, \t Total Gen Loss : 4059.66259765625, \t Total Dis Loss : 1.4965792161092395e-06\n",
      "Steps : 522500, \t Total Gen Loss : 3865.010498046875, \t Total Dis Loss : 3.390135361769353e-06\n",
      "Steps : 522600, \t Total Gen Loss : 3494.15869140625, \t Total Dis Loss : 6.048243903933326e-06\n",
      "Steps : 522700, \t Total Gen Loss : 2976.5380859375, \t Total Dis Loss : 8.632786921225488e-05\n",
      "Steps : 522800, \t Total Gen Loss : 4146.322265625, \t Total Dis Loss : 6.089016579835516e-08\n",
      "Steps : 522900, \t Total Gen Loss : 3576.224365234375, \t Total Dis Loss : 1.4611830465582898e-06\n",
      "Steps : 523000, \t Total Gen Loss : 3825.911865234375, \t Total Dis Loss : 2.9715361051785294e-06\n",
      "Steps : 523100, \t Total Gen Loss : 3501.17333984375, \t Total Dis Loss : 0.003335647750645876\n",
      "Time for epoch 93 is 76.21741485595703 sec\n",
      "Steps : 523200, \t Total Gen Loss : 3187.249267578125, \t Total Dis Loss : 1.9228293695050525e-06\n",
      "Steps : 523300, \t Total Gen Loss : 3489.261962890625, \t Total Dis Loss : 7.455828381353058e-07\n",
      "Steps : 523400, \t Total Gen Loss : 3214.46728515625, \t Total Dis Loss : 6.248788849916309e-06\n",
      "Steps : 523500, \t Total Gen Loss : 3787.496826171875, \t Total Dis Loss : 1.9659955796669237e-06\n",
      "Steps : 523600, \t Total Gen Loss : 2746.1708984375, \t Total Dis Loss : 1.4769729204999749e-05\n",
      "Steps : 523700, \t Total Gen Loss : 3405.279296875, \t Total Dis Loss : 6.679664693365339e-06\n",
      "Steps : 523800, \t Total Gen Loss : 3411.8759765625, \t Total Dis Loss : 1.5306603700082633e-06\n",
      "Steps : 523900, \t Total Gen Loss : 4060.436279296875, \t Total Dis Loss : 1.2870428690803237e-05\n",
      "Steps : 524000, \t Total Gen Loss : 2934.2275390625, \t Total Dis Loss : 2.83244435195229e-06\n",
      "Steps : 524100, \t Total Gen Loss : 3516.958984375, \t Total Dis Loss : 4.220984465064248e-06\n",
      "Steps : 524200, \t Total Gen Loss : 3497.703125, \t Total Dis Loss : 2.957757033072994e-06\n",
      "Steps : 524300, \t Total Gen Loss : 4010.7177734375, \t Total Dis Loss : 0.0019024620996788144\n",
      "Steps : 524400, \t Total Gen Loss : 3740.478271484375, \t Total Dis Loss : 3.470705632935278e-05\n",
      "Steps : 524500, \t Total Gen Loss : 4055.560791015625, \t Total Dis Loss : 1.500104531260149e-06\n",
      "Steps : 524600, \t Total Gen Loss : 3738.15869140625, \t Total Dis Loss : 0.0001220842095790431\n",
      "Steps : 524700, \t Total Gen Loss : 3636.445068359375, \t Total Dis Loss : 1.3741946531808935e-05\n",
      "Steps : 524800, \t Total Gen Loss : 3366.172119140625, \t Total Dis Loss : 6.849563942523673e-05\n",
      "Steps : 524900, \t Total Gen Loss : 3985.74755859375, \t Total Dis Loss : 9.004662615552661e-07\n",
      "Steps : 525000, \t Total Gen Loss : 3296.90576171875, \t Total Dis Loss : 4.848198841500562e-06\n",
      "Steps : 525100, \t Total Gen Loss : 3122.615478515625, \t Total Dis Loss : 6.594488013433875e-07\n",
      "Steps : 525200, \t Total Gen Loss : 3481.246826171875, \t Total Dis Loss : 2.529456423872034e-06\n",
      "Steps : 525300, \t Total Gen Loss : 3017.149169921875, \t Total Dis Loss : 2.001662869588472e-05\n",
      "Steps : 525400, \t Total Gen Loss : 3752.74365234375, \t Total Dis Loss : 2.741430535024847e-06\n",
      "Steps : 525500, \t Total Gen Loss : 3161.33349609375, \t Total Dis Loss : 2.673889866855461e-06\n",
      "Steps : 525600, \t Total Gen Loss : 3780.817138671875, \t Total Dis Loss : 8.37729544400645e-07\n",
      "Steps : 525700, \t Total Gen Loss : 3735.833740234375, \t Total Dis Loss : 0.0008890567696653306\n",
      "Steps : 525800, \t Total Gen Loss : 2847.061279296875, \t Total Dis Loss : 1.6258588857454015e-06\n",
      "Steps : 525900, \t Total Gen Loss : 3875.23974609375, \t Total Dis Loss : 2.518291694286745e-06\n",
      "Steps : 526000, \t Total Gen Loss : 3803.536865234375, \t Total Dis Loss : 2.070243453999865e-06\n",
      "Steps : 526100, \t Total Gen Loss : 3118.824951171875, \t Total Dis Loss : 3.1228425996232545e-06\n",
      "Steps : 526200, \t Total Gen Loss : 3535.389404296875, \t Total Dis Loss : 5.9021031120209955e-06\n",
      "Steps : 526300, \t Total Gen Loss : 3489.094482421875, \t Total Dis Loss : 0.010488715022802353\n",
      "Steps : 526400, \t Total Gen Loss : 3396.76416015625, \t Total Dis Loss : 2.9437967441481305e-06\n",
      "Steps : 526500, \t Total Gen Loss : 2947.822998046875, \t Total Dis Loss : 3.82374128093943e-06\n",
      "Steps : 526600, \t Total Gen Loss : 3738.970458984375, \t Total Dis Loss : 4.086235639988445e-05\n",
      "Steps : 526700, \t Total Gen Loss : 3489.65234375, \t Total Dis Loss : 1.2837554095312953e-05\n",
      "Steps : 526800, \t Total Gen Loss : 3752.221923828125, \t Total Dis Loss : 7.489960808015894e-06\n",
      "Steps : 526900, \t Total Gen Loss : 3780.27197265625, \t Total Dis Loss : 1.0323374226572923e-06\n",
      "Steps : 527000, \t Total Gen Loss : 3489.642822265625, \t Total Dis Loss : 6.130003953330743e-07\n",
      "Steps : 527100, \t Total Gen Loss : 3775.60986328125, \t Total Dis Loss : 3.729770105564967e-05\n",
      "Steps : 527200, \t Total Gen Loss : 4304.6162109375, \t Total Dis Loss : 1.2040041497130005e-07\n",
      "Steps : 527300, \t Total Gen Loss : 3798.239990234375, \t Total Dis Loss : 2.5147583073703572e-06\n",
      "Steps : 527400, \t Total Gen Loss : 3310.582275390625, \t Total Dis Loss : 3.3272272048634477e-06\n",
      "Steps : 527500, \t Total Gen Loss : 3562.03076171875, \t Total Dis Loss : 2.542546781114652e-06\n",
      "Steps : 527600, \t Total Gen Loss : 4331.583984375, \t Total Dis Loss : 1.4753530876987497e-06\n",
      "Steps : 527700, \t Total Gen Loss : 3675.0126953125, \t Total Dis Loss : 2.1986625142744742e-06\n",
      "Steps : 527800, \t Total Gen Loss : 3417.283447265625, \t Total Dis Loss : 3.4515276183810784e-06\n",
      "Steps : 527900, \t Total Gen Loss : 4318.45458984375, \t Total Dis Loss : 6.15958560956642e-05\n",
      "Steps : 528000, \t Total Gen Loss : 3787.25, \t Total Dis Loss : 3.975419167545624e-05\n",
      "Steps : 528100, \t Total Gen Loss : 3365.74169921875, \t Total Dis Loss : 7.364043085544836e-06\n",
      "Steps : 528200, \t Total Gen Loss : 3102.0439453125, \t Total Dis Loss : 1.32426866912283e-05\n",
      "Steps : 528300, \t Total Gen Loss : 3483.95703125, \t Total Dis Loss : 2.492618477845099e-06\n",
      "Steps : 528400, \t Total Gen Loss : 3279.35791015625, \t Total Dis Loss : 1.25282838325802e-06\n",
      "Steps : 528500, \t Total Gen Loss : 3302.9873046875, \t Total Dis Loss : 3.2541931432206184e-05\n",
      "Steps : 528600, \t Total Gen Loss : 3431.908447265625, \t Total Dis Loss : 6.1854393607063685e-06\n",
      "Steps : 528700, \t Total Gen Loss : 2798.082763671875, \t Total Dis Loss : 4.084578904439695e-05\n",
      "Time for epoch 94 is 76.99918460845947 sec\n",
      "Steps : 528800, \t Total Gen Loss : 3674.47509765625, \t Total Dis Loss : 5.849028457305394e-05\n",
      "Steps : 528900, \t Total Gen Loss : 3986.26708984375, \t Total Dis Loss : 9.38924858928658e-06\n",
      "Steps : 529000, \t Total Gen Loss : 3616.829833984375, \t Total Dis Loss : 2.8020594982081093e-06\n",
      "Steps : 529100, \t Total Gen Loss : 3440.48193359375, \t Total Dis Loss : 1.5712106687715277e-05\n",
      "Steps : 529200, \t Total Gen Loss : 3824.803955078125, \t Total Dis Loss : 3.3706558042467805e-06\n",
      "Steps : 529300, \t Total Gen Loss : 3702.427734375, \t Total Dis Loss : 1.7815304090618156e-05\n",
      "Steps : 529400, \t Total Gen Loss : 3061.879638671875, \t Total Dis Loss : 5.799398059025407e-05\n",
      "Steps : 529500, \t Total Gen Loss : 3837.795166015625, \t Total Dis Loss : 3.30418988596648e-05\n",
      "Steps : 529600, \t Total Gen Loss : 3045.94091796875, \t Total Dis Loss : 1.136198261519894e-05\n",
      "Steps : 529700, \t Total Gen Loss : 3373.561279296875, \t Total Dis Loss : 3.301150627521565e-06\n",
      "Steps : 529800, \t Total Gen Loss : 3095.391845703125, \t Total Dis Loss : 1.3594884649137384e-06\n",
      "Steps : 529900, \t Total Gen Loss : 3354.74560546875, \t Total Dis Loss : 3.7659808640455594e-06\n",
      "Steps : 530000, \t Total Gen Loss : 3738.328125, \t Total Dis Loss : 4.195114343019668e-06\n",
      "Steps : 530100, \t Total Gen Loss : 3050.166259765625, \t Total Dis Loss : 0.004070585127919912\n",
      "Steps : 530200, \t Total Gen Loss : 4135.3837890625, \t Total Dis Loss : 9.813746146392077e-06\n",
      "Steps : 530300, \t Total Gen Loss : 3038.47802734375, \t Total Dis Loss : 9.24653941183351e-05\n",
      "Steps : 530400, \t Total Gen Loss : 4073.96533203125, \t Total Dis Loss : 2.5193445253535174e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 530500, \t Total Gen Loss : 3833.565185546875, \t Total Dis Loss : 2.0629596519938787e-07\n",
      "Steps : 530600, \t Total Gen Loss : 3484.14111328125, \t Total Dis Loss : 6.049349849490682e-06\n",
      "Steps : 530700, \t Total Gen Loss : 3655.23046875, \t Total Dis Loss : 1.4278452908911277e-05\n",
      "Steps : 530800, \t Total Gen Loss : 3286.69921875, \t Total Dis Loss : 4.3464532950565626e-07\n",
      "Steps : 530900, \t Total Gen Loss : 3058.63671875, \t Total Dis Loss : 3.8389712244679686e-06\n",
      "Steps : 531000, \t Total Gen Loss : 3548.48388671875, \t Total Dis Loss : 3.6520873436529655e-06\n",
      "Steps : 531100, \t Total Gen Loss : 3311.819580078125, \t Total Dis Loss : 3.1051290534378495e-06\n",
      "Steps : 531200, \t Total Gen Loss : 3675.9169921875, \t Total Dis Loss : 1.5653810123694711e-06\n",
      "Steps : 531300, \t Total Gen Loss : 3511.63525390625, \t Total Dis Loss : 4.163704033999238e-06\n",
      "Steps : 531400, \t Total Gen Loss : 3637.221435546875, \t Total Dis Loss : 7.684038791921921e-07\n",
      "Steps : 531500, \t Total Gen Loss : 3609.081298828125, \t Total Dis Loss : 2.1332523374439916e-06\n",
      "Steps : 531600, \t Total Gen Loss : 3713.2509765625, \t Total Dis Loss : 3.523807663441403e-06\n",
      "Steps : 531700, \t Total Gen Loss : 4079.612060546875, \t Total Dis Loss : 2.711303750402294e-05\n",
      "Steps : 531800, \t Total Gen Loss : 3219.17431640625, \t Total Dis Loss : 5.330461135599762e-06\n",
      "Steps : 531900, \t Total Gen Loss : 4048.38330078125, \t Total Dis Loss : 3.5855346141033806e-06\n",
      "Steps : 532000, \t Total Gen Loss : 3744.89990234375, \t Total Dis Loss : 5.9137822972843423e-05\n",
      "Steps : 532100, \t Total Gen Loss : 3951.58837890625, \t Total Dis Loss : 6.11585273873061e-05\n",
      "Steps : 532200, \t Total Gen Loss : 3611.1708984375, \t Total Dis Loss : 3.174831817887025e-06\n",
      "Steps : 532300, \t Total Gen Loss : 4035.699951171875, \t Total Dis Loss : 2.0470972231123596e-05\n",
      "Steps : 532400, \t Total Gen Loss : 4225.0830078125, \t Total Dis Loss : 1.436686306988122e-05\n",
      "Steps : 532500, \t Total Gen Loss : 3879.584228515625, \t Total Dis Loss : 9.531371688353829e-06\n",
      "Steps : 532600, \t Total Gen Loss : 4111.37353515625, \t Total Dis Loss : 4.139024440519279e-06\n",
      "Steps : 532700, \t Total Gen Loss : 3739.29931640625, \t Total Dis Loss : 1.646029431867646e-06\n",
      "Steps : 532800, \t Total Gen Loss : 3525.88916015625, \t Total Dis Loss : 2.04760017368244e-05\n",
      "Steps : 532900, \t Total Gen Loss : 3725.8671875, \t Total Dis Loss : 1.1971852472925093e-05\n",
      "Steps : 533000, \t Total Gen Loss : 3839.986328125, \t Total Dis Loss : 4.10549073421862e-05\n",
      "Steps : 533100, \t Total Gen Loss : 3403.868896484375, \t Total Dis Loss : 5.273516308079707e-07\n",
      "Steps : 533200, \t Total Gen Loss : 3524.498291015625, \t Total Dis Loss : 4.573728347168071e-06\n",
      "Steps : 533300, \t Total Gen Loss : 3494.029296875, \t Total Dis Loss : 5.532885552383959e-06\n",
      "Steps : 533400, \t Total Gen Loss : 3683.63818359375, \t Total Dis Loss : 1.0613548511173576e-06\n",
      "Steps : 533500, \t Total Gen Loss : 4173.60595703125, \t Total Dis Loss : 3.095470219705021e-06\n",
      "Steps : 533600, \t Total Gen Loss : 3717.832275390625, \t Total Dis Loss : 3.479636234260397e-06\n",
      "Steps : 533700, \t Total Gen Loss : 4105.5029296875, \t Total Dis Loss : 3.813991952483775e-06\n",
      "Steps : 533800, \t Total Gen Loss : 4227.35009765625, \t Total Dis Loss : 3.855171598843299e-06\n",
      "Steps : 533900, \t Total Gen Loss : 3282.049560546875, \t Total Dis Loss : 3.5791381378658116e-06\n",
      "Steps : 534000, \t Total Gen Loss : 3600.59326171875, \t Total Dis Loss : 1.1649008229142055e-06\n",
      "Steps : 534100, \t Total Gen Loss : 3372.46142578125, \t Total Dis Loss : 1.517094460723456e-05\n",
      "Steps : 534200, \t Total Gen Loss : 3602.388671875, \t Total Dis Loss : 3.349744656588882e-05\n",
      "Steps : 534300, \t Total Gen Loss : 3747.156005859375, \t Total Dis Loss : 4.601799446390942e-06\n",
      "Time for epoch 95 is 76.31082582473755 sec\n",
      "Steps : 534400, \t Total Gen Loss : 4114.5556640625, \t Total Dis Loss : 6.832913641119376e-06\n",
      "Steps : 534500, \t Total Gen Loss : 3536.818603515625, \t Total Dis Loss : 3.5103286791127175e-05\n",
      "Steps : 534600, \t Total Gen Loss : 3819.474365234375, \t Total Dis Loss : 2.7737835353036644e-06\n",
      "Steps : 534700, \t Total Gen Loss : 3689.14794921875, \t Total Dis Loss : 3.996165469288826e-05\n",
      "Steps : 534800, \t Total Gen Loss : 3234.687255859375, \t Total Dis Loss : 7.882204045017716e-06\n",
      "Steps : 534900, \t Total Gen Loss : 3403.13525390625, \t Total Dis Loss : 1.0143256758965435e-06\n",
      "Steps : 535000, \t Total Gen Loss : 4038.79345703125, \t Total Dis Loss : 3.3311637253063964e-06\n",
      "Steps : 535100, \t Total Gen Loss : 3418.848876953125, \t Total Dis Loss : 2.065661419692333e-06\n",
      "Steps : 535200, \t Total Gen Loss : 3296.412353515625, \t Total Dis Loss : 1.3426054010778898e-06\n",
      "Steps : 535300, \t Total Gen Loss : 3969.18017578125, \t Total Dis Loss : 3.0363205496541923e-06\n",
      "Steps : 535400, \t Total Gen Loss : 3789.33251953125, \t Total Dis Loss : 2.206266344728647e-06\n",
      "Steps : 535500, \t Total Gen Loss : 3650.98583984375, \t Total Dis Loss : 2.0113693608436733e-06\n",
      "Steps : 535600, \t Total Gen Loss : 3819.147705078125, \t Total Dis Loss : 9.353430527880846e-07\n",
      "Steps : 535700, \t Total Gen Loss : 3661.496826171875, \t Total Dis Loss : 9.529835551802535e-07\n",
      "Steps : 535800, \t Total Gen Loss : 2945.328125, \t Total Dis Loss : 1.4372916439242545e-06\n",
      "Steps : 535900, \t Total Gen Loss : 3684.68603515625, \t Total Dis Loss : 1.7254928934562486e-06\n",
      "Steps : 536000, \t Total Gen Loss : 3308.947265625, \t Total Dis Loss : 2.009263880609069e-06\n",
      "Steps : 536100, \t Total Gen Loss : 3793.505859375, \t Total Dis Loss : 8.39910535432864e-06\n",
      "Steps : 536200, \t Total Gen Loss : 3551.015625, \t Total Dis Loss : 4.4405813241610304e-05\n",
      "Steps : 536300, \t Total Gen Loss : 3628.67529296875, \t Total Dis Loss : 9.819005981626105e-07\n",
      "Steps : 536400, \t Total Gen Loss : 3767.19140625, \t Total Dis Loss : 3.694016470490169e-07\n",
      "Steps : 536500, \t Total Gen Loss : 3562.81494140625, \t Total Dis Loss : 2.224303716502618e-05\n",
      "Steps : 536600, \t Total Gen Loss : 3763.68017578125, \t Total Dis Loss : 4.959380021318793e-06\n",
      "Steps : 536700, \t Total Gen Loss : 3980.691162109375, \t Total Dis Loss : 8.28167685540393e-06\n",
      "Steps : 536800, \t Total Gen Loss : 3540.7119140625, \t Total Dis Loss : 0.0013985965633764863\n",
      "Steps : 536900, \t Total Gen Loss : 3376.79248046875, \t Total Dis Loss : 1.4185528016241733e-05\n",
      "Steps : 537000, \t Total Gen Loss : 3816.06005859375, \t Total Dis Loss : 8.540127964806743e-06\n",
      "Steps : 537100, \t Total Gen Loss : 3955.299560546875, \t Total Dis Loss : 3.283051682956284e-06\n",
      "Steps : 537200, \t Total Gen Loss : 3870.250732421875, \t Total Dis Loss : 5.762914270235342e-07\n",
      "Steps : 537300, \t Total Gen Loss : 3818.00390625, \t Total Dis Loss : 1.0268025107507128e-05\n",
      "Steps : 537400, \t Total Gen Loss : 3626.568603515625, \t Total Dis Loss : 2.9534548957599327e-06\n",
      "Steps : 537500, \t Total Gen Loss : 3519.18505859375, \t Total Dis Loss : 2.979393912028172e-06\n",
      "Steps : 537600, \t Total Gen Loss : 3979.126220703125, \t Total Dis Loss : 4.954725955030881e-05\n",
      "Steps : 537700, \t Total Gen Loss : 4294.02587890625, \t Total Dis Loss : 8.00321595306741e-06\n",
      "Steps : 537800, \t Total Gen Loss : 3799.330810546875, \t Total Dis Loss : 5.360448540159268e-06\n",
      "Steps : 537900, \t Total Gen Loss : 4015.452880859375, \t Total Dis Loss : 4.026385340694105e-07\n",
      "Steps : 538000, \t Total Gen Loss : 3038.117919921875, \t Total Dis Loss : 1.3155130318409647e-06\n",
      "Steps : 538100, \t Total Gen Loss : 3157.177734375, \t Total Dis Loss : 3.7705869999626884e-06\n",
      "Steps : 538200, \t Total Gen Loss : 3397.734375, \t Total Dis Loss : 0.00016481535567436367\n",
      "Steps : 538300, \t Total Gen Loss : 3436.57666015625, \t Total Dis Loss : 9.492252502241172e-06\n",
      "Steps : 538400, \t Total Gen Loss : 4301.92529296875, \t Total Dis Loss : 1.2298567526158877e-05\n",
      "Steps : 538500, \t Total Gen Loss : 3423.773681640625, \t Total Dis Loss : 1.8565428035799414e-05\n",
      "Steps : 538600, \t Total Gen Loss : 3714.1611328125, \t Total Dis Loss : 3.2734708383941324e-06\n",
      "Steps : 538700, \t Total Gen Loss : 3886.731201171875, \t Total Dis Loss : 4.2425476749485824e-06\n",
      "Steps : 538800, \t Total Gen Loss : 4076.118408203125, \t Total Dis Loss : 2.0768527519976487e-06\n",
      "Steps : 538900, \t Total Gen Loss : 3388.207763671875, \t Total Dis Loss : 1.5693417481088545e-06\n",
      "Steps : 539000, \t Total Gen Loss : 4142.94384765625, \t Total Dis Loss : 2.5750861823325977e-05\n",
      "Steps : 539100, \t Total Gen Loss : 3906.55322265625, \t Total Dis Loss : 2.0375551684992388e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 539200, \t Total Gen Loss : 3598.89794921875, \t Total Dis Loss : 7.933559800221701e-07\n",
      "Steps : 539300, \t Total Gen Loss : 3550.480712890625, \t Total Dis Loss : 5.768378741777269e-06\n",
      "Steps : 539400, \t Total Gen Loss : 3716.78125, \t Total Dis Loss : 4.835143840864475e-07\n",
      "Steps : 539500, \t Total Gen Loss : 3624.88232421875, \t Total Dis Loss : 2.436350769130513e-06\n",
      "Steps : 539600, \t Total Gen Loss : 3909.46728515625, \t Total Dis Loss : 2.9478547958206036e-07\n",
      "Steps : 539700, \t Total Gen Loss : 3411.002197265625, \t Total Dis Loss : 3.3590953307793825e-07\n",
      "Steps : 539800, \t Total Gen Loss : 3850.7333984375, \t Total Dis Loss : 3.4788567973009776e-06\n",
      "Steps : 539900, \t Total Gen Loss : 3468.194580078125, \t Total Dis Loss : 2.413936044831644e-06\n",
      "Steps : 540000, \t Total Gen Loss : 3686.218994140625, \t Total Dis Loss : 3.79721200260974e-07\n",
      "Time for epoch 96 is 76.00403690338135 sec\n",
      "Steps : 540100, \t Total Gen Loss : 3675.580810546875, \t Total Dis Loss : 2.5127942535618786e-06\n",
      "Steps : 540200, \t Total Gen Loss : 3120.75244140625, \t Total Dis Loss : 4.614219051291002e-07\n",
      "Steps : 540300, \t Total Gen Loss : 3256.329345703125, \t Total Dis Loss : 1.1655786238407018e-06\n",
      "Steps : 540400, \t Total Gen Loss : 4093.657470703125, \t Total Dis Loss : 1.078774094581604\n",
      "Steps : 540500, \t Total Gen Loss : 3856.46435546875, \t Total Dis Loss : 0.00016202300321310759\n",
      "Steps : 540600, \t Total Gen Loss : 3439.591552734375, \t Total Dis Loss : 0.0007508602575398982\n",
      "Steps : 540700, \t Total Gen Loss : 3794.982177734375, \t Total Dis Loss : 2.2552536393050104e-05\n",
      "Steps : 540800, \t Total Gen Loss : 3814.921142578125, \t Total Dis Loss : 7.818616722943261e-06\n",
      "Steps : 540900, \t Total Gen Loss : 3985.645751953125, \t Total Dis Loss : 6.549380032083718e-06\n",
      "Steps : 541000, \t Total Gen Loss : 3556.27099609375, \t Total Dis Loss : 5.384406904340722e-06\n",
      "Steps : 541100, \t Total Gen Loss : 3269.43115234375, \t Total Dis Loss : 2.1654373085766565e-06\n",
      "Steps : 541200, \t Total Gen Loss : 3773.54248046875, \t Total Dis Loss : 1.0564513104327489e-05\n",
      "Steps : 541300, \t Total Gen Loss : 2968.7109375, \t Total Dis Loss : 6.272327027545543e-06\n",
      "Steps : 541400, \t Total Gen Loss : 4171.26123046875, \t Total Dis Loss : 5.597968538495479e-06\n",
      "Steps : 541500, \t Total Gen Loss : 3637.848388671875, \t Total Dis Loss : 2.334820692340145e-06\n",
      "Steps : 541600, \t Total Gen Loss : 3452.50146484375, \t Total Dis Loss : 6.243861207622103e-06\n",
      "Steps : 541700, \t Total Gen Loss : 4002.183349609375, \t Total Dis Loss : 2.3598865936946822e-06\n",
      "Steps : 541800, \t Total Gen Loss : 3903.321044921875, \t Total Dis Loss : 5.665274329658132e-06\n",
      "Steps : 541900, \t Total Gen Loss : 4476.1865234375, \t Total Dis Loss : 6.592850695597008e-06\n",
      "Steps : 542000, \t Total Gen Loss : 3227.103515625, \t Total Dis Loss : 1.3902104001317639e-06\n",
      "Steps : 542100, \t Total Gen Loss : 3396.3291015625, \t Total Dis Loss : 2.3114312170946505e-06\n",
      "Steps : 542200, \t Total Gen Loss : 3088.48974609375, \t Total Dis Loss : 4.37079415860353e-06\n",
      "Steps : 542300, \t Total Gen Loss : 3559.095947265625, \t Total Dis Loss : 3.421777137191384e-06\n",
      "Steps : 542400, \t Total Gen Loss : 3301.9345703125, \t Total Dis Loss : 4.362761501397472e-06\n",
      "Steps : 542500, \t Total Gen Loss : 3294.467041015625, \t Total Dis Loss : 2.4777289127086988e-06\n",
      "Steps : 542600, \t Total Gen Loss : 4203.728515625, \t Total Dis Loss : 3.6017825095768785e-06\n",
      "Steps : 542700, \t Total Gen Loss : 3313.895263671875, \t Total Dis Loss : 4.134721621085191e-06\n",
      "Steps : 542800, \t Total Gen Loss : 3306.067626953125, \t Total Dis Loss : 4.647703008231474e-06\n",
      "Steps : 542900, \t Total Gen Loss : 3122.43994140625, \t Total Dis Loss : 6.994330306042684e-07\n",
      "Steps : 543000, \t Total Gen Loss : 3389.201416015625, \t Total Dis Loss : 1.2898022987428703e-06\n",
      "Steps : 543100, \t Total Gen Loss : 3503.66064453125, \t Total Dis Loss : 2.4255776224890724e-05\n",
      "Steps : 543200, \t Total Gen Loss : 2945.7216796875, \t Total Dis Loss : 1.9533385057002306e-05\n",
      "Steps : 543300, \t Total Gen Loss : 3831.899658203125, \t Total Dis Loss : 4.624406756192911e-06\n",
      "Steps : 543400, \t Total Gen Loss : 3607.4599609375, \t Total Dis Loss : 3.895571353496052e-06\n",
      "Steps : 543500, \t Total Gen Loss : 3757.63916015625, \t Total Dis Loss : 1.767365938576404e-05\n",
      "Steps : 543600, \t Total Gen Loss : 3881.86376953125, \t Total Dis Loss : 8.446041465504095e-05\n",
      "Steps : 543700, \t Total Gen Loss : 3142.854736328125, \t Total Dis Loss : 2.2802387320552953e-05\n",
      "Steps : 543800, \t Total Gen Loss : 3790.7021484375, \t Total Dis Loss : 3.849794666166417e-06\n",
      "Steps : 543900, \t Total Gen Loss : 4058.728515625, \t Total Dis Loss : 1.6566413250984624e-05\n",
      "Steps : 544000, \t Total Gen Loss : 3345.7216796875, \t Total Dis Loss : 3.983911483373959e-06\n",
      "Steps : 544100, \t Total Gen Loss : 3121.763671875, \t Total Dis Loss : 0.0001282296288991347\n",
      "Steps : 544200, \t Total Gen Loss : 3785.609130859375, \t Total Dis Loss : 1.4108431969361845e-06\n",
      "Steps : 544300, \t Total Gen Loss : 3525.923095703125, \t Total Dis Loss : 0.00011385961261112243\n",
      "Steps : 544400, \t Total Gen Loss : 4179.6982421875, \t Total Dis Loss : 1.0413832569611259e-05\n",
      "Steps : 544500, \t Total Gen Loss : 3452.543701171875, \t Total Dis Loss : 9.71904955804348e-06\n",
      "Steps : 544600, \t Total Gen Loss : 3684.16455078125, \t Total Dis Loss : 3.5310196835780516e-05\n",
      "Steps : 544700, \t Total Gen Loss : 3545.78955078125, \t Total Dis Loss : 4.040673047711607e-06\n",
      "Steps : 544800, \t Total Gen Loss : 3839.468505859375, \t Total Dis Loss : 3.991163976024836e-05\n",
      "Steps : 544900, \t Total Gen Loss : 3052.2783203125, \t Total Dis Loss : 4.758876457344741e-06\n",
      "Steps : 545000, \t Total Gen Loss : 3839.665771484375, \t Total Dis Loss : 3.5381049201532733e-06\n",
      "Steps : 545100, \t Total Gen Loss : 3531.3779296875, \t Total Dis Loss : 3.18617139782873e-06\n",
      "Steps : 545200, \t Total Gen Loss : 3448.71337890625, \t Total Dis Loss : 1.6552949091419578e-05\n",
      "Steps : 545300, \t Total Gen Loss : 3481.347412109375, \t Total Dis Loss : 1.2914396393171046e-05\n",
      "Steps : 545400, \t Total Gen Loss : 4068.923583984375, \t Total Dis Loss : 3.324945055283024e-06\n",
      "Steps : 545500, \t Total Gen Loss : 3416.290771484375, \t Total Dis Loss : 4.17178443967714e-06\n",
      "Steps : 545600, \t Total Gen Loss : 3063.577392578125, \t Total Dis Loss : 4.4121006794739515e-06\n",
      "Time for epoch 97 is 75.87512421607971 sec\n",
      "Steps : 545700, \t Total Gen Loss : 3589.84033203125, \t Total Dis Loss : 3.058312813664088e-06\n",
      "Steps : 545800, \t Total Gen Loss : 3354.542724609375, \t Total Dis Loss : 7.797395369379956e-07\n",
      "Steps : 545900, \t Total Gen Loss : 3586.32568359375, \t Total Dis Loss : 3.66973404197779e-06\n",
      "Steps : 546000, \t Total Gen Loss : 2405.42626953125, \t Total Dis Loss : 4.390166395751294e-06\n",
      "Steps : 546100, \t Total Gen Loss : 3611.110595703125, \t Total Dis Loss : 1.0050345736090094e-05\n",
      "Steps : 546200, \t Total Gen Loss : 3489.635986328125, \t Total Dis Loss : 1.9913316009478876e-06\n",
      "Steps : 546300, \t Total Gen Loss : 3877.61572265625, \t Total Dis Loss : 1.1594077022891724e-06\n",
      "Steps : 546400, \t Total Gen Loss : 3638.64990234375, \t Total Dis Loss : 2.2110293684818316e-06\n",
      "Steps : 546500, \t Total Gen Loss : 3170.728759765625, \t Total Dis Loss : 5.933462489338126e-06\n",
      "Steps : 546600, \t Total Gen Loss : 3487.783447265625, \t Total Dis Loss : 7.492775466744206e-07\n",
      "Steps : 546700, \t Total Gen Loss : 2843.93994140625, \t Total Dis Loss : 7.471740559594764e-07\n",
      "Steps : 546800, \t Total Gen Loss : 3049.25732421875, \t Total Dis Loss : 4.45008311089623e-07\n",
      "Steps : 546900, \t Total Gen Loss : 3993.58837890625, \t Total Dis Loss : 5.295409550853947e-07\n",
      "Steps : 547000, \t Total Gen Loss : 3884.8662109375, \t Total Dis Loss : 1.3012045201321598e-06\n",
      "Steps : 547100, \t Total Gen Loss : 4083.64453125, \t Total Dis Loss : 1.4158488284010673e-06\n",
      "Steps : 547200, \t Total Gen Loss : 3325.44970703125, \t Total Dis Loss : 3.3075045848818263e-06\n",
      "Steps : 547300, \t Total Gen Loss : 3114.977783203125, \t Total Dis Loss : 3.718527977980557e-06\n",
      "Steps : 547400, \t Total Gen Loss : 3933.509521484375, \t Total Dis Loss : 1.2465976396924816e-05\n",
      "Steps : 547500, \t Total Gen Loss : 3835.967041015625, \t Total Dis Loss : 2.1979506072966615e-06\n",
      "Steps : 547600, \t Total Gen Loss : 2854.4638671875, \t Total Dis Loss : 2.770962623799278e-07\n",
      "Steps : 547700, \t Total Gen Loss : 3579.319580078125, \t Total Dis Loss : 1.1365725640644087e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 547800, \t Total Gen Loss : 3465.354736328125, \t Total Dis Loss : 1.0607914191496093e-06\n",
      "Steps : 547900, \t Total Gen Loss : 4158.5947265625, \t Total Dis Loss : 4.328905447437137e-07\n",
      "Steps : 548000, \t Total Gen Loss : 3763.02978515625, \t Total Dis Loss : 2.1064793145342264e-06\n",
      "Steps : 548100, \t Total Gen Loss : 3422.43017578125, \t Total Dis Loss : 3.1104295317163633e-07\n",
      "Steps : 548200, \t Total Gen Loss : 4248.26953125, \t Total Dis Loss : 4.011542841908522e-05\n",
      "Steps : 548300, \t Total Gen Loss : 3948.457275390625, \t Total Dis Loss : 2.4476236148984754e-07\n",
      "Steps : 548400, \t Total Gen Loss : 3789.924560546875, \t Total Dis Loss : 5.147030606167391e-07\n",
      "Steps : 548500, \t Total Gen Loss : 3786.011474609375, \t Total Dis Loss : 4.973695695298375e-07\n",
      "Steps : 548600, \t Total Gen Loss : 3970.694580078125, \t Total Dis Loss : 7.998924047569744e-07\n",
      "Steps : 548700, \t Total Gen Loss : 3718.89453125, \t Total Dis Loss : 4.078322604073037e-07\n",
      "Steps : 548800, \t Total Gen Loss : 3421.76904296875, \t Total Dis Loss : 9.328030046162894e-07\n",
      "Steps : 548900, \t Total Gen Loss : 3918.15869140625, \t Total Dis Loss : 0.00010920907516265288\n",
      "Steps : 549000, \t Total Gen Loss : 3173.03125, \t Total Dis Loss : 0.00015233407611958683\n",
      "Steps : 549100, \t Total Gen Loss : 3368.036376953125, \t Total Dis Loss : 3.1357376428786665e-06\n",
      "Steps : 549200, \t Total Gen Loss : 3239.3603515625, \t Total Dis Loss : 0.00010609359742375091\n",
      "Steps : 549300, \t Total Gen Loss : 3445.191650390625, \t Total Dis Loss : 1.7978163668885827e-05\n",
      "Steps : 549400, \t Total Gen Loss : 4196.7998046875, \t Total Dis Loss : 2.773214873741381e-05\n",
      "Steps : 549500, \t Total Gen Loss : 3278.83642578125, \t Total Dis Loss : 3.4491647966206074e-05\n",
      "Steps : 549600, \t Total Gen Loss : 3019.0400390625, \t Total Dis Loss : 1.4019246918905992e-05\n",
      "Steps : 549700, \t Total Gen Loss : 3276.083251953125, \t Total Dis Loss : 1.3453117389872205e-05\n",
      "Steps : 549800, \t Total Gen Loss : 3062.04443359375, \t Total Dis Loss : 2.3132522528612753e-06\n",
      "Steps : 549900, \t Total Gen Loss : 3681.8408203125, \t Total Dis Loss : 1.0530982763157226e-05\n",
      "Steps : 550000, \t Total Gen Loss : 3695.301513671875, \t Total Dis Loss : 3.0975440949987387e-06\n",
      "Steps : 550100, \t Total Gen Loss : 3364.39990234375, \t Total Dis Loss : 3.5403811580181355e-06\n",
      "Steps : 550200, \t Total Gen Loss : 4187.23583984375, \t Total Dis Loss : 2.946519998658914e-06\n",
      "Steps : 550300, \t Total Gen Loss : 3558.32861328125, \t Total Dis Loss : 2.4136240881489357e-06\n",
      "Steps : 550400, \t Total Gen Loss : 3577.177978515625, \t Total Dis Loss : 1.6977322957245633e-05\n",
      "Steps : 550500, \t Total Gen Loss : 3267.156494140625, \t Total Dis Loss : 4.6467598622257356e-06\n",
      "Steps : 550600, \t Total Gen Loss : 3911.95654296875, \t Total Dis Loss : 8.638439794594888e-06\n",
      "Steps : 550700, \t Total Gen Loss : 3260.8505859375, \t Total Dis Loss : 9.892239177133888e-06\n",
      "Steps : 550800, \t Total Gen Loss : 4134.9169921875, \t Total Dis Loss : 2.888871222239686e-06\n",
      "Steps : 550900, \t Total Gen Loss : 3311.935302734375, \t Total Dis Loss : 1.2600675290741492e-05\n",
      "Steps : 551000, \t Total Gen Loss : 3660.83740234375, \t Total Dis Loss : 8.62997148942668e-06\n",
      "Steps : 551100, \t Total Gen Loss : 3696.85888671875, \t Total Dis Loss : 3.672658522191341e-06\n",
      "Steps : 551200, \t Total Gen Loss : 3951.656982421875, \t Total Dis Loss : 3.862275661958847e-06\n",
      "Time for epoch 98 is 75.96747183799744 sec\n",
      "Steps : 551300, \t Total Gen Loss : 3589.87841796875, \t Total Dis Loss : 0.001032662228681147\n",
      "Steps : 551400, \t Total Gen Loss : 3815.37841796875, \t Total Dis Loss : 5.258278633846203e-06\n",
      "Steps : 551500, \t Total Gen Loss : 4124.71435546875, \t Total Dis Loss : 2.224281161034014e-06\n",
      "Steps : 551600, \t Total Gen Loss : 3809.68310546875, \t Total Dis Loss : 1.6008531247280189e-06\n",
      "Steps : 551700, \t Total Gen Loss : 3653.300048828125, \t Total Dis Loss : 0.0005421913228929043\n",
      "Steps : 551800, \t Total Gen Loss : 3927.126220703125, \t Total Dis Loss : 7.560930498584639e-06\n",
      "Steps : 551900, \t Total Gen Loss : 3533.92822265625, \t Total Dis Loss : 5.645246801577741e-06\n",
      "Steps : 552000, \t Total Gen Loss : 3473.677734375, \t Total Dis Loss : 3.293849204055732e-06\n",
      "Steps : 552100, \t Total Gen Loss : 3348.06494140625, \t Total Dis Loss : 4.1563207560102455e-06\n",
      "Steps : 552200, \t Total Gen Loss : 4219.15869140625, \t Total Dis Loss : 2.8906533771078102e-06\n",
      "Steps : 552300, \t Total Gen Loss : 3254.01953125, \t Total Dis Loss : 5.713866812584456e-06\n",
      "Steps : 552400, \t Total Gen Loss : 3043.740966796875, \t Total Dis Loss : 4.7829366849327926e-06\n",
      "Steps : 552500, \t Total Gen Loss : 3369.29541015625, \t Total Dis Loss : 3.8310772652039304e-06\n",
      "Steps : 552600, \t Total Gen Loss : 3557.39990234375, \t Total Dis Loss : 1.0109941285918467e-05\n",
      "Steps : 552700, \t Total Gen Loss : 4176.61572265625, \t Total Dis Loss : 1.1384372555767186e-05\n",
      "Steps : 552800, \t Total Gen Loss : 3599.17138671875, \t Total Dis Loss : 4.769799943460384e-06\n",
      "Steps : 552900, \t Total Gen Loss : 3229.53759765625, \t Total Dis Loss : 6.107453828008147e-06\n",
      "Steps : 553000, \t Total Gen Loss : 3558.531005859375, \t Total Dis Loss : 5.9816488828801084e-06\n",
      "Steps : 553100, \t Total Gen Loss : 3270.2646484375, \t Total Dis Loss : 1.006132151815109e-06\n",
      "Steps : 553200, \t Total Gen Loss : 3834.658447265625, \t Total Dis Loss : 1.5483308288821718e-06\n",
      "Steps : 553300, \t Total Gen Loss : 3899.512939453125, \t Total Dis Loss : 2.3344768123934045e-05\n",
      "Steps : 553400, \t Total Gen Loss : 3935.537841796875, \t Total Dis Loss : 6.951174327696208e-06\n",
      "Steps : 553500, \t Total Gen Loss : 3202.569091796875, \t Total Dis Loss : 4.386949058243772e-06\n",
      "Steps : 553600, \t Total Gen Loss : 2750.6123046875, \t Total Dis Loss : 4.137095857004169e-06\n",
      "Steps : 553700, \t Total Gen Loss : 3428.210205078125, \t Total Dis Loss : 2.6966276891471352e-06\n",
      "Steps : 553800, \t Total Gen Loss : 3263.03125, \t Total Dis Loss : 7.249907412187895e-06\n",
      "Steps : 553900, \t Total Gen Loss : 3414.0166015625, \t Total Dis Loss : 1.95863231056137e-06\n",
      "Steps : 554000, \t Total Gen Loss : 3388.263671875, \t Total Dis Loss : 1.8917306761068176e-06\n",
      "Steps : 554100, \t Total Gen Loss : 3850.156005859375, \t Total Dis Loss : 4.5366941776592284e-06\n",
      "Steps : 554200, \t Total Gen Loss : 3801.51123046875, \t Total Dis Loss : 2.6614238777256105e-06\n",
      "Steps : 554300, \t Total Gen Loss : 3659.714111328125, \t Total Dis Loss : 2.2567094219994033e-06\n",
      "Steps : 554400, \t Total Gen Loss : 3848.730712890625, \t Total Dis Loss : 1.4795314200455323e-06\n",
      "Steps : 554500, \t Total Gen Loss : 3777.0634765625, \t Total Dis Loss : 2.7788632905867416e-06\n",
      "Steps : 554600, \t Total Gen Loss : 3084.647705078125, \t Total Dis Loss : 2.9537513910327107e-06\n",
      "Steps : 554700, \t Total Gen Loss : 3723.539306640625, \t Total Dis Loss : 4.2556648622849025e-06\n",
      "Steps : 554800, \t Total Gen Loss : 3200.60498046875, \t Total Dis Loss : 2.905441306211287e-06\n",
      "Steps : 554900, \t Total Gen Loss : 3592.747802734375, \t Total Dis Loss : 2.2874123715155292e-06\n",
      "Steps : 555000, \t Total Gen Loss : 3060.787841796875, \t Total Dis Loss : 4.683879524236545e-06\n",
      "Steps : 555100, \t Total Gen Loss : 3503.222900390625, \t Total Dis Loss : 0.00010521321382839233\n",
      "Steps : 555200, \t Total Gen Loss : 3656.66259765625, \t Total Dis Loss : 2.7601123292697594e-05\n",
      "Steps : 555300, \t Total Gen Loss : 3571.132080078125, \t Total Dis Loss : 1.3435269465844613e-06\n",
      "Steps : 555400, \t Total Gen Loss : 4031.733154296875, \t Total Dis Loss : 6.685633252345724e-06\n",
      "Steps : 555500, \t Total Gen Loss : 3360.35205078125, \t Total Dis Loss : 1.8278826246387325e-05\n",
      "Steps : 555600, \t Total Gen Loss : 3183.657958984375, \t Total Dis Loss : 1.933578823809512e-05\n",
      "Steps : 555700, \t Total Gen Loss : 3445.832275390625, \t Total Dis Loss : 8.693417476024479e-05\n",
      "Steps : 555800, \t Total Gen Loss : 3335.958984375, \t Total Dis Loss : 1.0635435501171742e-05\n",
      "Steps : 555900, \t Total Gen Loss : 3736.053955078125, \t Total Dis Loss : 2.743192226262181e-06\n",
      "Steps : 556000, \t Total Gen Loss : 3157.64697265625, \t Total Dis Loss : 9.816098099690862e-06\n",
      "Steps : 556100, \t Total Gen Loss : 3013.022705078125, \t Total Dis Loss : 0.00015328133304137737\n",
      "Steps : 556200, \t Total Gen Loss : 3561.7333984375, \t Total Dis Loss : 7.673207619518507e-06\n",
      "Steps : 556300, \t Total Gen Loss : 3445.476806640625, \t Total Dis Loss : 9.186832699015213e-07\n",
      "Steps : 556400, \t Total Gen Loss : 3597.309814453125, \t Total Dis Loss : 0.0003902415919583291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 556500, \t Total Gen Loss : 2793.0693359375, \t Total Dis Loss : 2.879442945413757e-05\n",
      "Steps : 556600, \t Total Gen Loss : 3975.733642578125, \t Total Dis Loss : 5.645239070872776e-06\n",
      "Steps : 556700, \t Total Gen Loss : 3996.15234375, \t Total Dis Loss : 1.3152152860129718e-05\n",
      "Steps : 556800, \t Total Gen Loss : 3631.287841796875, \t Total Dis Loss : 1.415594215359306e-05\n",
      "Time for epoch 99 is 75.69306898117065 sec\n",
      "Steps : 556900, \t Total Gen Loss : 3469.403564453125, \t Total Dis Loss : 1.928650090121664e-05\n",
      "Steps : 557000, \t Total Gen Loss : 3805.81201171875, \t Total Dis Loss : 7.945001016196329e-06\n",
      "Steps : 557100, \t Total Gen Loss : 3860.520751953125, \t Total Dis Loss : 7.783403816574719e-06\n",
      "Steps : 557200, \t Total Gen Loss : 3726.618896484375, \t Total Dis Loss : 1.8650718629942276e-05\n",
      "Steps : 557300, \t Total Gen Loss : 3554.25390625, \t Total Dis Loss : 1.63530614827323e-06\n",
      "Steps : 557400, \t Total Gen Loss : 4108.9423828125, \t Total Dis Loss : 0.035904236137866974\n",
      "Steps : 557500, \t Total Gen Loss : 3676.426025390625, \t Total Dis Loss : 4.9604363994149026e-06\n",
      "Steps : 557600, \t Total Gen Loss : 3531.212158203125, \t Total Dis Loss : 5.8795900258701295e-05\n",
      "Steps : 557700, \t Total Gen Loss : 2962.251220703125, \t Total Dis Loss : 2.338525064260466e-06\n",
      "Steps : 557800, \t Total Gen Loss : 3618.57373046875, \t Total Dis Loss : 3.6954857023374643e-06\n",
      "Steps : 557900, \t Total Gen Loss : 3699.345703125, \t Total Dis Loss : 1.6088677057268796e-06\n",
      "Steps : 558000, \t Total Gen Loss : 3530.598876953125, \t Total Dis Loss : 4.3157999130016833e-07\n",
      "Steps : 558100, \t Total Gen Loss : 4130.86279296875, \t Total Dis Loss : 1.5206644548015902e-06\n",
      "Steps : 558200, \t Total Gen Loss : 3627.365234375, \t Total Dis Loss : 7.342928711295826e-06\n",
      "Steps : 558300, \t Total Gen Loss : 3371.927001953125, \t Total Dis Loss : 1.7764734366210178e-05\n",
      "Steps : 558400, \t Total Gen Loss : 3529.84765625, \t Total Dis Loss : 1.2610935300472192e-06\n",
      "Steps : 558500, \t Total Gen Loss : 3779.46826171875, \t Total Dis Loss : 3.569245791368303e-06\n",
      "Steps : 558600, \t Total Gen Loss : 3839.326904296875, \t Total Dis Loss : 2.1209339138295036e-06\n",
      "Steps : 558700, \t Total Gen Loss : 3552.398193359375, \t Total Dis Loss : 2.3064901597535936e-06\n",
      "Steps : 558800, \t Total Gen Loss : 3426.564453125, \t Total Dis Loss : 6.2580716075899545e-06\n",
      "Steps : 558900, \t Total Gen Loss : 3424.042724609375, \t Total Dis Loss : 3.5197737702219456e-07\n",
      "Steps : 559000, \t Total Gen Loss : 3792.549560546875, \t Total Dis Loss : 1.2107311704312451e-06\n",
      "Steps : 559100, \t Total Gen Loss : 3940.529296875, \t Total Dis Loss : 9.349273568659555e-07\n",
      "Steps : 559200, \t Total Gen Loss : 3809.13037109375, \t Total Dis Loss : 5.1040915423072875e-05\n",
      "Steps : 559300, \t Total Gen Loss : 3319.78857421875, \t Total Dis Loss : 3.6218557397660334e-06\n",
      "Steps : 559400, \t Total Gen Loss : 3753.495849609375, \t Total Dis Loss : 5.59456293558469e-06\n",
      "Steps : 559500, \t Total Gen Loss : 3570.59423828125, \t Total Dis Loss : 2.627180583658628e-05\n",
      "Steps : 559600, \t Total Gen Loss : 3938.260009765625, \t Total Dis Loss : 1.011762037705921e-06\n",
      "Steps : 559700, \t Total Gen Loss : 3553.899658203125, \t Total Dis Loss : 1.746064526741975e-06\n",
      "Steps : 559800, \t Total Gen Loss : 3321.7001953125, \t Total Dis Loss : 0.0007723909802734852\n",
      "Steps : 559900, \t Total Gen Loss : 3080.826416015625, \t Total Dis Loss : 2.535638941481011e-06\n",
      "Steps : 560000, \t Total Gen Loss : 3499.5888671875, \t Total Dis Loss : 1.2892679706055787e-06\n",
      "Steps : 560100, \t Total Gen Loss : 3313.69091796875, \t Total Dis Loss : 9.471955877415894e-07\n",
      "Steps : 560200, \t Total Gen Loss : 4122.18896484375, \t Total Dis Loss : 8.93981621175044e-07\n",
      "Steps : 560300, \t Total Gen Loss : 3152.676025390625, \t Total Dis Loss : 2.873106950573856e-06\n",
      "Steps : 560400, \t Total Gen Loss : 4037.740966796875, \t Total Dis Loss : 0.0001349803787888959\n",
      "Steps : 560500, \t Total Gen Loss : 3597.452880859375, \t Total Dis Loss : 1.3428089005174115e-05\n",
      "Steps : 560600, \t Total Gen Loss : 3507.984619140625, \t Total Dis Loss : 4.265469397068955e-05\n",
      "Steps : 560700, \t Total Gen Loss : 3342.178466796875, \t Total Dis Loss : 8.529330807505175e-05\n",
      "Steps : 560800, \t Total Gen Loss : 3702.2294921875, \t Total Dis Loss : 0.00014097914390731603\n",
      "Steps : 560900, \t Total Gen Loss : 3258.3095703125, \t Total Dis Loss : 0.00020995998056605458\n",
      "Steps : 561000, \t Total Gen Loss : 3717.290771484375, \t Total Dis Loss : 1.428130690328544e-05\n",
      "Steps : 561100, \t Total Gen Loss : 4185.4072265625, \t Total Dis Loss : 3.8051333831390366e-05\n",
      "Steps : 561200, \t Total Gen Loss : 2820.668701171875, \t Total Dis Loss : 2.998007221322041e-05\n",
      "Steps : 561300, \t Total Gen Loss : 3759.3408203125, \t Total Dis Loss : 5.225254426477477e-05\n",
      "Steps : 561400, \t Total Gen Loss : 3269.035400390625, \t Total Dis Loss : 2.893595819841721e-06\n",
      "Steps : 561500, \t Total Gen Loss : 3398.694580078125, \t Total Dis Loss : 1.9052916968576028e-06\n",
      "Steps : 561600, \t Total Gen Loss : 3866.211181640625, \t Total Dis Loss : 1.5392301065730862e-05\n",
      "Steps : 561700, \t Total Gen Loss : 3464.658447265625, \t Total Dis Loss : 7.982442184584215e-06\n",
      "Steps : 561800, \t Total Gen Loss : 3960.13134765625, \t Total Dis Loss : 1.1305786756565794e-05\n",
      "Steps : 561900, \t Total Gen Loss : 3414.679443359375, \t Total Dis Loss : 7.488662959076464e-06\n",
      "Steps : 562000, \t Total Gen Loss : 3160.351318359375, \t Total Dis Loss : 9.987166777136736e-06\n",
      "Steps : 562100, \t Total Gen Loss : 3938.672119140625, \t Total Dis Loss : 6.169989319460001e-06\n",
      "Steps : 562200, \t Total Gen Loss : 3589.278564453125, \t Total Dis Loss : 1.6772327171565848e-06\n",
      "Steps : 562300, \t Total Gen Loss : 3634.520263671875, \t Total Dis Loss : 2.5559952518960927e-06\n",
      "Steps : 562400, \t Total Gen Loss : 3872.455322265625, \t Total Dis Loss : 2.4957471396191977e-06\n",
      "Steps : 562500, \t Total Gen Loss : 3683.724365234375, \t Total Dis Loss : 3.204018867108971e-06\n",
      "Time for epoch 100 is 77.2479133605957 sec\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        steps += 1\n",
    "        gen_loss, disc_loss = train_step(tf.cast(images, tf.float32)) # warning 제거 위해 임의로 자료형 cast함\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print ('Steps : {}, \\t Total Gen Loss : {}, \\t Total Dis Loss : {}'.format(steps, gen_loss.numpy(), disc_loss.numpy()))\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_path)\n",
    "        \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7fa51b944710>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 도중 저장된 체크포인트 활용\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(test_dataset, set_lambda=0.9):\n",
    "    an_scores = []\n",
    "    gt_labels = []\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(test_dataset):\n",
    "        generated_images = generator(tf.cast(x_batch_train, tf.float32), training=True) # custom: cast\n",
    "        _, feat_real = discriminator(tf.cast(x_batch_train, tf.float32), training=True) # custom: cast\n",
    "        _, feat_fake = discriminator(tf.cast(generated_images, tf.float32), training=True) # custom: cast\n",
    "\n",
    "        generated_images, feat_real, feat_fake = generated_images.numpy(), feat_real.numpy(), feat_fake.numpy()        \n",
    "\n",
    "        rec = abs(tf.cast(x_batch_train, tf.float32) - tf.cast(generated_images, tf.float32)) # custom: cast\n",
    "        lat = (feat_real - feat_fake) ** 2\n",
    "\n",
    "        rec = tf.reduce_sum(rec, [1,2,3])\n",
    "        lat = tf.reduce_sum(lat, [1,2,3])\n",
    "        \n",
    "        error = (set_lambda * tf.cast(rec, tf.float32)) + ((1 - set_lambda) * tf.cast(lat, tf.float32))\n",
    "        \n",
    "        an_scores.append(error)\n",
    "        gt_labels.append(y_batch_train)\n",
    "        \n",
    "    an_scores = np.concatenate(an_scores, axis=0).reshape([-1])\n",
    "    gt_labels = np.concatenate(gt_labels, axis=0).reshape([-1])\n",
    "    \n",
    "    an_scores = (an_scores - np.amin(an_scores)) / (np.amax(an_scores) - np.amin(an_scores))\n",
    "    \n",
    "    return an_scores, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 15000\n"
     ]
    }
   ],
   "source": [
    "an_scores, gt_labels = _evaluate(test_dataset)\n",
    "\n",
    "print(len(an_scores), len(gt_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000,)\n",
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "# 라벨에 따라 anomaly score의 분포가 다르게 나타나는지 검증\n",
    "normal = []\n",
    "anormaly = []\n",
    "for score, label in zip(an_scores, gt_labels):\n",
    "    if label == 0:\n",
    "        anormaly.append(score)\n",
    "    else:\n",
    "        normal.append(score)\n",
    "\n",
    "normal = np.array(normal)\n",
    "print(normal.shape)\n",
    "anormaly = np.array(anormaly)\n",
    "print(anormaly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(normal, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.hist(anormaly, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43589398 0.53740144\n",
      "0.1276057 0.15030402\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(normal, norm.pdf(normal, np.mean(normal), np.std(normal)), 'o')\n",
    "plt.plot(anormaly, norm.pdf(anormaly, np.mean(anormaly), np.std(anormaly)), 'o')\n",
    "\n",
    "print(np.mean(normal), np.mean(anormaly))\n",
    "print(np.std(normal), np.std(anormaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에러 발생 확률 : 0.442\n"
     ]
    }
   ],
   "source": [
    "# 에러 발생 확률 (이영빈님 코드 참고)\n",
    "detect_error = []\n",
    "\n",
    "for i in range(len(bol_test_labels)):\n",
    "    if tf.cast(bol_test_labels[i], dtype = tf.int32) != gt_labels[i]:\n",
    "        detect_error.append(i)\n",
    "        error_rate = len(detect_error)/len(bol_test_labels)\n",
    "        \n",
    "print('에러 발생 확률 : {}'.format(error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEgAAABYCAYAAABWO7HcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWTklEQVR4nO2cP5IlW5LWf+7nRMTNzHqve1phA4ACZiAjsJCRUWAFbGRWMAKYISDBBljBCJhhoyGgYNDT/V5l5r0Rx90R3E/crB6bl3Sl8EqoU3YtLW/ePye+4/++zz1KIoLv6x9e+mtv4Ftf3wF6Z30H6J31HaB31neA3lnfAXpnfQfonfXNACQivxOR/ywizyLyP0XkL3/tPQH0X3sDb9ZfATvwj4B/CfwXEfmbiPjvv+am5FuopEXkCfg74J9HxN/Wc38N/K+I+Pe/5t6+FRf7p4BNcGr9DfDPfqX9nOtbAegT8Mc/ee6PwA+/wl6+WN8KQJ+BH//kuR+Bn3+FvXyxvhWA/hboIvJP3jz3L4BfNUDDNxKkAUTkPwIB/Bsyi/1X4F/92lnsW7EggH8HPAD/G/gPwL/9tcGBb8iCvtX1LVnQN7m+A/TO+g7QO+s7QO+s7wC9s36Rzf/jf/2XgYC0hqgiqvTe6a2zrg+0trAuj3Rd2LYf6L2zrRsQRDjujocDEBFEgIdj7hzHwb7fuO2vHMeNwMh8KkR+AhFGhNVnRB6n5F8JEAAPwh3c83fqeQGpZ/K78zXhDlHvsQEE/+O//SfhH1i/CFBrK6JC6x1RQVRprdNao7WF1joiCiK56dpYBCBC1E4jIn+PIAoAJD8vr0QJd4I4PysvKvCI+2dPAKNgjDeQiIJPaEFEJlQF0gnd+Wn/PwXOLwL06fF3aFPWbYUCSLUh2ugFTm5WCbwecQKDaJ6cJCiOk7YgIA1tC6ILgeN1ytLyu92nBRoioCqcEaHAy29RVBoR+f2UxUrkwcl5eFGvqfcXkB8CqPcVbYpqTxdriraeIInWXhMUD8Nc2G0nIk/M3c8Libro8o7TWibo7p7A4ef3iwiCpFHdn737UOQPlemWE584X9t7P/dg7NhRm5sQvVMo/yJA63rJE1BFm6Zr9QXRhvt0AQec4Y5jjH3gDmZ/ckLnL4Kq3q9VG9o64o7E/bRVta6jIRInKIlJgiQCip5ARkAod+sSYVnW8/NuN8tDmpYWznvrFwGKMNyDMeIESG2greVmynKIQJzcsQnmgRdAEVFmDqqKitLoiQ6CKLTeCJa0xgBwIiRdNqSed9qyICIMG4QHYU6IAwoSaBPC83M9HG3Cw8OFCOd2g2ENaZpWiWYM+4gFuWcGOcaBmKKtod7R1ph2H9O4w0HSQcwDi/KleeYi9NbKeqJcVBEVlAQt6nQjMnin+wpmA/eg9442xW+ZHWP6qwQid5cMAg2hNWXbFtwDc6ONhjZFAJUAf5Ncvgag2+0Zj+DwUfFCkNEqFikiad4BdZqASoUHyZRchyQCIxpiecGqDdWWpz+DvMwL/nLfrTV6bzw9PdB6w2MwRpQlCI2Z0uN8rweowrJ2zAxujihobzQRFAi3jwF02E5Eoo8KhCBYGWimfq204+5EHmNue5agAmlnggMSUjVOPpSOCIREvYoK5HI+VCQvdun0paGav6PTWeSMTuF3jFWF1qRimVc21Pw8qX18BKDX41oX53lSrYE0RBouo7bhuTGh3K6i54lQnK/JOFuxRYGmuXEEtDZbNeG8UA9oTehN6V1pXWiapUSreNIi3TbCscp4qlrgOOGG2wC3tDadJUiVIV8LkLmXf6ePo3FiMGNPnKY90y8Fxj29T6BmsE4wCxCpVKv1Yj0rzYoRGb9EJFE703h+s8yP4P7dmbUk6zQL3JwwRwKazgPMQ403xeSfDdCILN97q3qlCdLSfdJNIDQq7c5KpK70rHKj0jc0VULAKzY5hs6L05ZAeGUXqSDigtTBHMeNYaQ1hOfrHHxU/BMq+ylNFhqNcTPGYdht0BSW3hiiOMIw+ZiLSfm+qt5L95MClK9LnHVbLkecL774tID5+5niJbNNCHa+I01eRSoTxqzrcHPE0yLcAvWspFu/lx3md64WIZg5PvL1rSnL0gkPfHpWfMCCWtMKiFKlPriDF0D5BX6/+CrrpdxpFmyt0ntM16mUrKJ0aXSE4Z61jWe8UhoySWVWhNjIeDiG4+a0SG64bRvujpkzzHDjdFWTwTgGPpy2LTxcHrDrzmEjXfgjddAZMwLcA0Uy27zx+ahaR4uX5Rvv1Wy+prKWO6jgDkvrbOvCgyws0rDbleGGRX5v1w4RHA7SslZKZp9VulseRFsbnz49cbvu3G43tPaajD0Y+4EdBrX/3joSB2GTy33AgmbCdM+g5x6E3nnP29cULmd8/eJTCpyIzFJR/GnpnYe+smnnNnaILCdEkkOZ+XkRQjH+SWM8cJymytPjEwQcx36WCfk9wTDDbGRIQ2nS7u4a8p4BvUc1/P4BkcUhRSlE5tb1jCkzDhBUFnkTd1RZtw3I4B/uXF9eeXpo6Kp0HJdAWmWZ4njJ4fOfueBeNAGhaRar7sY4Do79hoTTBcJHAjGSCi2tIcA4BmPPR6j8vcP8swDyWYxAnqznTyj3OzUXOdN9Vs3zp5yxSAR6z68LN8KdY9/xdYB3Mvdk5nrL/OX8ly6c2pkiElmJi2BlJTYGRKQoMi02qrAtguzm9XrLGvwjAIGXhDAJoyAhiGZxfxe98pSjqrxJLRLIiimt88OnT5g7z69X9nFlv155doj+wh4HRuDaCdHTLZskwVVpCK2q4QWIzEgBzy8v7PuN8IEUSx/HgYjweHmgqdJUsQhen1/Teqz8UD4Ug+7Sy9//mLfB5s3Penq65oxJZ0gUudOJCMY4ONxx8SrcrAiAVmGnSQ9UCqiMOwm+ZiA/dszGWTUTgbuXBZ914fn8qVHFuwb0TpqXqco1qIB2jyt5AhL65ksmKlKmTxWQQBifP7+mI1rWShrKOAbXONCezD78zla7LrRt49I7a+9EGM0CYSXc0TB8H7y+JEBmgzFGgmCBivByvdKbsi1LEaYCGnm7468D6K7oyVmlykkTZuaalKKCdMzjmvLoLODgth9JFInidh0xA/wM9c2LQsagidIJmiTfb6QupOG4GzZuYAZ2JKeKSAFOWrIUYPgkRCOrdc2abFl6yrwfAogMku0MKIDG2z9OBPAwPCZcU8KgQAvcg8+fX1ha42FZEFGWvtHUUDc6hkbQSps2u9EF+nKhRVKSHgPi4Bg7Pg5uzz+fWYuSbnW5ENoTGHdut1dkOLd9py8L63phWTrLunCUtX0AIPki2J6o3JPX6cQqioRg3DOcluX5bMlohv3DHT3Frfzw3jqNYBw3hh3szz9zu97gOujLRu8LPm64G9f9wM1wzzAgqvR1ZVlXYnkgpPFy3bExMqH4KevhEfSWmpaZpUTztQDpF5LFnW58GfnqpypnZ6qAmbWQvdmDi7P7oNHoks4WklLGQsB4xcbO609/4JDOri+09ZG+bjQcIdiPUVoSKdy1znZ55PL0BP1CiHIbP1WB26iCqK4kkJb9veM4Tof4KoDmmrryvfZ5A1IRWalCjKi2j6QmrMWjuMfeNw3EwMKQcK7mHGHsz39g3K6oXen9AZZO9I63pTKUQztbgoQqpp3rCMbLFZqB6JnFZrtI+4I0BYExRqqMHh+lGneAgC+6EcHdilJ6zUooFYjSkTSLRp0VNlPdC8Ii5R3POHBwMHxwvD7jY0f9ALlAV0ZrWLmKkFX59PhQwaWzW0rD2rIIi5gFrSIatCoqowrLevsX1f6fDVDy0Kwd7uB8mR7lT1//5rcIP90uL6xkHnPCjLgN7NhT35FBw+jiaAd92vBtYzxeeI6FVxohRkRmwajsapUQJAo8s/zZlpRpekdQtBKIwWnRZzfpqwGaHcBfZHRf6rrZZr5rzhPG6WkSgZgR4yCOAz9uiA1MDcRZcFRg7YI1RbqiJlX0pgWUEFWIVxwDJDxjyixs5a45qSgeX+qHb3LMVwL0JgWm3mJ3olp9dXevOF1tZ+KsL2bbufeW5UIIsR/4T38kbgfcDuy4gQ/0ItDA1WgaNAlEvQQ5I0JpKpnKSyPSvhAieAi4lUaVLt1aS2HeB4im/F0DDK31zLrDP6YHzfUF6WTG29loKZeLeyrNN00XrV+ojoMZdr0ix0CG0SrwTsXW3JAIRuoaya/oqHC2a7L8SAoSkvzQsxhD8C8SbHZZKv6RaV5nbz6cu/L3lQDlREc7o72XWtayPVAi2j0uRUBIsfsTnGwN2Tiw1yu33/9fNlEetNMaaJPkYmFc9xstnBBDvKH9Ee2drW90qeDs+bnSW7qYKG6Km6Dlopk4yA5GtZrMg6MCdJPA9xthHykUq47JrPHlIIFIdaOqeYg2hEA8s1fIzOtlPUEy8tZpy0KMwW1/ZelCU6lWTAr8HnA9jNYGy7GjOli7VW0OFlbfS7qaziDdyhIjRbI3OzDAIgU0iUjXtCOpytcClNRCoCxIo3SVqn/iTYpHU9NBI2XZysPZea0M2ARdFpbHB47PP3G7fWYzZWnKqlt2bGmYOy+3g0V2WK70fmNhS0tBOMII2lnnZDGYW3UPwo1x7ATZxZiC3vDUrMOMRsBx5b0Bhl+upLUsJnJAYMYiVcVnbykmGZyAUBZUcgRVk5SOrX2l//hbwLHjmq2c216NwEYouBnmQfdAIzPbKga9ETUAEdKRbQMUj8acQjGLklozyRxH0hptcj8wOxilBshHuFhyqZLxKr1qdSOQ2QQNSsipwnBytdkovIv5gUBfWJ4+YccVeVkZxyt+3GhNce+peYeTk3KBYizibBpIpyxXcVmQZcVDOKyEPAxCce6dVncnVBHt5RA5+RF2IG6nGvFVAE2ynv6djX4H0MAy9mWKLRErXCp7RRU99xgwt+Gi7Logjz/wiHOTYGg2A/04MFL7vlw2trUno49B951VN9rSeHh8grbB5bfsI3i+DsbYsZFENMQ5ysKtqqSWp4t0ZZjiKizSPlYHScyEPtu8DmJpOVLZKtKn4pza4uRbUkXj1Janqmja0tUuD9h2IY4rcT0ydrjTpdHaRlc9tSANS7BEWJcV+oovC0hwPQK3OxhTSdTaY83CZXZTISr5qCj6ob5YZPXQRJBqQ+dkmXHM5uEczysqYnVSoSBdil7cOZwHaSXa6X1DH57YMDwObL/x/Pmarvb4gKqwrQtNgrADdSvhLMuNfb8xRmDH4Nhv7LcrRLaPwnO28eGSA1Q2DiQsy4CmIAu27x8rFNO1ZEpfZVGlHnoCJNUqDq/zE+6y6dkGfvM+qPcJLjXz2BdoGYAhY4+Z1dzil1qOe+7D3TEf2WX1UQ+DqAfp5U0V99SgJAzxgZ6DnnOK9isBoixIE4GiDhn03GbPLCPMGMeb9J/Sq5WWTcSp+/jJ+GvGqG/IMtC+Emb0lin79fU1LehyIfoFKUmjMWj9wCV4cWM353rbsXEQcRDjgPCqrXIExt05blewHcaVtXeaKGEGH1IUC6DkZJnNcsYv/TlKRJOK5sGdciTA8caA4j6LU3EpGXlDtePaT714zkcPc27HQIcjKV1jDt0cw9jNOCxnf9xH8q5zwKgO1YtOxIxP5RmS4tvHpjsAPPBxJGksmUDRKgFygmvqz87Uju4SxwQqRYHss+XnVPCWjvYN2kroQWsNNyMCjuE8Xw98GYzujJ5BWnfDMZ6PUcKblTXY2UTJ3rxUk7JiTx1sk+RjUyL5eoDmLQBwcqtA0m8nxZiVNNVKmZJEFZSq3C3LrMqpJKfiJD2QjqOp7ZRoFICLYKIcoWAgBq6BjiPF17I0q+FPkTi17lFWb254BW3CkBh0WvbW1D4m2k8taAqTs7WT2evO8pMx11DnjEGTw4lkKxqIkkRkjuRFDXFKx2hYEc85rxgoLsoRQpjQDEIDyKFSL43HI60jU3s1A4qxWwXwlC8TpCbCohDqBB/gYjYOZl97Zh+ffSYtsWoOIElKsn1Z5w0JKU+kBwIpV8yMNAcpF2+ow3J5yNsthtUtCIGuG7I9sTz+yPbwI23dQDut56T/0jrDBtcj0r18nDlvHsaom1f60lM+GcpudgftHR/7ZYBs1FRoO2eEorK3Am/72lEqn7YcfPKpy1CqQEC0OtlIva0JNGpOesm7hGzdM4UT6Lqi60bfHuiXx7q3o9F7r5HADuOGmmbtA/dEwH0mSYiq9BsuikVqUMoHY9C88HlRGfjO76/wVFOu82/RmQMquOMES9MksJOfhNIFljZoLWgBj8tfEO48bz8y3NjDkb6i64X16bcsDz/w+PQb1vXCp09PQHB7+ZnPn//A5+ffZ89s7GhN0I+6S8AtpzscxaUT/QmzA/GR4tpHAXqLVFTm+SJLTV2Mki/DM3AXaISf0+2TauTss+eDpARrWyGUfQgUyZS+oNsDbX2gLRvL5Yl1e+Dy9EPSkv2WU/9Vo0V41aZyumk2LYsjRnLHiIqZMXXGjwBUEHtVtyJ5n8UkoapypnBi4HYgkj1wqUkLJcffpGqcsCgOdJxC3LJcEFlY9AGqYm7rxnJ5pK8X+nLh8uPvuDw8slwesxvCH07Op7VXs1QC5hCDm5XolzfguActsppO+vQRgCrG3GcVi7QypyTktKLzPkH3zF6T6L5h9CJS5X0GereokiCzl2ijr0vWQMeg941te2R9eGTdLizrgraGmTHGqIfl58Q8thl76vFmDsfdsGPkQXqS3w8Faf7k/WcDsQRzLdkziGxyUNmEEtWlXlt05e5mOcprdQcPbcn0ro3L4yeaBdfnV9bLIz/8+Bf88Jsfefz0xHDFHV5fnzmuV16eX7i93rBjJoSc3IesueZcZM5EOnbs7K8vqO2ID1j6OWv0VQDNGmcG6Ly8gj04b6Ossq7+ktVql7uFhaVEMiVR81HDUBdEN5CFw1Ic6+TA6LosbNvG0+MjTUim//rKcQyO68HYd27XF/bbzjgc1TiL0ql6JhPIwSyrgtKKSDeZRemHACoLecOvpio3g3YMS4H+FMiSKPampw5jw960hfJWJpWV3i+oriCd3QYSB13y5rdtXXm8PPDp8ZFhO8fthZ//7v/wen3FjpQ4bs/PHNcbdgTRgqw706db1W4UOB6eIBVVSvO+KwhfBRC8kUtEkb4iWkFaUgceZVl5N2JnXS+0ttH6ytJadjTHrUZ4B156zmX7xMPlR8Lq3tTbM4Fxe31GW2dbHhC78vrz7/n5+SdeXp95fn1lDGNpGxLQm8KyEA9PiBqizjFud+kFPTu8Xs1G6b14mRN6H7T6aoBmWkYU7TnFgcybeyW1F4HWFnpbWJYNbRvaVvqy5NBA64Qbx3FNFUAb28Mnnp5+w+16Yxw7HC+Ewzhu9HD6uhG2c335ic8//YGffv4pp/ER+qX67Sq03li2FSmyNmwQYaRJZ41G3El03m9b8fCjY8A5ANDp6xPaW46QkAL5drlkRpFAtNrL2lDpyLKhy4Wnp0+sfUF94OPg5z/+PjnaemFdVmTd8P3G4TnZ4XbQHfZj53j5+U5vdEGls65PiC4s6yXVTc3J2uPqOf8TA1BaW1NwwwhPW1ogQeRgE2fVYMj9DpGvAyjvZEH7Quudtq7ZPQjoy5bPaVKJXpYFgvQNXTb6+sCyrLQY2DjQ5+csEvslaYJMmSRjhJ8VuzHG7Qyquj7R1p7ga911TUCrwU9mERsncQ5xQmbuPENkxkjJ3r+lIviL6/v/H/TO+v5/d7yzvgP0zvoO0DvrO0DvrO8AvbO+A/TO+n87WL38FBtB3gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i  in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(test_data[detect_error[i]], cmap='gray')\n",
    "    plt.title(gt_labels[detect_error[i]])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
